{"sha": "0d5a7c12c6ad0b0cac8943e771e5c8b985206e4e", "log": "MATH-442 Original code provided by Dietmar Wolz.   ", "commit": "\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/math/optimization/direct/CMAESOptimizer.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.commons.math.optimization.direct;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import org.apache.commons.math.analysis.MultivariateRealFunction;\n+import org.apache.commons.math.exception.MultiDimensionMismatchException;\n+import org.apache.commons.math.exception.NoDataException;\n+import org.apache.commons.math.exception.NotPositiveException;\n+import org.apache.commons.math.exception.OutOfRangeException;\n+import org.apache.commons.math.exception.TooManyEvaluationsException;\n+import org.apache.commons.math.linear.Array2DRowRealMatrix;\n+import org.apache.commons.math.linear.EigenDecomposition;\n+import org.apache.commons.math.linear.EigenDecompositionImpl;\n+import org.apache.commons.math.linear.MatrixUtils;\n+import org.apache.commons.math.linear.RealMatrix;\n+import org.apache.commons.math.optimization.GoalType;\n+import org.apache.commons.math.optimization.MultivariateRealOptimizer;\n+import org.apache.commons.math.optimization.RealPointValuePair;\n+import org.apache.commons.math.random.MersenneTwister;\n+import org.apache.commons.math.random.RandomGenerator;\n+\n+/**\n+ * CMA-ES algorithm. This code is translated and adapted from the Matlab version\n+ * of this algorithm as implemented in module {@code cmaes.m} version 3.51.\n+ *\n+ * Implements the active Covariance Matrix Adaptation Evolution Strategy (CMA-ES)\n+ * for non-linear, non-convex, non-smooth, global function minimization.\n+ * The CMA-Evolution Strategy (CMA-ES) is a reliable stochastic optimization method\n+ * which should be applied, if derivative based methods, e.g. quasi-Newton BFGS or\n+ * conjugate gradient, fail due to a rugged search landscape (e.g. noise, local\n+ * optima, outlier, etc.)  of the objective function. Like a\n+ * quasi-Newton method the CMA-ES learns and applies a variable metric\n+ * of the underlying search space. Unlike a quasi-Newton method the\n+ * CMA-ES does neither estimate nor use gradients, making it considerably more\n+ * reliable in terms of finding a good, or even close to optimal, solution, finally.\n+ *\n+ * <p>In general, on smooth objective functions the CMA-ES is roughly ten times\n+ * slower than BFGS (counting objective function evaluations, no gradients provided).\n+ * For up to <math>N=10</math> variables also the derivative-free simplex\n+ * direct search method (Nelder and Mead) can be faster, but it is\n+ * far less reliable than CMA-ES.</p>\n+ *\n+ * <p>The CMA-ES is particularly well suited for non-separable\n+ * and/or badly conditioned problems.\n+ * To observe the advantage of CMA compared to a conventional\n+ * evolution strategy, it will usually take about <math>30 N</math> function\n+ * evaluations. On difficult problems the complete\n+ * optimization (a single run) is expected to take <em>roughly</em> between\n+ * <math>30 N</math> and <math>300 N<sup>2</sup></math>\n+ * function evaluations.</p>\n+ *\n+ * For more information, please refer to the following links:\n+ * <ul>\n+ *  <li><a href=\"http://www.lri.fr/~hansen/cmaes.m\">Matlab code</a></li>\n+ *  <li><a href=\"http://www.lri.fr/~hansen/cmaesintro.html\">Introduction to CMA-ES</a></li>\n+ *  <li><a href=\"http://en.wikipedia.org/wiki/CMA-ES\">Wikipedia</a></li>\n+ * </ul>\n+ *\n+ * @version $Revision$ $Date$\n+ * @since 3.0\n+ */\n+\n+public class CMAESOptimizer extends\n+        BaseAbstractScalarOptimizer<MultivariateRealFunction> implements\n+        MultivariateRealOptimizer {\n+\n+    /** Default value for {@link #checkFeasableCount}: {@value}. */\n+    public static final int DEFAULT_CHECKFEASABLECOUNT = 0;\n+    /** Default value for {@link #stopfitness}: {@value}. */\n+    public static final double DEFAULT_STOPFITNESS = 0;\n+    /** Default value for {@link #isActiveCMA}: {@value}. */\n+    public static final boolean DEFAULT_ISACTIVECMA = true;\n+    /** Default value for {@link #maxIterations}: {@value}. */\n+    public static final int DEFAULT_MAXITERATIONS = 30000;\n+    /** Default value for {@link #diagonalOnly}: {@value}. */\n+    public static final int DEFAULT_DIAGONALONLY = 0;\n+    /** Default value for {@link #random}. */\n+    public static final RandomGenerator DEFAULT_RANDOMGENERATOR = new MersenneTwister();\n+\n+    // global search parameters\n+    /**\n+     * Population size, offspring number. The primary strategy parameter to play\n+     * with, which can be increased from its default value. Increasing the\n+     * population size improves global search properties in exchange to speed.\n+     * Speed decreases, as a rule, at most linearely with increasing population\n+     * size. It is advisable to begin with the default small population size.\n+     */\n+    private int lambda; // population size\n+    /**\n+     * Covariance update mechanism, default is active CMA. isActiveCMA = true\n+     * turns on \"active CMA\" with a negative update of the covariance matrix and\n+     * checks for positive definiteness. OPTS.CMA.active = 2 does not check for\n+     * pos. def. and is numerically faster. Active CMA usually speeds up the\n+     * adaptation.\n+     */\n+    private boolean isActiveCMA;\n+    /**\n+     * Determines how often a new random offspring is generated in case it is\n+     * not feasible / beyond the defined limits, default is 0. Only relevant if\n+     * boundaries != null.\n+     */\n+    private int checkFeasableCount;\n+    /**\n+     * Lower and upper boundaries of the objective variables. boundaries == null\n+     * means no boundaries.\n+     */\n+    private double[][] boundaries;\n+    /**\n+     * Individual sigma values - initial search volume. inputSigma determines\n+     * the initial coordinate wise standard deviations for the search. Setting\n+     * SIGMA one third of the initial search region is appropriate.\n+     */\n+    private double[] inputSigma;\n+    /** Number of objective variables/problem dimension */\n+    private int dimension;\n+    /**\n+     * Defines the number of initial iterations, where the covariance matrix\n+     * remains diagonal and the algorithm has internally linear time complexity.\n+     * diagonalOnly = 1 means keeping the covariance matrix always diagonal and\n+     * this setting also exhibits linear space complexity. This can be\n+     * particularly useful for dimension > 100.\n+     * @see <a href=\"http://hal.archives-ouvertes.fr/inria-00287367/en\">A Simple Modification in CMA-ES</a> .\n+     */\n+    private int diagonalOnly = 0;\n+    /** Number of objective variables/problem dimension */\n+    private boolean isMinimize = true;\n+    /** Indicates whether statistic data is collected. */\n+    private boolean generateStatistics = false;\n+\n+    // termination criteria\n+    /** Maximal number of iterations allowed. */\n+    private int maxIterations;\n+    /** Limit for fitness value. */\n+    private double stopfitness;\n+    /** Stop if x-changes larger stopTolUpX. */\n+    private double stopTolUpX;\n+    /** Stop if x-change smaller stopTolX. */\n+    private double stopTolX;\n+    /** Stop if fun-changes smaller stopTolFun. */\n+    private double stopTolFun;\n+    /** Stop if back fun-changes smaller stopTolHistFun. */\n+    private double stopTolHistFun;\n+\n+    // selection strategy parameters\n+    /** Number of parents/points for recombination. */\n+    private int mu; //\n+    /** log(mu + 0.5), stored for efficiency. */\n+    private double logMu2;\n+    /** Array for weighted recombination. */\n+    private RealMatrix weights;\n+    /** Variance-effectiveness of sum w_i x_i. */\n+    private double mueff; //\n+\n+    // dynamic strategy parameters and constants\n+    /** Overall standard deviation - search volume. */\n+    private double sigma;\n+    /** Cumulation constant. */\n+    private double cc;\n+    /** Cumulation constant for step-size. */\n+    private double cs;\n+    /** Damping for step-size. */\n+    private double damps;\n+    /** Learning rate for rank-one update. */\n+    private double ccov1;\n+    /** Learning rate for rank-mu update' */\n+    private double ccovmu;\n+    /** Expectation of ||N(0,I)|| == norm(randn(N,1)). */\n+    private double chiN;\n+    /** Learning rate for rank-one update - diagonalOnly */\n+    private double ccov1Sep;\n+    /** Learning rate for rank-mu update - diagonalOnly */\n+    private double ccovmuSep;\n+\n+    // CMA internal values - updated each generation\n+    /** Objective variables. */\n+    private RealMatrix xmean;\n+    /** Evolution path. */\n+    private RealMatrix pc;\n+    /** Evolution path for sigma. */\n+    private RealMatrix ps;\n+    /** Norm of ps, stored for efficiency. */\n+    private double normps;\n+    /** Coordinate system. */\n+    private RealMatrix B;\n+    /** Scaling. */\n+    private RealMatrix D;\n+    /** B*D, stored for efficiency. */\n+    private RealMatrix BD;\n+    /** Diagonal of sqrt(D), stored for efficiency. */\n+    private RealMatrix diagD;\n+    /** Covariance matrix. */\n+    private RealMatrix C;\n+    /** Diagonal of C, used for diagonalOnly. */\n+    private RealMatrix diagC;\n+    /** Number of iterations already performed. */\n+    private int iterations;\n+\n+    /** History queue of best values. */\n+    private double[] fitnessHistory;\n+    /** Size of history queue of best values. */\n+    private int historySize;\n+\n+    /** Random generator. */\n+    private RandomGenerator random;\n+\n+    /** History of sigma values. */\n+    private List<Double> statisticsSigmaHistory = new ArrayList<Double>();\n+    /** History of mean matrix. */\n+    private List<RealMatrix> statisticsMeanHistory = new ArrayList<RealMatrix>();\n+    /** History of fitness values. */\n+    private List<Double> statisticsFitnessHistory = new ArrayList<Double>();\n+    /** History of D matrix. */\n+    private List<RealMatrix> statisticsDHistory = new ArrayList<RealMatrix>();\n+\n+    /**\n+     * Default constructor, uses default parameters\n+     */\n+    public CMAESOptimizer() {\n+        this(0);\n+    }\n+\n+    /**\n+     * @param lambda\n+     *            Population size.\n+     */\n+    public CMAESOptimizer(int lambda) {\n+        this(lambda, null, null, DEFAULT_MAXITERATIONS, DEFAULT_STOPFITNESS,\n+                DEFAULT_ISACTIVECMA, DEFAULT_DIAGONALONLY,\n+                DEFAULT_CHECKFEASABLECOUNT, DEFAULT_RANDOMGENERATOR, false);\n+    }\n+\n+    /**\n+     * @param lambda\n+     *            Population size.\n+     * @param inputSigma\n+     *            Initial search volume - sigma of offspring objective\n+     *            variables.\n+     * @param boundaries\n+     *            Boundaries for objective variables.\n+     */\n+    public CMAESOptimizer(int lambda, double[] inputSigma,\n+            double[][] boundaries) {\n+        this(lambda, inputSigma, boundaries, DEFAULT_MAXITERATIONS, DEFAULT_STOPFITNESS,\n+                DEFAULT_ISACTIVECMA, DEFAULT_DIAGONALONLY,\n+                DEFAULT_CHECKFEASABLECOUNT, DEFAULT_RANDOMGENERATOR, false);\n+    }\n+\n+    /**\n+     * @param lambda\n+     *            Population size.\n+     * @param inputSigma\n+     *            Initial search volume - sigma of offspring objective\n+     *            variables.\n+     * @param boundaries\n+     *            Boundaries for objective variables.\n+     * @param maxIterations\n+     *            Maximal number of iterations.\n+     * @param stopfitness\n+     *            stop if objective function value < stopfitness.\n+     * @param isActiveCMA\n+     *            Chooses the covariance matrix update method.\n+     * @param diagonalOnly\n+     *            Number of initial iterations, where the covariance matrix\n+     *            remains diagonal.\n+     * @param checkFeasableCount\n+     *            Determines how often new. random objective variables are\n+     *            generated in case they are out of bounds.\n+     * @param random\n+     *            Used random generator.\n+     * @param generateStatistics\n+     *            Indicates whether statistic data is collected.\n+     */\n+    public CMAESOptimizer(int lambda, double[] inputSigma,\n+            double[][] boundaries, int maxIterations, double stopfitness,\n+            boolean isActiveCMA, int diagonalOnly, int checkFeasableCount,\n+            RandomGenerator random, boolean generateStatistics) {\n+        this.lambda = lambda;\n+        this.inputSigma = inputSigma;\n+        this.boundaries = boundaries;\n+        this.maxIterations = maxIterations;\n+        this.stopfitness = stopfitness;\n+        this.isActiveCMA = isActiveCMA;\n+        this.diagonalOnly = diagonalOnly;\n+        this.checkFeasableCount = checkFeasableCount;\n+        this.random = random;\n+        this.generateStatistics = generateStatistics;\n+    }\n+\n+    /**\n+     * @return History of sigma values.\n+     */\n+    public List<Double> getStatisticsSigmaHistory() {\n+        return statisticsSigmaHistory;\n+    }\n+\n+    /**\n+     * @return History of mean matrix.\n+     */\n+    public List<RealMatrix> getStatisticsMeanHistory() {\n+        return statisticsMeanHistory;\n+    }\n+\n+    /**\n+     * @return History of fitness values.\n+     */\n+    public List<Double> getStatisticsFitnessHistory() {\n+        return statisticsFitnessHistory;\n+    }\n+\n+    /**\n+     * @return History of D matrix.\n+     */\n+    public List<RealMatrix> getStatisticsDHistory() {\n+        return statisticsDHistory;\n+    }\n+\n+    /** {@inheritDoc} */\n+    @Override\n+    protected RealPointValuePair doOptimize() {\n+        checkParameters();\n+         // -------------------- Initialization --------------------------------\n+        isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+        final FitnessFunction fitfun = new FitnessFunction(boundaries,\n+                isMinimize);\n+        final double[] guess = fitfun.encode(getStartPoint());\n+        // number of objective variables/problem dimension\n+        dimension = guess.length;\n+        initializeCMA(guess);\n+        iterations = 0;\n+        double bestValue = fitfun.value(guess);\n+        push(fitnessHistory, bestValue);\n+        RealPointValuePair optimum = new RealPointValuePair(getStartPoint(),\n+                isMinimize ? bestValue : -bestValue);\n+        RealPointValuePair lastResult = null;\n+\n+        // -------------------- Generation Loop --------------------------------\n+\n+        generationLoop:\n+            for (iterations = 1; iterations <= maxIterations; iterations++) {\n+                // Generate and evaluate lambda offspring\n+                RealMatrix arz = randn1(dimension, lambda);\n+                RealMatrix arx = zeros(dimension, lambda);\n+                double[] fitness = new double[lambda];\n+                // generate random offspring\n+                for (int k = 0; k < lambda; k++) {\n+                    RealMatrix arxk = null;\n+                    for (int i = 0; i < checkFeasableCount+1; i++) {\n+                        if (diagonalOnly <= 0)\n+                            arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n+                                    .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n+                        else\n+                            arxk = xmean.add(times(diagD,arz.getColumnMatrix(k))\n+                                    .scalarMultiply(sigma));\n+                        if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0)))\n+                            break;\n+                        // regenerate random arguments for row\n+                        arz.setColumn(k, randn(dimension));\n+                    }\n+                    copyColumn(arxk, 0, arx, k);\n+                    try {\n+                        fitness[k] = fitfun.value(arx.getColumn(k)); // compute fitness\n+                    } catch (TooManyEvaluationsException e) {\n+                        break generationLoop;\n+                    }\n+                }\n+                // Sort by fitness and compute weighted mean into xmean\n+                int[] arindex = sortedIndices(fitness);\n+                // Calculate new xmean, this is selection and recombination\n+                RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n+                RealMatrix bestArx = selectColumns(arx,Arrays.copyOf(arindex, mu));\n+                xmean = bestArx.multiply(weights);\n+                RealMatrix bestArz = selectColumns(arz,Arrays.copyOf(arindex, mu));\n+                RealMatrix zmean = bestArz.multiply(weights);\n+                boolean hsig = updateEvolutionPaths(zmean, xold);\n+                if (diagonalOnly <= 0)\n+                    updateCovariance(hsig, bestArx, arz, arindex, xold);\n+                else\n+                    updateCovarianceDiagonalOnly(hsig, bestArz, xold);\n+                // Adapt step size sigma - Eq. (5)\n+                sigma *= Math.exp(Math.min(1.0,(normps/chiN - 1.)*cs/damps));\n+                double bestFitness = fitness[arindex[0]];\n+                double worstFitness = fitness[arindex[arindex.length-1]];\n+                if (bestValue > bestFitness) {\n+                    bestValue = bestFitness;\n+                    lastResult = optimum;\n+                    optimum = new RealPointValuePair(\n+                            fitfun.decode(bestArx.getColumn(0)),\n+                            isMinimize ? bestFitness : -bestFitness);\n+                    if (getConvergenceChecker() != null && lastResult != null) {\n+                        if (getConvergenceChecker().converged(\n+                                iterations, optimum, lastResult))\n+                            break generationLoop;\n+                    }\n+                }\n+                // handle termination criteria\n+                // Break, if fitness is good enough\n+                if (stopfitness != 0) { // only if stopfitness is defined\n+                    if (bestFitness < (isMinimize ? stopfitness : -stopfitness))\n+                        break generationLoop;\n+                }\n+                double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+                double[] pcCol = pc.getColumn(0);\n+                for (int i = 0; i < dimension; i++) {\n+                    if (sigma*(Math.max(Math.abs(pcCol[i]), sqrtDiagC[i])) > stopTolX)\n+                        break;\n+                    if (i >= dimension-1)\n+                        break generationLoop;\n+                }\n+                for (int i = 0; i < dimension; i++)\n+                    if (sigma*sqrtDiagC[i] > stopTolUpX)\n+                        break generationLoop;\n+                double historyBest = min(fitnessHistory);\n+                double historyWorst = max(fitnessHistory);\n+                if (iterations > 2 && Math.max(historyWorst, worstFitness) -\n+                        Math.min(historyBest, bestFitness) < stopTolFun)\n+                    break generationLoop;\n+                if (iterations > fitnessHistory.length &&\n+                        historyWorst-historyBest < stopTolHistFun)\n+                    break generationLoop;\n+                // condition number of the covariance matrix exceeds 1e14\n+                if (max(diagD)/min(diagD) > 1e7)\n+                    break generationLoop;\n+                // user defined termination\n+                if (getConvergenceChecker() != null) {\n+                    RealPointValuePair current =\n+                        new RealPointValuePair(bestArx.getColumn(0),\n+                                isMinimize ? bestFitness : -bestFitness);\n+                    if (lastResult != null &&\n+                            getConvergenceChecker().converged(\n+                                    iterations, current, lastResult))\n+                        break generationLoop;\n+                    lastResult = current;\n+                }\n+                // Adjust step size in case of equal function values (flat fitness)\n+                if (bestValue == fitness[arindex[(int)(0.1+lambda/4.)]])\n+                    sigma = sigma * Math.exp(0.2+cs/damps);\n+                if (iterations > 2 && Math.max(historyWorst, bestFitness) -\n+                        Math.min(historyBest, bestFitness) == 0)\n+                    sigma = sigma * Math.exp(0.2+cs/damps);\n+                // store best in history\n+                push(fitnessHistory,bestFitness);\n+                fitfun.setValueRange(worstFitness-bestFitness);\n+                if (generateStatistics) {\n+                    statisticsSigmaHistory.add(sigma);\n+                    statisticsFitnessHistory.add(bestFitness);\n+                    statisticsMeanHistory.add(xmean.transpose());\n+                    statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+                }\n+            }\n+        return optimum;\n+    }\n+\n+    /**\n+     * Checks dimensions and values of boundaries and inputSigma if defined.\n+     */\n+    private void checkParameters() {\n+        double[] init = getStartPoint();\n+        if (boundaries != null) {\n+            if (boundaries.length != 2)\n+                throw new MultiDimensionMismatchException(\n+                        new Integer[] { boundaries.length },\n+                        new Integer[] { 2 });\n+            if (boundaries[0] == null || boundaries[1] == null)\n+                throw new NoDataException();\n+            if (boundaries[0].length != init.length)\n+                throw new MultiDimensionMismatchException(\n+                        new Integer[] { boundaries[0].length },\n+                        new Integer[] { init.length });\n+            if (boundaries[1].length != init.length)\n+                throw new MultiDimensionMismatchException(\n+                        new Integer[] { boundaries[1].length },\n+                        new Integer[] { init.length });\n+            for (int i = 0; i < init.length; i++) {\n+                if (boundaries[0][i] > init[i] || boundaries[1][i] < init[i])\n+                    throw new OutOfRangeException(init[i], boundaries[0][i],\n+                            boundaries[1][i]);\n+            }\n+        }\n+        if (inputSigma != null) {\n+            if (inputSigma.length != init.length)\n+                throw new MultiDimensionMismatchException(\n+                        new Integer[] { inputSigma.length },\n+                        new Integer[] { init.length });\n+            for (int i = 0; i < init.length; i++) {\n+                if (inputSigma[i] < 0)\n+                    throw new NotPositiveException(inputSigma[i]);\n+                if (boundaries != null) {\n+                    if (inputSigma[i] > 1.0)\n+                        throw new OutOfRangeException(inputSigma[i], 0, 1.0);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Initialization of the dynamic search parameters\n+     *\n+     * @param guess\n+     *            initial guess for the arguments of the fitness function\n+     */\n+\n+    private void initializeCMA(double[] guess) {\n+        if (lambda <= 0)\n+            lambda = 4 + (int) (3. * Math.log(dimension));\n+        // initialize sigma\n+        double[][] sigmaArray = new double[guess.length][1];\n+        for (int i = 0; i < guess.length; i++)\n+            sigmaArray[i][0] = inputSigma != null ? inputSigma[i] : 0.3;\n+        RealMatrix insigma = new Array2DRowRealMatrix(sigmaArray, false);\n+        sigma = max(insigma); // overall standard deviation\n+\n+        // initialize termination criteria\n+        stopTolUpX = 1e3 * max(insigma);\n+        stopTolX = 1e-11 * max(insigma);\n+        stopTolFun = 1e-12;\n+        stopTolHistFun = 1e-13;\n+\n+        // initialize selection strategy parameters\n+        mu = lambda / 2; // number of parents/points for recombination\n+        logMu2 = Math.log(mu + 0.5);\n+        weights = log(sequence(1, mu, 1)).scalarMultiply(-1.).scalarAdd(logMu2);\n+        double sumw = 0;\n+        double sumwq = 0;\n+        for (int i = 0; i < mu; i++) {\n+            double w = weights.getEntry(i, 0);\n+            sumw += w;\n+            sumwq += w * w;\n+        }\n+        weights = weights.scalarMultiply(1. / sumw);\n+        mueff = sumw * sumw / sumwq; // variance-effectiveness of sum w_i x_i\n+\n+        // initialize dynamic strategy parameters and constants\n+        cc = (4. + mueff / dimension) /\n+                (dimension + 4. + 2. * mueff / dimension);\n+        cs = (mueff + 2.) / (dimension + mueff + 3.);\n+        damps = (1. + 2. * Math.max(0, Math.sqrt((mueff - 1.) /\n+                (dimension + 1.)) - 1.)) *\n+                Math.max(0.3, 1. - dimension /\n+                        (1e-6 + Math.min(maxIterations, getMaxEvaluations() /\n+                                lambda))) + cs; // minor increment\n+        ccov1 = 2. / ((dimension + 1.3) * (dimension + 1.3) + mueff);\n+        ccovmu = Math.min(1 - ccov1, 2. * (mueff - 2. + 1. / mueff) /\n+                ((dimension + 2.) * (dimension + 2.) + mueff));\n+        ccov1Sep = Math.min(1, ccov1 * (dimension + 1.5) / 3.);\n+        ccovmuSep = Math.min(1 - ccov1, ccovmu * (dimension + 1.5) / 3.);\n+        chiN = Math.sqrt(dimension) *\n+                (1. - 1. / (4. * dimension) + 1 / (21. * dimension * dimension));\n+        // intialize CMA internal values - updated each generation\n+        xmean = MatrixUtils.createColumnRealMatrix(guess); // objective\n+                                                           // variables\n+        diagD = insigma.scalarMultiply(1. / sigma);\n+        diagC = square(diagD);\n+        pc = zeros(dimension, 1); // evolution paths for C and sigma\n+        ps = zeros(dimension, 1); // B defines the coordinate system\n+        normps = norm(ps);\n+\n+        B = eye(dimension, dimension);\n+        D = ones(dimension, 1); // diagonal D defines the scaling\n+        BD = times(B, repmat(diagD.transpose(), dimension, 1));\n+        C = B.multiply(diag(square(D)).multiply(B.transpose())); // covariance\n+        historySize = 10 + (int) (3. * 10. * dimension / lambda);\n+        fitnessHistory = new double[historySize]; // history of fitness values\n+        for (int i = 0; i < historySize; i++)\n+            fitnessHistory[i] = Double.MAX_VALUE;\n+    }\n+\n+    /**\n+     * Update of the evolution paths ps and pc\n+     *\n+     * @param zmean\n+     *            weighted row matrix of the gaussian random numbers generating\n+     *            the current offspring\n+     * @param xold\n+     *            xmean matrix of the previous generation\n+     * @return hsig flag indicating a small correction\n+     */\n+    private boolean updateEvolutionPaths(RealMatrix zmean, RealMatrix xold) {\n+        ps = ps.scalarMultiply(1. - cs).add(\n+                B.multiply(zmean).scalarMultiply(\n+                        Math.sqrt(cs * (2. - cs) * mueff)));\n+        normps = norm(ps);\n+        boolean hsig = normps /\n+            Math.sqrt(1. - Math.pow(1. - cs, 2. * iterations)) /\n+                chiN < 1.4 + 2. / (dimension + 1.);\n+        pc = pc.scalarMultiply(1. - cc);\n+        if (hsig)\n+            pc = pc.add(xmean.subtract(xold).scalarMultiply(\n+                    Math.sqrt(cc * (2. - cc) * mueff) / sigma));\n+        return hsig;\n+    }\n+\n+    /**\n+     * Update of the covariance matrix C for diagonalOnly > 0\n+     *\n+     * @param hsig\n+     *            flag indicating a small correction\n+     * @param bestArz\n+     *            fitness-sorted matrix of the gaussian random values of the\n+     *            current offspring\n+     * @param xold\n+     *            xmean matrix of the previous generation\n+     */\n+    private void updateCovarianceDiagonalOnly(boolean hsig, final RealMatrix bestArz,\n+            final RealMatrix xold) {\n+        // minor correction if hsig==false\n+        double oldFac = hsig ? 0 : ccov1Sep * cc * (2. - cc);\n+        oldFac += 1. - ccov1Sep - ccovmuSep;\n+        diagC = diagC.scalarMultiply(oldFac) // regard old matrix\n+                // plus rank one update\n+                .add(square(pc).scalarMultiply(ccov1Sep))\n+                // plus rank mu update\n+                .add((times(diagC, square(bestArz).multiply(weights)))\n+                        .scalarMultiply(ccovmuSep));\n+        diagD = sqrt(diagC); // replaces eig(C)\n+        if (diagonalOnly > 1 && iterations > diagonalOnly) {\n+            // full covariance matrix from now on\n+            diagonalOnly = 0;\n+            B = eye(dimension, dimension);\n+            BD = diag(diagD);\n+            C = diag(diagC);\n+        }\n+    }\n+\n+    /**\n+     * Update of the covariance matrix C\n+     *\n+     * @param hsig\n+     *            flag indicating a small correction\n+     * @param bestArx\n+     *            fitness-sorted matrix of the argument vectors producing the\n+     *            current offspring\n+     * @param arz\n+     *            unsorted matrix containing the gaussian random values of the\n+     *            current offspring\n+     * @param arindex\n+     *            indices indicating the fitness-order of the current offspring\n+     * @param xold\n+     *            xmean matrix of the previous generation\n+     */\n+    private void updateCovariance(boolean hsig, final RealMatrix bestArx,\n+            final RealMatrix arz, final int[] arindex, final RealMatrix xold) {\n+        double negccov = 0;\n+        if (ccov1 + ccovmu > 0) {\n+            RealMatrix arpos = bestArx.subtract(repmat(xold, 1, mu))\n+                    .scalarMultiply(1. / sigma); // mu difference vectors\n+            RealMatrix roneu = pc.multiply(pc.transpose())\n+                    .scalarMultiply(ccov1); // rank one update\n+            // minor correction if hsig==false\n+            double oldFac = hsig ? 0 : ccov1 * cc * (2. - cc);\n+            oldFac += 1. - ccov1 - ccovmu;\n+            if (isActiveCMA) {\n+                // Adapt covariance matrix C active CMA\n+                negccov = (1. - ccovmu) * 0.25 * mueff /\n+                (Math.pow(dimension + 2., 1.5) + 2. * mueff);\n+                double negminresidualvariance = 0.66;\n+                // keep at least 0.66 in all directions, small popsize are most\n+                // critical\n+                double negalphaold = 0.5; // where to make up for the variance\n+                                          // loss,\n+                // prepare vectors, compute negative updating matrix Cneg\n+                int[] arReverseIndex = reverse(arindex);\n+                RealMatrix arzneg = selectColumns(arz,\n+                        Arrays.copyOf(arReverseIndex, mu));\n+                RealMatrix arnorms = sqrt(sumRows(square(arzneg)));\n+                int[] idxnorms = sortedIndices(arnorms.getRow(0));\n+                RealMatrix arnormsSorted = selectColumns(arnorms, idxnorms);\n+                int[] idxReverse = reverse(idxnorms);\n+                RealMatrix arnormsReverse = selectColumns(arnorms, idxReverse);\n+                arnorms = divide(arnormsReverse, arnormsSorted);\n+                int[] idxInv = inverse(idxnorms);\n+                RealMatrix arnormsInv = selectColumns(arnorms, idxInv);\n+                // check and set learning rate negccov\n+                double negcovMax = (1. - negminresidualvariance) /\n+                        square(arnormsInv).multiply(weights).getEntry(0, 0);\n+                if (negccov > negcovMax)\n+                    negccov = negcovMax;\n+                arzneg = times(arzneg, repmat(arnormsInv, dimension, 1));\n+                RealMatrix artmp = BD.multiply(arzneg);\n+                RealMatrix Cneg = artmp.multiply(diag(weights)).multiply(\n+                        artmp.transpose());\n+                oldFac += negalphaold * negccov;\n+                C = C.scalarMultiply(oldFac)\n+                        // regard old matrix\n+                        .add(roneu)\n+                        // plus rank one update\n+                        .add(arpos.scalarMultiply(\n+                                // plus rank mu update\n+                                ccovmu + (1. - negalphaold) * negccov)\n+                                .multiply(\n+                                        times(repmat(weights, 1, dimension),\n+                                                arpos.transpose())))\n+                        .subtract(Cneg.scalarMultiply(negccov));\n+            } else {\n+                // Adapt covariance matrix C - nonactive\n+                C = C.scalarMultiply(oldFac) // regard old matrix\n+                        .add(roneu)\n+                        // plus rank one update\n+                        .add(arpos.scalarMultiply(ccovmu) // plus rank mu update\n+                                .multiply(\n+                                        times(repmat(weights, 1, dimension),\n+                                                arpos.transpose())));\n+            }\n+        }\n+        updateBD(negccov);\n+    }\n+\n+    /**\n+     * Update B and D from C\n+     *\n+     * @param negccov\n+     *            Negative covariance factor.\n+     */\n+    private void updateBD(double negccov) {\n+        if (ccov1 + ccovmu + negccov > 0 &&\n+                (iterations % 1. / (ccov1 + ccovmu + negccov) / dimension / 10.) < 1.) {\n+            // to achieve O(N^2)\n+            C = triu(C, 0).add(triu(C, 1).transpose());\n+            // enforce symmetry to prevent complex numbers\n+            EigenDecomposition eig = new EigenDecompositionImpl(C, 1.0);\n+            B = eig.getV(); // eigen decomposition, B==normalized eigenvectors\n+            D = eig.getD();\n+            diagD = diag(D);\n+            if (min(diagD) <= 0) {\n+                for (int i = 0; i < dimension; i++)\n+                    if (diagD.getEntry(i, 0) < 0)\n+                        diagD.setEntry(i, 0, 0.);\n+                double tfac = max(diagD) / 1e14;\n+                C = C.add(eye(dimension, dimension).scalarMultiply(tfac));\n+                diagD = diagD.add(ones(dimension, 1).scalarMultiply(tfac));\n+            }\n+            if (max(diagD) > 1e14 * min(diagD)) {\n+                double tfac = max(diagD) / 1e14 - min(diagD);\n+                C = C.add(eye(dimension, dimension).scalarMultiply(tfac));\n+                diagD = diagD.add(ones(dimension, 1).scalarMultiply(tfac));\n+            }\n+            diagC = diag(C);\n+            diagD = sqrt(diagD); // D contains standard deviations now\n+            BD = times(B, repmat(diagD.transpose(), dimension, 1)); // O(n^2)\n+        }\n+    }\n+\n+    /**\n+     * Pushes the current best fitness value in a history queue.\n+     *\n+     * @param vals\n+     *            the history queue\n+     * @param val\n+     *            current best fitness value\n+     */\n+    private static void push(double[] vals, double val) {\n+        for (int i = vals.length-1; i > 0; i--)\n+            vals[i] = vals[i-1];\n+        vals[0] = val;\n+    }\n+\n+    /**\n+     * Sorts fitness values.\n+     *\n+     * @param doubles\n+     *            array of values to be sorted\n+     * @return sorted array of indices pointing into doubles\n+     */\n+    private int[] sortedIndices(final double[] doubles) {\n+        DoubleIndex[] dis = new DoubleIndex[doubles.length];\n+        for (int i = 0; i < doubles.length; i++)\n+            dis[i] = new DoubleIndex(doubles[i], i);\n+        Arrays.sort(dis);\n+        int[] indices = new int[doubles.length];\n+        for (int i = 0; i < doubles.length; i++)\n+            indices[i] = dis[i].index;\n+        return indices;\n+    }\n+\n+    /**\n+     * Used to sort fitness values. Sorting is always in lower value first\n+     * order.\n+     */\n+    private static class DoubleIndex implements Comparable<DoubleIndex> {\n+\n+        /** Value to compare. */\n+        private double value;\n+        /** Index into sorted array. */\n+        private int index;\n+\n+        /**\n+         * @param value\n+         *            Value to compare.\n+         * @param index\n+         *            Index into sorted array.\n+         */\n+        DoubleIndex(double value, int index) {\n+            this.value = value;\n+            this.index = index;\n+        }\n+\n+        /** {@inheritDoc} */\n+        public int compareTo(DoubleIndex o) {\n+            return Double.compare(value, o.value);\n+        }\n+    }\n+\n+    /**\n+     * Normalizes fitness values to the range [0,1]. Adds a penalty to the\n+     * fitness value if out of range. The penalty is adjusted by calling\n+     * setValueRange().\n+     */\n+    private class FitnessFunction {\n+\n+        /** Optional bounds for the objective variables */\n+        private double[][] boundaries;\n+        /** Determines the penalty for boundary violations */\n+        private double valueRange = 1.0;\n+        /**\n+         * Flag indicating whether the objective variables are forced into their\n+         * bounds if defined\n+         */\n+        private boolean isRepairMode = true;\n+        /** Flag indicating the optimization goal. */\n+        private boolean isMinimize = true;\n+\n+        /**\n+         * @param boundaries\n+         *            Bounds for the objective variables.\n+         * @param isMinimize\n+         *            Flag indicating the optimization goal.\n+         */\n+        private FitnessFunction(final double[][] boundaries,\n+                final boolean isMinimize) {\n+            this.boundaries = boundaries;\n+            this.isMinimize = isMinimize;\n+        }\n+\n+        /**\n+         * @param x\n+         *            Original objective variables.\n+         * @return Normalized objective variables.\n+         */\n+        private double[] encode(final double[] x) {\n+            if (boundaries == null)\n+                return x;\n+            double[] res = new double[x.length];\n+            for (int i = 0; i < x.length; i++) {\n+                double diff = boundaries[1][i] - boundaries[0][i];\n+                res[i] = (x[i] - boundaries[0][i]) / diff;\n+            }\n+            return res;\n+        }\n+\n+        /**\n+         * @param x\n+         *            Normalized objective variables.\n+         * @return Original objective variables.\n+         */\n+        private double[] decode(final double[] x) {\n+            if (boundaries == null)\n+                return x;\n+            double[] res = new double[x.length];\n+            for (int i = 0; i < x.length; i++) {\n+                double diff = boundaries[1][i] - boundaries[0][i];\n+                res[i] = diff * x[i] + boundaries[0][i];\n+            }\n+            return res;\n+        }\n+\n+        /**\n+         * @param point\n+         *            Normalized objective variables.\n+         * @return Objective value + penalty for violated bounds.\n+         */\n+        private double value(final double[] point) {\n+            double value;\n+            if (boundaries != null && isRepairMode) {\n+                double[] repaired = repair(point);\n+                value = CMAESOptimizer.this\n+                        .computeObjectiveValue(decode(repaired)) +\n+                        penalty(point, repaired);\n+            } else\n+                value = CMAESOptimizer.this\n+                        .computeObjectiveValue(decode(point));\n+            return isMinimize ? value : -value;\n+        }\n+\n+        /**\n+         * @param x\n+         *            Normalized objective variables.\n+         * @return True if in bounds\n+         */\n+        private boolean isFeasible(final double[] x) {\n+            if (boundaries == null)\n+                return true;\n+            for (int i = 0; i < x.length; i++) {\n+                if (x[i] < 0)\n+                    return false;\n+                if (x[i] > 1.0)\n+                    return false;\n+            }\n+            return true;\n+        }\n+\n+        /**\n+         * @param valueRange\n+         *            Adjusts the penalty computation.\n+         */\n+        private void setValueRange(double valueRange) {\n+            this.valueRange = valueRange;\n+        }\n+\n+        /**\n+         * @param x\n+         *            Normalized objective variables.\n+         * @return Repaired objective variables - all in bounds.\n+         */\n+        private double[] repair(final double[] x) {\n+            double[] repaired = new double[x.length];\n+            for (int i = 0; i < x.length; i++) {\n+                if (x[i] < 0)\n+                    repaired[i] = 0;\n+                else if (x[i] > 1.0)\n+                    repaired[i] = 1.0;\n+                else\n+                    repaired[i] = x[i];\n+            }\n+            return repaired;\n+        }\n+\n+        /**\n+         * @param x\n+         *            Normalized objective variables.\n+         * @param repaired\n+         *            Repaired objective variables.\n+         * @return Penalty value according to the violation of the bounds.\n+         */\n+        private double penalty(final double[] x, final double[] repaired) {\n+            double penalty = 0;\n+            for (int i = 0; i < x.length; i++) {\n+                double diff = Math.abs(x[i] - repaired[i]);\n+                penalty += diff * valueRange;\n+            }\n+            return isMinimize ? penalty : -penalty;\n+        }\n+    }\n+\n+    // -----Matrix utility functions similar to the Matlab build in functions------\n+\n+    /**\n+     * @param m\n+     *            Input matrix\n+     * @return Matrix representing the element wise logarithm of m.\n+     */\n+    private static RealMatrix log(final RealMatrix m) {\n+        double[][] d = new double[m.getRowDimension()][m.getColumnDimension()];\n+        for (int r = 0; r < m.getRowDimension(); r++)\n+            for (int c = 0; c < m.getColumnDimension(); c++)\n+                d[r][c] = Math.log(m.getEntry(r, c));\n+        return new Array2DRowRealMatrix(d, false);\n+    }\n+\n+    /**\n+     * @param m\n+     *            Input matrix\n+     * @return Matrix representing the element wise square root of m.\n+     */\n+    private static RealMatrix sqrt(final RealMatrix m) {\n+        double[][] d = new double[m.getRowDimension()][m.getColumnDimension()];\n+        for (int r = 0; r < m.getRowDimension(); r++)\n+            for (int c = 0; c < m.getColumnDimension(); c++)\n+                d[r][c] = Math.sqrt(m.getEntry(r, c));\n+        return new Array2DRowRealMatrix(d, false);\n+    }\n+\n+    /**\n+     * @param m Input matrix\n+     * @return Matrix representing the element wise square (^2) of m.\n+     */\n+    private static RealMatrix square(final RealMatrix m) {\n+        double[][] d = new double[m.getRowDimension()][m.getColumnDimension()];\n+        for (int r = 0; r < m.getRowDimension(); r++)\n+            for (int c = 0; c < m.getColumnDimension(); c++) {\n+                double e = m.getEntry(r, c);\n+                d[r][c] = e * e;\n+            }\n+        return new Array2DRowRealMatrix(d, false);\n+    }\n+\n+    /**\n+     * @param m\n+     *            Input matrix 1.\n+     * @param n\n+     *            Input matrix 2.\n+     * @return Matrix where the elements of m and m are element wise multiplied.\n+     */\n+    private static RealMatrix times(final RealMatrix m, final RealMatrix n) {\n+        double[][] d = new double[m.getRowDimension()][m.getColumnDimension()];\n+        for (int r = 0; r < m.getRowDimension(); r++)\n+            for (int c = 0; c < m.getColumnDimension(); c++)\n+                d[r][c] = m.getEntry(r, c)*n.getEntry(r, c);\n+        return new Array2DRowRealMatrix(d, false);\n+    }\n+\n+    /**\n+     * @param m\n+     *            Input matrix 1.\n+     * @param n\n+     *            Input matrix 2.\n+     * @return Matrix where the elements of m and m are element wise divided.\n+     */\n+    private static RealMatrix divide(final RealMatrix m, final RealMatrix n) {\n+        double[][] d = new double[m.getRowDimension()][m.getColumnDimension()];\n+        for (int r = 0; r < m.getRowDimension(); r++)\n+            for (int c = 0; c < m.getColumnDimension(); c++)\n+                d[r][c] = m.getEntry(r, c)/n.getEntry(r, c);\n+        return new Array2DRowRealMatrix(d, false);\n+    }\n+\n+    /**\n+     * @param m Input matrix.\n+     * @param cols Columns to select.\n+     * @return Matrix representing the selected columns.\n+     */\n+    private static RealMatrix selectColumns(final RealMatrix m, final int[] cols) {\n+        double[][] d = new double[m.getRowDimension()][cols.length];\n+        for (int r = 0; r < m.getRowDimension(); r++)\n+            for (int c = 0; c < cols.length; c++)\n+                d[r][c] = m.getEntry(r, cols[c]);\n+        return new Array2DRowRealMatrix(d, false);\n+    }\n+\n+    /**\n+     * @param m Input matrix.\n+     * @param k diagonal position.\n+     * @return Upper triangular part of matrix.\n+     */\n+    private static RealMatrix triu(final RealMatrix m, int k) {\n+        double[][] d = new double[m.getRowDimension()][m.getColumnDimension()];\n+        for (int r = 0; r < m.getRowDimension(); r++)\n+            for (int c = 0; c < m.getColumnDimension(); c++)\n+                d[r][c] = r <= c - k ? m.getEntry(r, c) : 0;\n+                return new Array2DRowRealMatrix(d, false);\n+    }\n+\n+    /**\n+     * @param m\n+     *            Input matrix.\n+     * @return Norm of the matrix.\n+     */\n+    private static double norm(final RealMatrix m) {\n+        double sum = 0;\n+        for (int r = 0; r < m.getRowDimension(); r++)\n+            for (int c = 0; c < m.getColumnDimension(); c++) {\n+                double e = m.getEntry(r, c);\n+                sum += e*e;\n+            }\n+        return Math.sqrt(sum);\n+    }\n+\n+    /**\n+     * @param m\n+     *            Input matrix.\n+     * @return Row matrix representing the sums of the rows.\n+     */\n+    private static RealMatrix sumRows(final RealMatrix m) {\n+        double[][] d = new double[1][m.getColumnDimension()];\n+        for (int c = 0; c < m.getColumnDimension(); c++) {\n+            double sum = 0;\n+            for (int r = 0; r < m.getRowDimension(); r++)\n+                sum += m.getEntry(r, c);\n+            d[0][c] = sum;\n+        }\n+        return new Array2DRowRealMatrix(d, false);\n+    }\n+\n+    /**\n+     * @param m\n+     *            Input matrix.\n+     * @return Diagonal n X n matrix if m is a column matrix, Rolumn matrix\n+     *         representing the diagonal if m is a nXn matrix.\n+     */\n+    private static RealMatrix diag(final RealMatrix m) {\n+        if (m.getColumnDimension() == 1) {\n+            double[][] d = new double[m.getRowDimension()][m.getRowDimension()];\n+            for (int i = 0; i < m.getRowDimension(); i++)\n+                d[i][i] = m.getEntry(i, 0);\n+            return new Array2DRowRealMatrix(d, false);\n+        } else {\n+            double[][] d = new double[m.getRowDimension()][1];\n+            for (int i = 0; i < m.getColumnDimension(); i++)\n+                d[i][0] = m.getEntry(i, i);\n+            return new Array2DRowRealMatrix(d, false);\n+        }\n+    }\n+\n+    /**\n+     * Copies a row from m1 to m2.\n+     *\n+     * @param m1\n+     *            Source matrix 1.\n+     * @param col1\n+     *            Source column.\n+     * @param m2\n+     *            Target matrix.\n+     * @param col2\n+     *            Target column.\n+     */\n+    private static void copyColumn(final RealMatrix m1, int col1, RealMatrix m2, int col2) {\n+        for (int i = 0; i < m1.getRowDimension(); i++)\n+            m2.setEntry(i, col2, m1.getEntry(i, col1));\n+    }\n+\n+    /**\n+     * @param n\n+     *            Number of rows.\n+     * @param m\n+     *            Number of columns.\n+     * @return n X m matrix of 1.0-values.\n+     */\n+    private static RealMatrix ones(int n, int m) {\n+        double[][] d = new double[n][m];\n+        for (int r = 0; r < n; r++)\n+            Arrays.fill(d[r], 1.0);\n+        return new Array2DRowRealMatrix(d, false);\n+    }\n+\n+    /**\n+     * @param n\n+     *            Number of rows.\n+     * @param m\n+     *            Number of columns.\n+     * @return n X m matrix of 0.0-values, diagonal has values 1.0.\n+     */\n+    private static RealMatrix eye(int n, int m) {\n+        double[][] d = new double[n][m];\n+        for (int r = 0; r < n; r++)\n+            if (r < m)\n+                d[r][r] = 1;\n+        return new Array2DRowRealMatrix(d, false);\n+    }\n+\n+    /**\n+     * @param n\n+     *            Number of rows.\n+     * @param m\n+     *            Number of columns.\n+     * @return n X m matrix of 0.0-values.\n+     */\n+    private static RealMatrix zeros(int n, int m) {\n+        return new Array2DRowRealMatrix(n, m);\n+    }\n+\n+    /**\n+     * @param mat\n+     *            Input matrix.\n+     * @param n\n+     *            Number of row replicates.\n+     * @param m\n+     *            Number of column replicates.\n+     * @return Matrix which replicates the input matrix in both directions.\n+     */\n+    private static RealMatrix repmat(final RealMatrix mat, int n, int m) {\n+        int rd = mat.getRowDimension();\n+        int cd = mat.getColumnDimension();\n+        double[][] d = new double[n * rd][m * cd];\n+        for (int r = 0; r < n * rd; r++)\n+            for (int c = 0; c < m * cd; c++)\n+                d[r][c] = mat.getEntry(r % rd, c % cd);\n+        return new Array2DRowRealMatrix(d, false);\n+    }\n+\n+    /**\n+     * @param start\n+     *            Start value.\n+     * @param end\n+     *            End value.\n+     * @param step\n+     *            Step size.\n+     * @return Sequence as column matrix.\n+     */\n+    private static RealMatrix sequence(double start, double end, double step) {\n+        int size = (int) ((end - start) / step + 1);\n+        double[][] d = new double[size][1];\n+        double value = start;\n+        for (int r = 0; r < size; r++) {\n+            d[r][0] = value;\n+            value += step;\n+        }\n+        return new Array2DRowRealMatrix(d, false);\n+    }\n+\n+    /**\n+     * @param m\n+     *            Input matrix.\n+     * @return Maximum of matrix element values.\n+     */\n+    private static double max(final RealMatrix m) {\n+        double max = -Double.MAX_VALUE;\n+        for (int r = 0; r < m.getRowDimension(); r++)\n+            for (int c = 0; c < m.getColumnDimension(); c++) {\n+                double e = m.getEntry(r, c);\n+                if (max < e)\n+                    max = e;\n+            }\n+        return max;\n+    }\n+\n+    /**\n+     * @param m\n+     *            Input matrix.\n+     * @return Minimum of matrix element values.\n+     */\n+    private static double min(final RealMatrix m) {\n+        double min = Double.MAX_VALUE;\n+        for (int r = 0; r < m.getRowDimension(); r++)\n+            for (int c = 0; c < m.getColumnDimension(); c++) {\n+                double e = m.getEntry(r, c);\n+                if (min > e)\n+                    min = e;\n+            }\n+        return min;\n+    }\n+\n+    /**\n+     * @param m\n+     *            Input array.\n+     * @return Maximum of array values.\n+     */\n+    private static double max(final double[] m) {\n+        double max = -Double.MAX_VALUE;\n+        for (int r = 0; r < m.length; r++)\n+            if (max < m[r])\n+                max = m[r];\n+        return max;\n+    }\n+\n+    /**\n+     * @param m\n+     *            Input array.\n+     * @return Minimum of array values.\n+     */\n+    private static double min(final double[] m) {\n+        double min = Double.MAX_VALUE;\n+        for (int r = 0; r < m.length; r++)\n+            if (min > m[r])\n+                min = m[r];\n+        return min;\n+    }\n+\n+    /**\n+     * @param indices\n+     *            Input index array.\n+     * @return Inverse of the mapping defined by indices\n+     */\n+    private static int[] inverse(final int[] indices) {\n+        int[] inverse = new int[indices.length];\n+        for (int i = 0; i < indices.length; i++)\n+            inverse[indices[i]] = i;\n+        return inverse;\n+    }\n+\n+    /**\n+     * @param indices\n+     *            Input index array.\n+     * @return Indices in inverse order (last is first)\n+     */\n+    private static int[] reverse(final int[] indices) {\n+        int[] reverse = new int[indices.length];\n+        for (int i = 0; i < indices.length; i++)\n+            reverse[i] = indices[indices.length - i - 1];\n+        return reverse;\n+    }\n+\n+    /**\n+     * @param size\n+     *            Length of random array.\n+     * @return Array of gaussian random numbers.\n+     */\n+    private double[] randn(int size) {\n+        double[] randn = new double[size];\n+        for (int i = 0; i < size; i++)\n+            randn[i] = random.nextGaussian();\n+        return randn;\n+    }\n+\n+    /**\n+     * @param size\n+     *            Number of rows.\n+     * @param popSize\n+     *            Population size.\n+     * @return 2-dimensional matrix of gaussian random numbers.\n+     */\n+    private RealMatrix randn1(int size, int popSize) {\n+        double[][] d = new double[size][popSize];\n+        for (int r = 0; r < size; r++)\n+            for (int c = 0; c < popSize; c++)\n+                d[r][c] = random.nextGaussian();\n+        return new Array2DRowRealMatrix(d, false);\n+    }\n+}\n+\n--- /dev/null\n+++ b/src/test/java/org/apache/commons/math/optimization/direct/CMAESOptimizerTest.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.commons.math.optimization.direct;\n+\n+import java.util.Arrays;\n+import java.util.Random;\n+\n+import org.apache.commons.math.MathException;\n+import org.apache.commons.math.analysis.MultivariateRealFunction;\n+import org.apache.commons.math.exception.MathUserException;\n+import org.apache.commons.math.exception.MultiDimensionMismatchException;\n+import org.apache.commons.math.exception.NoDataException;\n+import org.apache.commons.math.exception.NotPositiveException;\n+import org.apache.commons.math.exception.OutOfRangeException;\n+import org.apache.commons.math.optimization.GoalType;\n+import org.apache.commons.math.optimization.MultivariateRealOptimizer;\n+import org.apache.commons.math.optimization.RealPointValuePair;\n+import org.apache.commons.math.random.MersenneTwister;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+/**\n+ * Test for {@link CMAESOptimizer}.\n+ */\n+public class CMAESOptimizerTest {\n+\n+    static final int DIM = 13;\n+    static final int LAMBDA = 4 + (int)(3.*Math.log(DIM));\n+   \n+    @Test(expected = OutOfRangeException.class)\n+    public void testInitOutofbounds() throws MathUserException, MathException {\n+        double[] startPoint = point(DIM,3);\n+        double[] insigma = null;\n+        double[][] boundaries = boundaries(DIM,-1,2);\n+        RealPointValuePair expected =\n+            new RealPointValuePair(point(DIM,1.0),0.0);\n+        doTest(new Rosen(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,\n+                1e-13, 1e-6, 100000, expected);\n+    }\n+    \n+    @Test(expected = MultiDimensionMismatchException.class)\n+    public void testBoundariesDimensionMismatch() throws MathUserException, MathException {\n+        double[] startPoint = point(DIM,0.5);\n+        double[] insigma = null;\n+        double[][] boundaries = boundaries(DIM+1,-1,2);\n+        RealPointValuePair expected =\n+            new RealPointValuePair(point(DIM,1.0),0.0);\n+        doTest(new Rosen(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,\n+                1e-13, 1e-6, 100000, expected);\n+    }\n+\n+    @Test(expected = NoDataException.class)\n+    public void testBoundariesNoData() throws MathUserException, MathException {\n+        double[] startPoint = point(DIM,0.5);\n+        double[] insigma = null;\n+        double[][] boundaries = boundaries(DIM,-1,2);\n+        boundaries[1] = null;\n+        RealPointValuePair expected =\n+            new RealPointValuePair(point(DIM,1.0),0.0);\n+        doTest(new Rosen(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,\n+                1e-13, 1e-6, 100000, expected);\n+    }\n+\n+    @Test(expected = NotPositiveException.class)\n+    public void testInputSigmaNegative() throws MathUserException, MathException {\n+        double[] startPoint = point(DIM,0.5);\n+        double[] insigma = point(DIM,-0.5);\n+        double[][] boundaries = null;\n+        RealPointValuePair expected =\n+            new RealPointValuePair(point(DIM,1.0),0.0);\n+        doTest(new Rosen(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,\n+                1e-13, 1e-6, 100000, expected);\n+    }\n+\n+    @Test(expected = OutOfRangeException.class)\n+    public void testInputSigmaOutOfRange() throws MathUserException, MathException {\n+        double[] startPoint = point(DIM,0.5);\n+        double[] insigma = point(DIM, 1.1);\n+        double[][] boundaries = boundaries(DIM,-1,2);\n+        RealPointValuePair expected =\n+            new RealPointValuePair(point(DIM,1.0),0.0);\n+        doTest(new Rosen(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,\n+                1e-13, 1e-6, 100000, expected);\n+    }\n+\n+    @Test(expected = MultiDimensionMismatchException.class)\n+    public void testInputSigmaDimensionMismatch() throws MathUserException, MathException {\n+        double[] startPoint = point(DIM,0.5);\n+        double[] insigma = point(DIM+1,-0.5);\n+        double[][] boundaries = null;;\n+        RealPointValuePair expected =\n+            new RealPointValuePair(point(DIM,1.0),0.0);\n+        doTest(new Rosen(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,\n+                1e-13, 1e-6, 100000, expected);\n+    }\n+    \n+    @Test\n+    public void testRosen() throws MathException {\n+        double[] startPoint = point(DIM,0.1);\n+        double[] insigma = point(DIM,0.1);\n+        double[][] boundaries = null;\n+        RealPointValuePair expected =\n+            new RealPointValuePair(point(DIM,1.0),0.0);\n+        doTest(new Rosen(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,\n+                1e-13, 1e-6, 100000, expected);\n+        doTest(new Rosen(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, LAMBDA, false, 0, 1e-13,\n+                1e-13, 1e-6, 100000, expected);\n+    }\n+\n+    @Test\n+    public void testMaximize() throws MathException {\n+        double[] startPoint = point(DIM,1.0);\n+        double[] insigma = point(DIM,0.1);\n+        double[][] boundaries = null;\n+        RealPointValuePair expected =\n+            new RealPointValuePair(point(DIM,0.0),1.0);\n+        doTest(new MinusElli(), startPoint, insigma, boundaries,\n+                GoalType.MAXIMIZE, LAMBDA, true, 0, 1.0-1e-13,\n+                2e-10, 5e-6, 100000, expected);\n+        doTest(new MinusElli(), startPoint, insigma, boundaries,\n+                GoalType.MAXIMIZE, LAMBDA, false, 0, 1.0-1e-13,\n+                2e-10, 5e-6, 100000, expected);\n+        boundaries = boundaries(DIM,-0.3,0.3); \n+        startPoint = point(DIM,0.1);\n+        doTest(new MinusElli(), startPoint, insigma, boundaries,\n+                GoalType.MAXIMIZE, LAMBDA, true, 0, 1.0-1e-13,\n+                2e-10, 5e-6, 100000, expected);\n+    }\n+\n+    @Test\n+    public void testEllipse() throws MathException {\n+        double[] startPoint = point(DIM,1.0);\n+        double[] insigma = point(DIM,0.1);\n+        double[][] boundaries = null;\n+        RealPointValuePair expected =\n+            new RealPointValuePair(point(DIM,0.0),0.0);\n+        doTest(new Elli(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,\n+                1e-13, 1e-6, 100000, expected);\n+        doTest(new Elli(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, LAMBDA, false, 0, 1e-13,\n+                1e-13, 1e-6, 100000, expected);\n+    }\n+\n+    @Test\n+    public void testElliRotated() throws MathException {\n+        double[] startPoint = point(DIM,1.0);\n+        double[] insigma = point(DIM,0.1);\n+        double[][] boundaries = null;\n+        RealPointValuePair expected =\n+            new RealPointValuePair(point(DIM,0.0),0.0);\n+        doTest(new ElliRotated(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,\n+                1e-13, 1e-6, 100000, expected);\n+        doTest(new ElliRotated(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, LAMBDA, false, 0, 1e-13,\n+                1e-13, 1e-6, 100000, expected);\n+    }\n+\n+    @Test\n+    public void testCigar() throws MathException {\n+        double[] startPoint = point(DIM,1.0);\n+        double[] insigma = point(DIM,0.1);\n+        double[][] boundaries = null;\n+        RealPointValuePair expected =\n+            new RealPointValuePair(point(DIM,0.0),0.0);\n+        doTest(new Cigar(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,\n+                1e-13, 1e-6, 200000, expected);\n+        doTest(new Cigar(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, LAMBDA, false, 0, 1e-13,\n+                1e-13, 1e-6, 100000, expected);\n+    }\n+\n+    @Test\n+    public void testTwoAxes() throws MathException {\n+        double[] startPoint = point(DIM,1.0);\n+        double[] insigma = point(DIM,0.1);\n+        double[][] boundaries = null;\n+        RealPointValuePair expected =\n+            new RealPointValuePair(point(DIM,0.0),0.0);\n+        doTest(new TwoAxes(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, 2*LAMBDA, true, 0, 1e-13,\n+                1e-13, 1e-6, 200000, expected);\n+        doTest(new TwoAxes(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, 2*LAMBDA, false, 0, 1e-13,\n+                1e-8, 1e-3, 200000, expected);\n+    }\n+\n+    @Test\n+    public void testCigTab() throws MathException {\n+        double[] startPoint = point(DIM,1.0);\n+        double[] insigma = point(DIM,0.3);\n+        double[][] boundaries = null;\n+        RealPointValuePair expected =\n+            new RealPointValuePair(point(DIM,0.0),0.0);\n+        doTest(new CigTab(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,\n+                1e-13, 5e-5, 100000, expected);\n+        doTest(new CigTab(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, LAMBDA, false, 0, 1e-13,\n+                1e-13, 5e-5, 100000, expected);\n+    }\n+\n+    @Test\n+    public void testSphere() throws MathException {\n+        double[] startPoint = point(DIM,1.0);\n+        double[] insigma = point(DIM,0.1);\n+        double[][] boundaries = null;\n+        RealPointValuePair expected =\n+            new RealPointValuePair(point(DIM,0.0),0.0);\n+        doTest(new Sphere(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,\n+                1e-13, 1e-6, 100000, expected);\n+        doTest(new Sphere(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, LAMBDA, false, 0, 1e-13,\n+                1e-13, 1e-6, 100000, expected);\n+    }\n+\n+    @Test\n+    public void testTablet() throws MathException {\n+        double[] startPoint = point(DIM,1.0);\n+        double[] insigma = point(DIM,0.1);\n+        double[][] boundaries = null;\n+        RealPointValuePair expected =\n+            new RealPointValuePair(point(DIM,0.0),0.0);\n+        doTest(new Tablet(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, LAMBDA, true, 0, 1e-13,\n+                1e-13, 1e-6, 100000, expected);\n+        doTest(new Tablet(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, LAMBDA, false, 0, 1e-13,\n+                1e-13, 1e-6, 100000, expected);\n+    }\n+\n+    @Test\n+    public void testDiffPow() throws MathException {\n+        double[] startPoint = point(DIM,1.0);\n+        double[] insigma = point(DIM,0.1);\n+        double[][] boundaries = null;\n+        RealPointValuePair expected =\n+            new RealPointValuePair(point(DIM,0.0),0.0);\n+        doTest(new DiffPow(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, 10, true, 0, 1e-13,\n+                1e-8, 1e-1, 100000, expected);\n+        doTest(new DiffPow(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, 10, false, 0, 1e-13,\n+                1e-8, 2e-1, 100000, expected);\n+    }\n+\n+    @Test\n+    public void testSsDiffPow() throws MathException {\n+        double[] startPoint = point(DIM,1.0);\n+        double[] insigma = point(DIM,0.1);\n+        double[][] boundaries = null;\n+        RealPointValuePair expected =\n+            new RealPointValuePair(point(DIM,0.0),0.0);\n+        doTest(new SsDiffPow(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, 10, true, 0, 1e-13,\n+                1e-4, 1e-1, 200000, expected);\n+        doTest(new SsDiffPow(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, 10, false, 0, 1e-13,\n+                1e-4, 1e-1, 200000, expected);\n+    }\n+\n+    @Test\n+    public void testAckley() throws MathException {\n+        double[] startPoint = point(DIM,1.0);\n+        double[] insigma = point(DIM,1.0);\n+        double[][] boundaries = null;\n+        RealPointValuePair expected =\n+            new RealPointValuePair(point(DIM,0.0),0.0);\n+        doTest(new Ackley(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, 2*LAMBDA, true, 0, 1e-13,\n+                1e-9, 1e-5, 100000, expected);\n+        doTest(new Ackley(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, 2*LAMBDA, false, 0, 1e-13,\n+                1e-9, 1e-5, 100000, expected);\n+    }\n+\n+    @Test\n+    public void testRastrigin() throws MathException {\n+        double[] startPoint = point(DIM,0.1);\n+        double[] insigma = point(DIM,0.1);\n+        double[][] boundaries = null;\n+        RealPointValuePair expected =\n+            new RealPointValuePair(point(DIM,0.0),0.0);\n+        doTest(new Rastrigin(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, (int)(200*Math.sqrt(DIM)), true, 0, 1e-13,\n+                1e-13, 1e-6, 200000, expected);\n+        doTest(new Rastrigin(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, (int)(200*Math.sqrt(DIM)), false, 0, 1e-13,\n+                1e-13, 1e-6, 200000, expected);\n+    }\n+\n+    @Test\n+    public void testConstrainedRosen() throws MathException {\n+        double[] startPoint = point(DIM,0.1);\n+        double[] insigma = point(DIM,0.1);\n+        double[][] boundaries = boundaries(DIM,-1,2);\n+        RealPointValuePair expected =\n+            new RealPointValuePair(point(DIM,1.0),0.0);\n+        doTest(new Rosen(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, 2*LAMBDA, true, 0, 1e-13,\n+                1e-13, 1e-6, 100000, expected);\n+        doTest(new Rosen(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, 2*LAMBDA, false, 0, 1e-13,\n+                1e-13, 1e-6, 100000, expected);\n+    }\n+\n+    @Test\n+    public void testDiagonalRosen() throws MathException {\n+        double[] startPoint = point(DIM,0.1);\n+        double[] insigma = point(DIM,0.1);\n+        double[][] boundaries = null;\n+        RealPointValuePair expected =\n+            new RealPointValuePair(point(DIM,1.0),0.0);\n+        doTest(new Rosen(), startPoint, insigma, boundaries,\n+                GoalType.MINIMIZE, LAMBDA, false, 1, 1e-13,\n+                1e-10, 1e-4, 1000000, expected);\n+     }\n+\n+    /**\n+     * @param func Function to optimize.\n+     * @param startPoint Starting point.\n+     * @param inSigma Individual input sigma.\n+     * @param boundaries Upper / lower point limit.\n+     * @param goal Minimization or maximization.\n+     * @param lambda Population size used for offspring.\n+     * @param isActive Covariance update mechanism.\n+     * @param diagonalOnly Simplified covariance update.\n+     * @param stopValue Termination criteria for optimization.\n+     * @param fTol Tolerance relative error on the objective function.\n+     * @param pointTol Tolerance for checking that the optimum is correct.\n+     * @param maxEvaluations Maximum number of evaluations.\n+     * @param expected Expected point / value.\n+     */\n+    private void doTest(MultivariateRealFunction func,\n+            double[] startPoint,\n+            double[] inSigma,\n+            double[][] boundaries,\n+            GoalType goal,\n+            int lambda,\n+            boolean isActive,\n+            int diagonalOnly, \n+            double stopValue,\n+            double fTol,\n+            double pointTol,\n+            int maxEvaluations,\n+            RealPointValuePair expected)\n+    throws MathException {\n+        int dim = startPoint.length;\n+        // test diagonalOnly = 0 - slow but normally fewer feval#\n+        MultivariateRealOptimizer optim =\n+            new CMAESOptimizer(\n+                    lambda, inSigma, boundaries, 30000,\n+                    stopValue, isActive, diagonalOnly, 0, new MersenneTwister(),false);\n+        RealPointValuePair result = optim.optimize(maxEvaluations, func, goal, startPoint);\n+        Assert.assertEquals(expected.getValue(),\n+                result.getValue(), fTol);\n+        for (int i = 0; i < dim; i++) {\n+            Assert.assertEquals(expected.getPoint()[i],\n+                    result.getPoint()[i], pointTol);\n+        }\n+    }\n+\n+    private static double[] point(int n, double value) {\n+        double[] ds = new double[n];\n+        Arrays.fill(ds, value);\n+        return ds;\n+    }\n+\n+    private static double[][] boundaries(int dim,\n+            double lower, double upper) {\n+        double[][] boundaries = new double[2][dim];\n+        for (int i = 0; i < dim; i++)\n+            boundaries[0][i] = lower;\n+        for (int i = 0; i < dim; i++)\n+            boundaries[1][i] = upper;\n+        return boundaries;\n+    }\n+\n+    private static class Sphere implements MultivariateRealFunction {\n+\n+        public double value(double[] x) {\n+            double f = 0;\n+            for (int i = 0; i < x.length; ++i)\n+                f += x[i] * x[i];\n+            return f;\n+        }\n+    }\n+\n+    private static class Cigar implements MultivariateRealFunction {\n+        private double factor;\n+\n+        Cigar() {\n+            this(1e3);\n+        }\n+\n+        Cigar(double axisratio) {\n+            factor = axisratio * axisratio;\n+        }\n+\n+        public double value(double[] x) {\n+            double f = x[0] * x[0];\n+            for (int i = 1; i < x.length; ++i)\n+                f += factor * x[i] * x[i];\n+            return f;\n+        }\n+    }\n+\n+    private static class Tablet implements MultivariateRealFunction {\n+        private double factor;\n+\n+        Tablet() {\n+            this(1e3);\n+        }\n+\n+        Tablet(double axisratio) {\n+            factor = axisratio * axisratio;\n+        }\n+\n+        public double value(double[] x) {\n+            double f = factor * x[0] * x[0];\n+            for (int i = 1; i < x.length; ++i)\n+                f += x[i] * x[i];\n+            return f;\n+        }\n+    }\n+\n+    private static class CigTab implements MultivariateRealFunction {\n+        private double factor;\n+\n+        CigTab() {\n+            this(1e4);\n+        }\n+\n+        CigTab(double axisratio) {\n+            factor = axisratio;\n+        }\n+\n+        public double value(double[] x) {\n+            int end = x.length - 1;\n+            double f = x[0] * x[0] / factor + factor * x[end] * x[end];\n+            for (int i = 1; i < end; ++i)\n+                f += x[i] * x[i];\n+            return f;\n+        }\n+    }\n+\n+    private static class TwoAxes implements MultivariateRealFunction {\n+\n+        private double factor;\n+\n+        TwoAxes() {\n+            this(1e6);\n+        }\n+\n+        TwoAxes(double axisratio) {\n+            factor = axisratio * axisratio;\n+        }\n+\n+        public double value(double[] x) {\n+            double f = 0;\n+            for (int i = 0; i < x.length; ++i)\n+                f += (i < x.length / 2 ? factor : 1) * x[i] * x[i];\n+            return f;\n+        }\n+    }\n+\n+    private static class ElliRotated implements MultivariateRealFunction {\n+        private Basis B = new Basis();\n+        private double factor;\n+\n+        ElliRotated() {\n+            this(1e3);\n+        }\n+\n+        ElliRotated(double axisratio) {\n+            factor = axisratio * axisratio;\n+        }\n+\n+        public double value(double[] x) {\n+            double f = 0;\n+            x = B.Rotate(x);\n+            for (int i = 0; i < x.length; ++i)\n+                f += Math.pow(factor, i / (x.length - 1.)) * x[i] * x[i];\n+            return f;\n+        }\n+    }\n+\n+    private static class Elli implements MultivariateRealFunction {\n+\n+        private double factor;\n+\n+        Elli() {\n+            this(1e3);\n+        }\n+\n+        Elli(double axisratio) {\n+            factor = axisratio * axisratio;\n+        }\n+\n+        public double value(double[] x) {\n+            double f = 0;\n+            for (int i = 0; i < x.length; ++i)\n+                f += Math.pow(factor, i / (x.length - 1.)) * x[i] * x[i];\n+            return f;\n+        }\n+    }\n+\n+    private static class MinusElli implements MultivariateRealFunction {\n+\n+        public double value(double[] x) {\n+            return 1.0-(new Elli().value(x));\n+        }\n+    }\n+\n+    private static class DiffPow implements MultivariateRealFunction {\n+\n+        public double value(double[] x) {\n+            double f = 0;\n+            for (int i = 0; i < x.length; ++i)\n+                f += Math.pow(Math.abs(x[i]), 2. + 10 * (double) i\n+                        / (x.length - 1.));\n+            return f;\n+        }\n+    }\n+\n+    private static class SsDiffPow implements MultivariateRealFunction {\n+\n+        public double value(double[] x) {\n+            double f = Math.pow(new DiffPow().value(x), 0.25);\n+            return f;\n+        }\n+    }\n+\n+    private static class Rosen implements MultivariateRealFunction {\n+\n+        public double value(double[] x) {\n+            double f = 0;\n+            for (int i = 0; i < x.length - 1; ++i)\n+                f += 1e2 * (x[i] * x[i] - x[i + 1]) * (x[i] * x[i] - x[i + 1])\n+                + (x[i] - 1.) * (x[i] - 1.);\n+            return f;\n+        }\n+    }\n+\n+    private static class Ackley implements MultivariateRealFunction {\n+        private double axisratio;\n+\n+        Ackley(double axra) {\n+            axisratio = axra;\n+        }\n+\n+        public Ackley() {\n+            this(1);\n+        }\n+\n+        public double value(double[] x) {\n+            double f = 0;\n+            double res2 = 0;\n+            double fac = 0;\n+            for (int i = 0; i < x.length; ++i) {\n+                fac = Math.pow(axisratio, (i - 1.) / (x.length - 1.));\n+                f += fac * fac * x[i] * x[i];\n+                res2 += Math.cos(2. * Math.PI * fac * x[i]);\n+            }\n+            f = (20. - 20. * Math.exp(-0.2 * Math.sqrt(f / x.length))\n+                    + Math.exp(1.) - Math.exp(res2 / x.length));\n+            return f;\n+        }\n+    }\n+\n+    private static class Rastrigin implements MultivariateRealFunction {\n+\n+        private double axisratio;\n+        private double amplitude;\n+\n+        Rastrigin() {\n+            this(1, 10);\n+        }\n+\n+        Rastrigin(double axisratio, double amplitude) {\n+            this.axisratio = axisratio;\n+            this.amplitude = amplitude;\n+        }\n+\n+        public double value(double[] x) {\n+            double f = 0;\n+            double fac;\n+            for (int i = 0; i < x.length; ++i) {\n+                fac = Math.pow(axisratio, (i - 1.) / (x.length - 1.));\n+                if (i == 0 && x[i] < 0)\n+                    fac *= 1.;\n+                f += fac * fac * x[i] * x[i] + amplitude\n+                * (1. - Math.cos(2. * Math.PI * fac * x[i]));\n+            }\n+            return f;\n+        }\n+    }\n+\n+    private static class Basis {\n+        double[][] basis;\n+        Random rand = new Random(2); // use not always the same basis\n+\n+        double[] Rotate(double[] x) {\n+            GenBasis(x.length);\n+            double[] y = new double[x.length];\n+            for (int i = 0; i < x.length; ++i) {\n+                y[i] = 0;\n+                for (int j = 0; j < x.length; ++j)\n+                    y[i] += basis[i][j] * x[j];\n+            }\n+            return y;\n+        }\n+\n+        void GenBasis(int DIM) {\n+            if (basis != null ? basis.length == DIM : false)\n+                return;\n+\n+            double sp;\n+            int i, j, k;\n+\n+            /* generate orthogonal basis */\n+            basis = new double[DIM][DIM];\n+            for (i = 0; i < DIM; ++i) {\n+                /* sample components gaussian */\n+                for (j = 0; j < DIM; ++j)\n+                    basis[i][j] = rand.nextGaussian();\n+                /* substract projection of previous vectors */\n+                for (j = i - 1; j >= 0; --j) {\n+                    for (sp = 0., k = 0; k < DIM; ++k)\n+                        sp += basis[i][k] * basis[j][k]; /* scalar product */\n+                    for (k = 0; k < DIM; ++k)\n+                        basis[i][k] -= sp * basis[j][k]; /* substract */\n+                }\n+                /* normalize */\n+                for (sp = 0., k = 0; k < DIM; ++k)\n+                    sp += basis[i][k] * basis[i][k]; /* squared norm */\n+                for (k = 0; k < DIM; ++k)\n+                    basis[i][k] /= Math.sqrt(sp);\n+            }\n+        }\n+    }\n+}", "timestamp": 1296837368, "metainfo": ""}