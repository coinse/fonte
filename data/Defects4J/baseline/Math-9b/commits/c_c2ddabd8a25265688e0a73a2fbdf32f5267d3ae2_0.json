{"sha": "c2ddabd8a25265688e0a73a2fbdf32f5267d3ae2", "log": "Added Miller updating regression implementation.  JIRA: MATH-607.  Contributed by Greg Sterijevski.  ", "commit": "\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/math/stat/regression/MillerUpdatingRegression.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.commons.math.stat.regression;\n+\n+import java.util.Arrays;\n+import org.apache.commons.math.MathException;\n+import org.apache.commons.math.exception.util.DummyLocalizable;\n+import org.apache.commons.math.exception.util.Localizable;\n+import org.apache.commons.math.util.FastMath;\n+import org.apache.commons.math.util.MathUtils;\n+\n+/**\n+ * <p>This class is a concrete implementation of the {@link UpdatingMultipleLinearRegression} interface.</p>\n+ *\n+ * <p>The algorithm is described in: <pre>\n+ * Algorithm AS 274: Least Squares Routines to Supplement Those of Gentleman\n+ * Author(s): Alan J. Miller\n+ * Source: Journal of the Royal Statistical Society.\n+ * Series C (Applied Statistics), Vol. 41, No. 2\n+ * (1992), pp. 458-478\n+ * Published by: Blackwell Publishing for the Royal Statistical Society\n+ * Stable URL: http://www.jstor.org/stable/2347583 </pre></p>\n+ *\n+ * <p>This method for multiple regression forms the solution to the OLS problem\n+ * by updating the QR decomposition as described by Gentleman.</p>\n+ *\n+ * @version $Id$\n+ * @since 3.0\n+ */\n+public class MillerUpdatingRegression implements UpdatingMultipleLinearRegression {\n+\n+    private final int nvars;\n+    private final double[] d;\n+    private final double[] rhs;\n+    private final double[] r;\n+    private final double[] tol;\n+    private final double[] rss;\n+    private final int[] vorder;\n+    private final double[] work_tolset;\n+    private long nobs = 0;\n+    private double sserr = 0.0;\n+    private boolean rss_set = false;\n+    private boolean tol_set = false;\n+    private final boolean[] lindep;\n+    private final double[] x_sing;\n+    private final double[] work_sing;\n+    private double sumy = 0.0;\n+    private double sumsqy = 0.0;\n+    private boolean hasIntercept;\n+    private final double epsilon;\n+\n+    /**\n+     *  Set the default constructor to private access\n+     *  to prevent inadvertent instantiation\n+     */\n+    @SuppressWarnings(\"unused\")\n+    private MillerUpdatingRegression() {\n+        this.d = null;\n+        this.hasIntercept = false;\n+        this.lindep = null;\n+        this.nobs = -1;\n+        this.nvars = -1;\n+        this.r = null;\n+        this.rhs = null;\n+        this.rss = null;\n+        this.rss_set = false;\n+        this.sserr = Double.NaN;\n+        this.sumsqy = Double.NaN;\n+        this.sumy = Double.NaN;\n+        this.tol = null;\n+        this.tol_set = false;\n+        this.vorder = null;\n+        this.work_sing = null;\n+        this.work_tolset = null;\n+        this.x_sing = null;\n+        this.epsilon = Double.NaN;\n+    }\n+\n+    public MillerUpdatingRegression(int numberOfVariables, boolean includeConstant, double errorTolerance) {\n+        if (numberOfVariables < 1) {\n+            throw new IllegalArgumentException(\"NumberOfVariables must be greater than or equal to one\");\n+        }\n+        if (includeConstant) {\n+            this.nvars = numberOfVariables + 1;\n+        } else {\n+            this.nvars = numberOfVariables;\n+        }\n+        this.hasIntercept = includeConstant;\n+        this.nobs = 0;\n+        this.d = new double[this.nvars];\n+        this.rhs = new double[this.nvars];\n+        this.r = new double[this.nvars * (this.nvars - 1) / 2];\n+        this.tol = new double[this.nvars];\n+        this.rss = new double[this.nvars];\n+        this.vorder = new int[this.nvars];\n+        this.x_sing = new double[this.nvars];\n+        this.work_sing = new double[this.nvars];\n+        this.work_tolset = new double[this.nvars];\n+        this.lindep = new boolean[this.nvars];\n+        for (int i = 0; i < this.nvars; i++) {\n+            vorder[i] = i;\n+        }\n+        if (errorTolerance > 0) {\n+            this.epsilon = errorTolerance;\n+        } else {\n+            this.epsilon = -errorTolerance;\n+        }\n+        return;\n+    }\n+\n+    public MillerUpdatingRegression(int numberOfVariables, boolean includeConstant) {\n+        this(numberOfVariables, includeConstant, MathUtils.EPSILON);\n+    }\n+\n+    public boolean hasIntercept() {\n+        return this.hasIntercept;\n+    }\n+\n+    public long getN() {\n+        return this.nobs;\n+    }\n+\n+    public void addObservation(final double[] x, final double y) {\n+\n+        if ((!this.hasIntercept && x.length != nvars) ||\n+               (this.hasIntercept && x.length + 1 != nvars)) {\n+            throw new IllegalArgumentException(\"Length of regressor list is less that numberOfVariables\");\n+        }\n+        if (!this.hasIntercept) {\n+            include(Arrays.copyOf(x, x.length), 1.0, y);\n+        } else {\n+            double[] tmp = new double[x.length + 1];\n+            System.arraycopy(x, 0, tmp, 1, x.length);\n+            tmp[0] = 1.0;\n+            include(tmp, 1.0, y);\n+        }\n+        ++nobs;\n+        return;\n+\n+    }\n+\n+    public void addObservations(double[][] x, double[] y) {\n+        if (x.length != y.length) {\n+            throw new IllegalArgumentException(\"Lengths of x and y matrices must be equal\");\n+        }\n+        for (int i = 0; i < x.length; i++) {\n+            this.addObservation(x[i], y[i]);\n+        }\n+        return;\n+    }\n+\n+    /*\n+     * The include method is where the QR decomposition occurs. This statement forms all\n+     * intermediate data which will be used for all derivative measures.\n+     * According to the miller paper, note that in the original implementation the x vector\n+     * is overwritten. In this implementation, the include method is passed a copy of the\n+     * original data vector so that there is no contamination of the data. Additionally,\n+     * this method differs slighlty from gentleman's method, in that the assumption is\n+     * of dense design matrices, there is some advantage in using the original gentleman algorithm\n+     * on sparse matrices.\n+     */\n+    private void include(final double[] x, final double wi, final double yi) {\n+        int nextr = 0;\n+        double w = wi;\n+        double y = yi;\n+        double xi;\n+        double di;\n+        double wxi;\n+        double dpi;\n+        double xk;\n+        double _w;\n+        this.rss_set = false;\n+        sumy = smartAdd(yi, sumy);\n+        sumsqy = smartAdd(sumsqy, yi * yi);\n+        for (int i = 0; i < x.length; i++) {\n+            if (w == 0.0) {\n+                return;\n+            }\n+            xi = x[i];\n+\n+            if (xi == 0.0) {\n+                nextr += nvars - i - 1;\n+                continue;\n+            }\n+            di = d[i];\n+            wxi = w * xi;\n+            _w = w;\n+            if (di != 0.0) {\n+                dpi = smartAdd(di, wxi * xi);\n+                double tmp = wxi * xi / di;\n+                if (FastMath.abs(tmp) > MathUtils.EPSILON) {\n+                    w = (di * w) / dpi;\n+                }\n+            } else {\n+                dpi = wxi * xi;\n+                w = 0.0;\n+            }\n+            d[i] = dpi;\n+            for (int k = i + 1; k < nvars; k++) {\n+                xk = x[k];\n+\n+                x[k] = smartAdd(xk, -xi * r[nextr]);\n+                if (di != 0.0) {\n+                    r[nextr] = smartAdd(di * r[nextr], (_w * xi) * xk) / dpi;\n+                } else {\n+                    r[nextr] = xk / xi;\n+                }\n+                ++nextr;\n+            }\n+            xk = y;\n+            y = smartAdd(xk, -xi * rhs[i]);\n+            if (di != 0.0) {\n+                rhs[i] = smartAdd(di * rhs[i], wxi * xk) / dpi;\n+            } else {\n+                rhs[i] = xk / xi;\n+            }\n+        }\n+        sserr = smartAdd(sserr, w * y * y);\n+        return;\n+    }\n+\n+    private double smartAdd(double a, double b) {\n+        double _a = FastMath.abs(a);\n+        double _b = FastMath.abs(b);\n+        if (_a > _b) {\n+            double eps = _a * MathUtils.EPSILON;\n+            if (_b > eps) {\n+                return a + b;\n+            }\n+            return a;\n+        } else {\n+            double eps = _b * MathUtils.EPSILON;\n+            if (_a > eps) {\n+                return a + b;\n+            }\n+            return b;\n+        }\n+    }\n+\n+    /*\n+     * As the name suggest clear, wipes the internals and reoders everything in the\n+     * canonical order.\n+     */\n+    public void clear() {\n+        Arrays.fill(d, 0.0);\n+        Arrays.fill(rhs, 0.0);\n+        Arrays.fill(r, 0.0);\n+        Arrays.fill(tol, 0.0);\n+        Arrays.fill(rss, 0.0);\n+        Arrays.fill(this.work_tolset, 0.0);\n+        Arrays.fill(this.work_sing, 0.0);\n+        Arrays.fill(this.x_sing, 0.0);\n+        Arrays.fill(lindep, false);\n+        for (int i = 0; i < nvars; i++) {\n+            vorder[i] = i;\n+        }\n+\n+        nobs = 0;\n+        sserr = 0.0;\n+        sumy = 0.0;\n+        sumsqy = 0.0;\n+        rss_set = false;\n+        tol_set = false;\n+        return;\n+    }\n+\n+    /*\n+     * This sets up tolerances for singularity testing\n+     */\n+    private void tolset() {\n+        int pos;\n+        double total;\n+        final double eps = this.epsilon;\n+        for (int i = 0; i < nvars; i++) {\n+            this.work_tolset[i] = Math.sqrt(d[i]);\n+        }\n+        tol[0] = eps * this.work_tolset[0];\n+        for (int col = 1; col < nvars; col++) {\n+            pos = col - 1;\n+            total = work_tolset[col];\n+            for (int row = 0; row < col; row++) {\n+                total += Math.abs(r[pos]) * work_tolset[row];\n+                pos += nvars - row - 2;\n+            }\n+            tol[col] = eps * total;\n+        }\n+        tol_set = true;\n+        return;\n+    }\n+\n+    /*\n+     * The regcf methods conducts the linear regression and extracts the\n+     * parameter vector. Notice that the algorithm can do subset regression\n+     * with no alteration.\n+     */\n+    private double[] regcf(int nreq) {\n+        int nextr;\n+        if (nreq < 1 || nreq > this.nvars) {\n+            throw new IllegalArgumentException(\"Number of regressors not correct\");\n+        }\n+        if (!this.tol_set) {\n+            tolset();\n+        }\n+        double[] ret = new double[nreq];\n+        boolean rankProblem = false;\n+        for (int i = nreq - 1; i > -1; i--) {\n+            if (Math.sqrt(d[i]) < tol[i]) {\n+                ret[i] = 0.0;\n+                d[i] = 0.0;\n+                rankProblem = true;\n+            } else {\n+                ret[i] = rhs[i];\n+                nextr = i * (nvars + nvars - i - 1) / 2;\n+                for (int j = i + 1; j < nreq; j++) {\n+\n+                    ret[i] = smartAdd(ret[i], -r[nextr] * ret[j]);\n+\n+                    ++nextr;\n+                }\n+            }\n+        }\n+        if (rankProblem) {\n+            for (int i = 0; i < nreq; i++) {\n+                if (this.lindep[i]) {\n+                    ret[i] = Double.NaN;\n+                }\n+            }\n+        }\n+        return ret;\n+    }\n+\n+    /*\n+     * The method which checks for singularities and then eliminates the offending\n+     * columns\n+     */\n+    private void singcheck() {\n+        double temp;\n+        double y;\n+        double weight;\n+        int pos;\n+\n+        for (int i = 0; i < nvars; i++) {\n+            work_sing[i] = Math.sqrt(d[i]);\n+        }\n+\n+        for (int col = 0; col < nvars; col++) {\n+            // Set elements within R to zero if they are less than tol(col) in\n+            // absolute value after being scaled by the square root of their row\n+            // multiplier\n+            temp = tol[col];\n+            pos = col - 1;\n+            for (int row = 0; row < col - 1; row++) {\n+                if (Math.abs(r[pos]) * work_sing[row] < temp) {\n+                    r[pos] = 0.0;\n+                }\n+                pos += nvars - row - 2;\n+            }\n+            // If diagonal element is near zero, set it to zero, set appropriate\n+            // element of LINDEP, and use INCLUD to augment the projections in\n+            // the lower rows of the orthogonalization.\n+            lindep[col] = false;\n+            if (work_sing[col] < temp) {\n+                lindep[col] = true;\n+                if (col < nvars - 1) {\n+                    Arrays.fill(x_sing, 0.0);\n+                    int _pi = col * (nvars + nvars - col - 1) / 2;\n+                    for (int _xi = col + 1; _xi < nvars; _xi++, _pi++) {\n+                        x_sing[_xi] = r[_pi];\n+                        r[_pi] = 0.0;\n+                    }\n+                    y = rhs[col];\n+                    weight = d[col];\n+                    d[col] = 0.0;\n+                    rhs[col] = 0.0;\n+                    this.include(x_sing, weight, y);\n+                } else {\n+                    sserr += d[col] * rhs[col] * rhs[col];\n+                }\n+            }\n+        }\n+        return;\n+    }\n+\n+    /*\n+     * Calculates the sum of squared errors for the full regression\n+     * and all subsets in the following manner:\n+     * rss[] ={\n+     * ResidualSumOfSquares_allNvars,\n+     * ResidualSumOfSquares_FirstNvars-1,\n+     * ResidualSumOfSquares_FirstNvars-2,\n+     * ..., ResidualSumOfSquares_FirstVariable}\n+     */\n+    private void ss() {\n+        double total = sserr;\n+        rss[nvars - 1] = sserr;\n+        for (int i = nvars - 1; i > 0; i--) {\n+            total += d[i] * rhs[i] * rhs[i];\n+            rss[i - 1] = total;\n+        }\n+        rss_set = true;\n+        return;\n+    }\n+\n+    /*\n+     * Calculates the cov matrix assuming only the first nreq variables are\n+     * included in the calculation. The returned array contains a symmetric\n+     * matrix stored in lower triangular form. The matrix will have\n+     * ( nreq + 1 ) * nreq / 2 elements. For illustration\n+     * cov =\n+     * {\n+     *  cov_00,\n+     *  cov_10, cov_11,\n+     *  cov_20, cov_21, cov22,\n+     *  ...\n+     * }\n+     */\n+    private double[] cov(int nreq) {\n+        if (this.nobs <= nreq) {\n+            return null;\n+        }\n+        double rnk = 0.0;\n+        for (int i = 0; i < nreq; i++) {\n+            if (!this.lindep[i]) {\n+                rnk += 1.0;\n+            }\n+        }\n+        double var = rss[nreq - 1] / (nobs - rnk);\n+        double[] rinv = new double[nreq * (nreq - 1) / 2];\n+        inverse(rinv, nreq);\n+        double[] covmat = new double[nreq * (nreq + 1) / 2];\n+        Arrays.fill(covmat, Double.NaN);\n+        int pos2;\n+        int pos1;\n+        int start = 0;\n+        double total = 0;\n+        for (int row = 0; row < nreq; row++) {\n+            pos2 = start;\n+            if (!this.lindep[row]) {\n+                for (int col = row; col < nreq; col++) {\n+                    if (!this.lindep[col]) {\n+                        pos1 = start + col - row;\n+                        if (row == col) {\n+                            total = 1.0 / d[col];\n+                        } else {\n+                            total = rinv[pos1 - 1] / d[col];\n+                        }\n+                        for (int k = col + 1; k < nreq; k++) {\n+                            if (!this.lindep[k]) {\n+                                total += rinv[pos1] * rinv[pos2] / d[k];\n+                            }\n+                            ++pos1;\n+                            ++pos2;\n+                        }\n+                        covmat[ (col + 1) * col / 2 + row] = total * var;\n+                    } else {\n+                        pos2 += nreq - col - 1;\n+                    }\n+                }\n+            }\n+            start += nreq - row - 1;\n+        }\n+        return covmat;\n+    }\n+\n+    /*\n+     * This internal method calculates the inverse of the upper-triangular portion\n+     * of the R matrix.\n+     */\n+    private void inverse(double[] rinv, int nreq) {\n+        int pos = nreq * (nreq - 1) / 2 - 1;\n+        int pos1 = -1;\n+        int pos2 = -1;\n+        double total = 0.0;\n+        int start;\n+        Arrays.fill(rinv, Double.NaN);\n+        for (int row = nreq - 1; row > 0; --row) {\n+            if (!this.lindep[row]) {\n+                start = (row - 1) * (nvars + nvars - row) / 2;\n+                for (int col = nreq; col > row; --col) {\n+                    pos1 = start;\n+                    pos2 = pos;\n+                    total = 0.0;\n+                    for (int k = row; k < col - 1; k++) {\n+                        pos2 += nreq - k - 1;\n+                        if (!this.lindep[k]) {\n+                            total += -r[pos1] * rinv[pos2];\n+                        }\n+                        ++pos1;\n+                    }\n+                    rinv[pos] = total - r[pos1];\n+                    --pos;\n+                }\n+            } else {\n+                pos -= nreq - row;\n+            }\n+        }\n+        return;\n+    }\n+\n+    /*\n+     * In the original algorithm only the partial correlations of the regressors\n+     * is returned to the user. In this implementation, we have\n+     * corr =\n+     * {\n+     *   corrxx - lower triangular\n+     *   corrxy - bottom row of the matrix\n+     * }\n+     */\n+    public double[] getPartialCorrelations(int in) {\n+        /*\n+        Replaces subroutines PCORR and COR of:\n+        ALGORITHM AS274  APPL. STATIST. (1992) VOL.41, NO. 2\n+\n+        Calculate partial correlations after the variables in rows\n+        1, 2, ..., IN have been forced into the regression.\n+        If IN = 1, and the first row of R represents a constant in the\n+        model, then the usual simple correlations are returned.\n+\n+        If IN = 0, the value returned in array CORMAT for the correlation\n+        of variables Xi & Xj is:\n+        sum ( Xi.Xj ) / Sqrt ( sum (Xi^2) . sum (Xj^2) )\n+\n+        On return, array CORMAT contains the upper triangle of the matrix of\n+        partial correlations stored by rows, excluding the 1's on the diagonal.\n+        e.g. if IN = 2, the consecutive elements returned are:\n+        (3,4) (3,5) ... (3,ncol), (4,5) (4,6) ... (4,ncol), etc.\n+        Array YCORR stores the partial correlations with the Y-variable\n+        starting with YCORR(IN+1) = partial correlation with the variable in\n+        position (IN+1).\n+\n+        --------------------------------------------------------------------------*/\n+        double[] output = new double[(nvars - in + 1) * (nvars - in) / 2];\n+        int base_pos;\n+        int pos;\n+        int pos1;\n+        int pos2;\n+        int rms_off = -in;\n+        int wrk_off = -(in + 1);\n+        double[] rms = new double[nvars - in];\n+        double[] work = new double[nvars - in - 1];\n+        double sumxx;\n+        double sumxy;\n+        double sumyy;\n+        int offXX = (nvars - in) * (nvars - in - 1) / 2;\n+        if (in < -1 || in >= nvars) {\n+            return null;\n+        }\n+        int nvm = nvars - 1;\n+        base_pos = r.length - (nvm - in) * (nvm - in + 1) / 2;\n+        if (d[in] > 0.0) {\n+            rms[in + rms_off] = 1.0 / Math.sqrt(d[in]);\n+        }\n+        for (int col = in + 1; col < nvars; col++) {\n+            pos = base_pos + col - 1 - in;\n+            sumxx = d[col];\n+            for (int row = in; row < col; row++) {\n+                sumxx += d[row] * r[pos] * r[pos];\n+                pos += nvars - row - 2;\n+            }\n+            if (sumxx > 0.0) {\n+                rms[col + rms_off] = 1.0 / Math.sqrt(sumxx);\n+            } else {\n+                rms[col + rms_off] = 0.0;\n+            }\n+        }\n+        sumyy = sserr;\n+        for (int row = in; row < nvars; row++) {\n+            sumyy += d[row] * rhs[row] * rhs[row];\n+        }\n+        if (sumyy > 0.0) {\n+            sumyy = 1.0 / Math.sqrt(sumyy);\n+        }\n+        pos = 0;\n+        for (int col1 = in; col1 < nvars; col1++) {\n+            sumxy = 0.0;\n+            Arrays.fill(work, 0.0);\n+            pos1 = base_pos + col1 - in - 1;\n+            for (int row = in; row < col1; row++) {\n+                pos2 = pos1 + 1;\n+                for (int col2 = col1 + 1; col2 < nvars; col2++) {\n+                    work[col2 + wrk_off] += d[row] * r[pos1] * r[pos2];\n+                    pos2++;\n+                }\n+                sumxy += d[row] * r[pos1] * rhs[row];\n+                pos1 += nvars - row - 2;\n+            }\n+            pos2 = pos1 + 1;\n+            for (int col2 = col1 + 1; col2 < nvars; col2++) {\n+                work[col2 + wrk_off] += d[col1] * r[pos2];\n+                ++pos2;\n+                output[ (col2 - 1 - in) * (col2 - in) / 2 + col1 - in] =\n+                        work[col2 + wrk_off] * rms[col1 + rms_off] * rms[col2 + rms_off];\n+                ++pos;\n+            }\n+            sumxy += d[col1] * rhs[col1];\n+            output[col1 + rms_off + offXX] = sumxy * rms[col1 + rms_off] * sumyy;\n+        }\n+\n+        return output;\n+    }\n+\n+    /**\n+     * ALGORITHM AS274 APPL. STATIST. (1992) VOL.41, NO. 2\n+     * Move variable from position FROM to position TO in an\n+     * orthogonal reduction produced by AS75.1.\n+     *\n+     * @param from initial position\n+     * @param to destination\n+     */\n+    private void vmove(int from, int to) {\n+        double d1;\n+        double d2;\n+        double X;\n+        double d1new;\n+        double d2new;\n+        double cbar;\n+        double sbar;\n+        double Y;\n+        int first;\n+        int inc;\n+        int m1;\n+        int m2;\n+        int mp1;\n+        int pos;\n+        boolean bSkipTo40 = false;\n+        if (from == to) {\n+            return;\n+        }\n+        if (!this.rss_set) {\n+            ss();\n+        }\n+        int count = 0;\n+        if (from < to) {\n+            first = from;\n+            inc = 1;\n+            count = to - from;\n+        } else {\n+            first = from - 1;\n+            inc = -1;\n+            count = from - to;\n+        }\n+\n+        int m = first;\n+        int idx = 0;\n+        while (idx < count) {\n+            m1 = m * (nvars + nvars - m - 1) / 2;\n+            m2 = m1 + nvars - m - 1;\n+            mp1 = m + 1;\n+\n+            d1 = d[m];\n+            d2 = d[mp1];\n+            // Special cases.\n+            if (d1 > this.epsilon || d2 > this.epsilon) {\n+                X = r[m1];\n+                if (Math.abs(X) * Math.sqrt(d1) < tol[mp1]) {\n+                    X = 0.0;\n+                }\n+                if (d1 < this.epsilon || Math.abs(X) < this.epsilon) {\n+                    d[m] = d2;\n+                    d[mp1] = d1;\n+                    r[m1] = 0.0;\n+                    for (int col = m + 2; col < nvars; col++) {\n+                        ++m1;\n+                        X = r[m1];\n+                        r[m1] = r[m2];\n+                        r[m2] = X;\n+                        ++m2;\n+                    }\n+                    X = rhs[m];\n+                    rhs[m] = rhs[mp1];\n+                    rhs[mp1] = X;\n+                    bSkipTo40 = true;\n+                    break;\n+                } else if (d2 < this.epsilon) {\n+                    d[m] = d1 * X * X;\n+                    r[m1] = 1.0 / X;\n+                    for (int _i = m1 + 1; _i < m1 + nvars - m - 1; _i++) {\n+                        r[_i] /= X;\n+                    }\n+                    rhs[m] = rhs[m] / X;\n+                    bSkipTo40 = true;\n+                    break;\n+                }\n+                if (!bSkipTo40) {\n+                    d1new = d2 + d1 * X * X;\n+                    cbar = d2 / d1new;\n+                    sbar = X * d1 / d1new;\n+                    d2new = d1 * cbar;\n+                    d[m] = d1new;\n+                    d[mp1] = d2new;\n+                    r[m1] = sbar;\n+                    for (int col = m + 2; col < nvars; col++) {\n+                        ++m1;\n+                        Y = r[m1];\n+                        r[m1] = cbar * r[m2] + sbar * Y;\n+                        r[m2] = Y - X * r[m2];\n+                        ++m2;\n+                    }\n+                    Y = rhs[m];\n+                    rhs[m] = cbar * rhs[mp1] + sbar * Y;\n+                    rhs[mp1] = Y - X * rhs[mp1];\n+                }\n+            }\n+            if (m > 0) {\n+                pos = m;\n+                for (int row = 0; row < m; row++) {\n+                    X = r[pos];\n+                    r[pos] = r[pos - 1];\n+                    r[pos - 1] = X;\n+                    pos += nvars - row - 2;\n+                }\n+            }\n+            // Adjust variable order (VORDER), the tolerances (TOL) and\n+            // the vector of residual sums of squares (RSS).\n+            m1 = vorder[m];\n+            vorder[m] = vorder[mp1];\n+            vorder[mp1] = m1;\n+            X = tol[m];\n+            tol[m] = tol[mp1];\n+            tol[mp1] = X;\n+            rss[m] = rss[mp1] + d[mp1] * rhs[mp1] * rhs[mp1];\n+\n+            m += inc;\n+            ++idx;\n+        }\n+    }\n+\n+    private int reorderRegressors(int[] list, int pos1) {\n+\n+//     ALGORITHM AS274  APPL. STATIST. (1992) VOL.41, NO. 2\n+\n+//     Re-order the variables in an orthogonal reduction produced by\n+//     AS75.1 so that the N variables in LIST start at position POS1,\n+//     though will not necessarily be in the same order as in LIST.\n+//     Any variables in VORDER before position POS1 are not moved.\n+\n+//     Auxiliary routine called: VMOVE\n+//\n+//--------------------------------------------------------------------------\n+\n+        int next;\n+        int i;\n+        int l;\n+        if (list.length < 1 || list.length > nvars + 1 - pos1) {\n+            return -1;\n+        }\n+        next = pos1;\n+        i = pos1;\n+        while (i < nvars) {\n+            l = vorder[i];\n+            for (int j = 0; j < list.length; j++) {\n+                if (l == list[j]) {\n+                    if (i > next) {\n+                        this.vmove(i, next);\n+                        ++next;\n+                        if (next >= list.length + pos1) {\n+                            return 0;\n+                        } else {\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            ++i;\n+        }\n+        return 0;\n+    }\n+\n+    /*\n+     * Gets the diagonal of the Hat matrix also known as the leverage matrix\n+     *\n+     *\n+     * @returns the diagonal element of the hatmatrix\n+     */\n+    public double getDiagonalOfHatMatrix(double[] row_data) {\n+        double[] wk = new double[this.nvars];\n+        int pos;\n+        double total;\n+\n+        if (row_data.length > nvars) {\n+            return Double.NaN;\n+        }\n+        double[] xrow;\n+        if (this.hasIntercept) {\n+            xrow = new double[row_data.length + 1];\n+            xrow[0] = 1.0;\n+            System.arraycopy(row_data, 0, xrow, 1, row_data.length);\n+        } else {\n+            xrow = row_data;\n+        }\n+        double hii = 0.0;\n+        for (int col = 0; col < xrow.length; col++) {\n+            if (Math.sqrt(d[col]) < tol[col]) {\n+                wk[col] = 0.0;\n+            } else {\n+                pos = col - 1;\n+                total = xrow[col];\n+                for (int row = 0; row < col; row++) {\n+                    total = smartAdd(total, -wk[row] * r[pos]);\n+                    pos += nvars - row - 2;\n+                }\n+                wk[col] = total;\n+                hii = smartAdd(hii, (total * total) / d[col]);\n+            }\n+        }\n+        return hii;\n+    }\n+\n+    /*\n+     * Gets the order of the regressors, useful if sometype of reording\n+     * has been called. Calling regress with int[]{} args will trigger\n+     * a reordering\n+     * @returns int[] with the current order of the regressors\n+     */\n+    public int[] getOrderOfRegressors() {\n+        return MathUtils.copyOf(vorder);\n+    }\n+\n+\n+    public RegressionResults regress() throws MathException {\n+        return regress(this.nvars);\n+    }\n+\n+    public RegressionResults regress(int numberOfRegressors) throws MathException {\n+        if (this.nobs <= numberOfRegressors) {\n+            Localizable outMsg = new DummyLocalizable(\"Number of observations not \" +\n+                     \"greater than the number of number of variables\");\n+            throw new MathException(outMsg, (Object) null);\n+        }\n+        if( numberOfRegressors > this.nvars ){\n+            Localizable outMsg = new DummyLocalizable(\"Number of variables requested \" +\n+                    \"in regression greater than the number of number of variables\");\n+            throw new MathException(outMsg, (Object) null);\n+        }\n+        this.tolset();\n+\n+        this.singcheck();\n+\n+        double[] beta = this.regcf(numberOfRegressors);\n+\n+        this.ss();\n+\n+        double[] cov = this.cov(numberOfRegressors);\n+\n+        int rnk = 0;\n+        for (int i = 0; i < this.lindep.length; i++) {\n+            if (!this.lindep[i]) {\n+                ++rnk;\n+            }\n+        }\n+\n+        boolean needsReorder = false;\n+        for (int i = 0; i < numberOfRegressors; i++) {\n+            if (this.vorder[i] != i) {\n+                needsReorder = true;\n+                break;\n+            }\n+        }\n+        if (!needsReorder) {\n+            return new RegressionResults(\n+                    beta, new double[][]{cov}, true, this.nobs, rnk,\n+                    this.sumy, this.sumsqy, this.sserr, this.hasIntercept, false);\n+        } else {\n+            double[] betaNew = new double[beta.length];\n+            double[] covNew = new double[cov.length];\n+\n+            int[] newIndices = new int[beta.length];\n+            for (int i = 0; i < nvars; i++) {\n+                for (int j = 0; j < numberOfRegressors; j++) {\n+                    if (this.vorder[j] == i) {\n+                        betaNew[i] = beta[ j];\n+                        newIndices[i] = j;\n+                    }\n+                }\n+            }\n+\n+            int idx1 = 0;\n+            int idx2;\n+            int _i;\n+            int _j;\n+            for (int i = 0; i < beta.length; i++) {\n+                _i = newIndices[i];\n+                for (int j = 0; j <= i; j++, idx1++) {\n+                    _j = newIndices[j];\n+                    if (_i > _j) {\n+                        idx2 = _i * (_i + 1) / 2 + _j;\n+                    } else {\n+                        idx2 = _j * (_j + 1) / 2 + _i;\n+                    }\n+                    covNew[idx1] = cov[idx2];\n+                }\n+            }\n+            return new RegressionResults(\n+                    betaNew, new double[][]{covNew}, true, this.nobs, rnk,\n+                    this.sumy, this.sumsqy, this.sserr, this.hasIntercept, false);\n+        }\n+    }\n+\n+    public RegressionResults regress(int[] variablesToInclude) throws MathException {\n+        if (variablesToInclude.length > this.nvars) {\n+            Localizable outMsg = new DummyLocalizable(\"Number of variables in included list \" +\n+                    \"greater than the number of number of variables\");\n+            throw new MathException(outMsg, (Object) null);\n+        }\n+        if (this.nobs <= this.nvars) {\n+            Localizable outMsg = new DummyLocalizable(\"Number of observations not \" +\n+                    \"greater than the number of number of variables\");\n+            throw new MathException(outMsg, (Object) null);\n+        }\n+        Arrays.sort(variablesToInclude);\n+        int iExclude = 0;\n+        for (int i = 0; i < variablesToInclude.length; i++) {\n+            if (i >= this.nvars) {\n+                Localizable outMsg = new DummyLocalizable(\"Requesting variable for inclusion \" +\n+                        \"which does not exist in data supplied\");\n+                throw new MathException(outMsg, (Object) null);\n+            }\n+            if (i > 0 && variablesToInclude[i] == variablesToInclude[i - 1]) {\n+                variablesToInclude[i] = -1;\n+                ++iExclude;\n+            }\n+        }\n+        int[] series;\n+        if (iExclude > 0) {\n+            int j = 0;\n+            series = new int[variablesToInclude.length - iExclude];\n+            for (int i = 0; i < variablesToInclude.length; i++) {\n+                if (variablesToInclude[i] > -1) {\n+                    series[j] = variablesToInclude[i];\n+                    ++j;\n+                }\n+            }\n+        } else {\n+            series = variablesToInclude;\n+        }\n+\n+        this.reorderRegressors(series, 0);\n+\n+        this.tolset();\n+\n+        this.singcheck();\n+\n+        double[] beta = this.regcf(series.length);\n+\n+        this.ss();\n+\n+        double[] cov = this.cov(series.length);\n+\n+        int rnk = 0;\n+        for (int i = 0; i < this.lindep.length; i++) {\n+            if (!this.lindep[i]) {\n+                ++rnk;\n+            }\n+        }\n+\n+        boolean needsReorder = false;\n+        for (int i = 0; i < this.nvars; i++) {\n+            if (this.vorder[i] != series[i]) {\n+                needsReorder = true;\n+                break;\n+            }\n+        }\n+        if (!needsReorder) {\n+            return new RegressionResults(\n+                    beta, new double[][]{cov}, true, this.nobs, rnk,\n+                    this.sumy, this.sumsqy, this.sserr, this.hasIntercept, false);\n+        } else {\n+            double[] betaNew = new double[beta.length];\n+            int[] newIndices = new int[beta.length];\n+            for (int i = 0; i < series.length; i++) {\n+                for (int j = 0; j < this.vorder.length; j++) {\n+                    if (this.vorder[j] == series[i]) {\n+                        betaNew[i] = beta[ j];\n+                        newIndices[i] = j;\n+                    }\n+                }\n+            }\n+            double[] covNew = new double[cov.length];\n+            int idx1 = 0;\n+            int idx2;\n+            int _i;\n+            int _j;\n+            for (int i = 0; i < beta.length; i++) {\n+                _i = newIndices[i];\n+                for (int j = 0; j <= i; j++, idx1++) {\n+                    _j = newIndices[j];\n+                    if (_i > _j) {\n+                        idx2 = _i * (_i + 1) / 2 + _j;\n+                    } else {\n+                        idx2 = _j * (_j + 1) / 2 + _i;\n+                    }\n+                    covNew[idx1] = cov[idx2];\n+                }\n+            }\n+            return new RegressionResults(\n+                    betaNew, new double[][]{covNew}, true, this.nobs, rnk,\n+                    this.sumy, this.sumsqy, this.sserr, this.hasIntercept, false);\n+        }\n+    }\n+}\n--- /dev/null\n+++ b/src/test/java/org/apache/commons/math/stat/regression/MillerUpdatingRegressionTest.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.commons.math.stat.regression;\n+\n+import org.apache.commons.math.linear.RealMatrix;\n+import org.apache.commons.math.stat.correlation.PearsonsCorrelation;\n+import org.apache.commons.math.MathException;\n+import org.apache.commons.math.TestUtils;\n+import org.apache.commons.math.util.FastMath;\n+import org.junit.Test;\n+import static org.junit.Assert.*;\n+\n+/**\n+ * MillerUpdatingRegression tests.\n+ */\n+public class MillerUpdatingRegressionTest {\n+\n+    public MillerUpdatingRegressionTest() {\n+    }\n+    /* This is the Greene Airline Cost data. \n+     * The data can be downloaded from http://www.indiana.edu/~statmath/stat/all/panel/airline.csv\n+     */ \n+    private final static double[][] airdata = {\n+        /*\"I\",*/new double[]{1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6},\n+        /*\"T\",*/ new double[]{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15},\n+        /*\"C\",*/ new double[]{1140640, 1215690, 1309570, 1511530, 1676730, 1823740, 2022890, 2314760, 2639160, 3247620, 3787750, 3867750, 3996020, 4282880, 4748320, 569292, 640614, 777655, 999294, 1203970, 1358100, 1501350, 1709270, 2025400, 2548370, 3137740, 3557700, 3717740, 3962370, 4209390, 286298, 309290, 342056, 374595, 450037, 510412, 575347, 669331, 783799, 913883, 1041520, 1125800, 1096070, 1198930, 1170470, 145167, 170192, 247506, 309391, 354338, 373941, 420915, 474017, 532590, 676771, 880438, 1052020, 1193680, 1303390, 1436970, 91361, 95428, 98187, 115967, 138382, 156228, 183169, 210212, 274024, 356915, 432344, 524294, 530924, 581447, 610257, 68978, 74904, 83829, 98148, 118449, 133161, 145062, 170711, 199775, 276797, 381478, 506969, 633388, 804388, 1009500},\n+        /*\"Q\",*/ new double[]{0.952757, 0.986757, 1.09198, 1.17578, 1.16017, 1.17376, 1.29051, 1.39067, 1.61273, 1.82544, 1.54604, 1.5279, 1.6602, 1.82231, 1.93646, 0.520635, 0.534627, 0.655192, 0.791575, 0.842945, 0.852892, 0.922843, 1, 1.19845, 1.34067, 1.32624, 1.24852, 1.25432, 1.37177, 1.38974, 0.262424, 0.266433, 0.306043, 0.325586, 0.345706, 0.367517, 0.409937, 0.448023, 0.539595, 0.539382, 0.467967, 0.450544, 0.468793, 0.494397, 0.493317, 0.086393, 0.09674, 0.1415, 0.169715, 0.173805, 0.164272, 0.170906, 0.17784, 0.192248, 0.242469, 0.256505, 0.249657, 0.273923, 0.371131, 0.421411, 0.051028, 0.052646, 0.056348, 0.066953, 0.070308, 0.073961, 0.084946, 0.095474, 0.119814, 0.150046, 0.144014, 0.1693, 0.172761, 0.18667, 0.213279, 0.037682, 0.039784, 0.044331, 0.050245, 0.055046, 0.052462, 0.056977, 0.06149, 0.069027, 0.092749, 0.11264, 0.154154, 0.186461, 0.246847, 0.304013},\n+        /*\"PF\",*/ new double[]{106650, 110307, 110574, 121974, 196606, 265609, 263451, 316411, 384110, 569251, 871636, 997239, 938002, 859572, 823411, 103795, 111477, 118664, 114797, 215322, 281704, 304818, 348609, 374579, 544109, 853356, 1003200, 941977, 856533, 821361, 118788, 123798, 122882, 131274, 222037, 278721, 306564, 356073, 378311, 555267, 850322, 1015610, 954508, 886999, 844079, 114987, 120501, 121908, 127220, 209405, 263148, 316724, 363598, 389436, 547376, 850418, 1011170, 951934, 881323, 831374, 118222, 116223, 115853, 129372, 243266, 277930, 317273, 358794, 397667, 566672, 848393, 1005740, 958231, 872924, 844622, 117112, 119420, 116087, 122997, 194309, 307923, 323595, 363081, 386422, 564867, 874818, 1013170, 930477, 851676, 819476},\n+        /*\"LF\",*/ new double[]{0.534487, 0.532328, 0.547736, 0.540846, 0.591167, 0.575417, 0.594495, 0.597409, 0.638522, 0.676287, 0.605735, 0.61436, 0.633366, 0.650117, 0.625603, 0.490851, 0.473449, 0.503013, 0.512501, 0.566782, 0.558133, 0.558799, 0.57207, 0.624763, 0.628706, 0.58915, 0.532612, 0.526652, 0.540163, 0.528775, 0.524334, 0.537185, 0.582119, 0.579489, 0.606592, 0.60727, 0.582425, 0.573972, 0.654256, 0.631055, 0.56924, 0.589682, 0.587953, 0.565388, 0.577078, 0.432066, 0.439669, 0.488932, 0.484181, 0.529925, 0.532723, 0.549067, 0.55714, 0.611377, 0.645319, 0.611734, 0.580884, 0.572047, 0.59457, 0.585525, 0.442875, 0.462473, 0.519118, 0.529331, 0.557797, 0.556181, 0.569327, 0.583465, 0.631818, 0.604723, 0.587921, 0.616159, 0.605868, 0.594688, 0.635545, 0.448539, 0.475889, 0.500562, 0.500344, 0.528897, 0.495361, 0.510342, 0.518296, 0.546723, 0.554276, 0.517766, 0.580049, 0.556024, 0.537791, 0.525775}\n+    };\n+\n+    /**\n+     * Test of hasIntercept method, of class MillerUpdatingRegression.\n+     */\n+    @Test\n+    public void testHasIntercept() {\n+        MillerUpdatingRegression instance = new MillerUpdatingRegression(10, false);\n+        if (instance.hasIntercept()) {\n+            fail(\"Should not have intercept\");\n+        }\n+        instance = new MillerUpdatingRegression(10, true);\n+        if (!instance.hasIntercept()) {\n+            fail(\"Should have intercept\");\n+        }\n+    }\n+\n+    /**\n+     * Test of getN method, of class MillerUpdatingRegression.\n+     */\n+    @Test\n+    public void testAddObsGetNClear() {\n+        System.out.println(\"getN - test add observation - clear\");\n+        MillerUpdatingRegression instance = new MillerUpdatingRegression(3, true);\n+        double[][] xAll = new double[airdata[0].length][];\n+        double[] y = new double[airdata[0].length];\n+        for (int i = 0; i < airdata[0].length; i++) {\n+            xAll[i] = new double[3];\n+            xAll[i][0] = Math.log(airdata[3][i]);\n+            xAll[i][1] = Math.log(airdata[4][i]);\n+            xAll[i][2] = airdata[5][i];\n+            y[i] = Math.log(airdata[2][i]);\n+        }\n+        instance.addObservations(xAll, y);\n+        if (instance.getN() != xAll.length) {\n+            fail(\"Number of observations not correct in bulk addition\");\n+        }\n+        instance.clear();\n+        for (int i = 0; i < xAll.length; i++) {\n+            instance.addObservation(xAll[i], y[i]);\n+        }\n+        if (instance.getN() != xAll.length) {\n+            fail(\"Number of observations not correct in drip addition\");\n+        }\n+        return;\n+    }\n+\n+    @Test\n+    public void testNegativeTestAddObs() {\n+        System.out.println(\"Test Add obs should fail if number of vars changes\");\n+        MillerUpdatingRegression instance = new MillerUpdatingRegression(3, true);\n+        try {\n+            instance.addObservation(new double[]{1.0}, 0.0);\n+            fail(\"Should throw IllegalArgumentException\");\n+        } catch (IllegalArgumentException iae) {\n+        } catch (Exception e) {\n+            fail(\"Should throw IllegalArgumentException\");\n+        }\n+        try {\n+            instance.addObservation(new double[]{1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0}, 0.0);\n+            fail(\"Should throw IllegalArgumentException\");\n+        } catch (IllegalArgumentException iae) {\n+        } catch (Exception e) {\n+            fail(\"Should throw IllegalArgumentException\");\n+        }\n+        try {\n+            instance.addObservation(new double[]{1.0, 1.0, 1.0}, 0.0);\n+        } catch (Exception e) {\n+            fail(\"Should throw IllegalArgumentException\");\n+        }\n+\n+        //now we try it without an intercept\n+        instance = new MillerUpdatingRegression(3, false);\n+        try {\n+            instance.addObservation(new double[]{1.0}, 0.0);\n+            fail(\"Should throw IllegalArgumentException [NOINTERCEPT]\");\n+        } catch (IllegalArgumentException iae) {\n+        } catch (Exception e) {\n+            fail(\"Should throw IllegalArgumentException [NOINTERCEPT]\");\n+        }\n+        try {\n+            instance.addObservation(new double[]{1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0}, 0.0);\n+            fail(\"Should throw IllegalArgumentException [NOINTERCEPT]\");\n+        } catch (IllegalArgumentException iae) {\n+        } catch (Exception e) {\n+            fail(\"Should throw IllegalArgumentException [NOINTERCEPT]\");\n+        }\n+        try {\n+            instance.addObservation(new double[]{1.0, 1.0, 1.0}, 0.0);\n+        } catch (Exception e) {\n+            fail(\"Should throw IllegalArgumentException [NOINTERCEPT]\");\n+        }\n+    }\n+\n+    @Test\n+    public void testNegativeTestAddMultipleObs() {\n+        System.out.println(\"Test Add Multiple obs should fail if length of arrays is not same\");\n+        MillerUpdatingRegression instance = new MillerUpdatingRegression(3, true);\n+        try {\n+            double[][] tst = {{1.0, 1.0, 1.0}, {1.20, 2.0, 2.1}};\n+            double[] y = {1.0};\n+            instance.addObservations(tst, y);\n+\n+            fail(\"Should throw IllegalArgumentException\");\n+        } catch (IllegalArgumentException iae) {\n+        } catch (Exception e) {\n+            fail(\"Should throw IllegalArgumentException\");\n+        }\n+\n+        try {\n+            double[][] tst = {{1.0, 1.0, 1.0}, {1.20, 2.0, 2.1}};\n+            double[] y = {1.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0};\n+            instance.addObservations(tst, y);\n+\n+            fail(\"Should throw IllegalArgumentException\");\n+        } catch (IllegalArgumentException iae) {\n+        } catch (Exception e) {\n+            fail(\"Should throw IllegalArgumentException\");\n+        }\n+    }\n+\n+    /* Results can be found at http://www.indiana.edu/~statmath/stat/all/panel/panel4.html\n+     * This test concerns a known data set\n+     */\n+    @Test\n+    public void testRegressAirlineConstantExternal() {\n+        MillerUpdatingRegression instance = new MillerUpdatingRegression(4, false);\n+        double[][] x = new double[airdata[0].length][];\n+        double[] y = new double[airdata[0].length];\n+        for (int i = 0; i < airdata[0].length; i++) {\n+            x[i] = new double[4];\n+            x[i][0] = 1.0;\n+            x[i][1] = Math.log(airdata[3][i]);\n+            x[i][2] = Math.log(airdata[4][i]);\n+            x[i][3] = airdata[5][i];\n+            y[i] = Math.log(airdata[2][i]);\n+        }\n+\n+        instance.addObservations(x, y);\n+        try {\n+            RegressionResults result = instance.regress();\n+            if (result == null) {\n+                fail(\"The test case is a prototype.\");\n+            }\n+            TestUtils.assertEquals(\n+                    new double[]{9.5169, 0.8827, 0.4540, -1.6275},\n+                    result.getParameterEstimates(), 1e-4);\n+\n+\n+            TestUtils.assertEquals(\n+                    new double[]{.2292445, .0132545, .0203042, .345302},\n+                    result.getStdErrorOfEstimates(), 1.0e-4);\n+\n+            TestUtils.assertEquals(0.01552839, result.getMeanSquareError(), 1.0e-8);\n+        } catch (Exception e) {\n+            fail(\"Should not throw exception but does\");\n+        }\n+    }\n+\n+    @Test\n+    public void testRegressAirlineConstantInternal() {\n+        MillerUpdatingRegression instance = new MillerUpdatingRegression(3, true);\n+        double[][] x = new double[airdata[0].length][];\n+        double[] y = new double[airdata[0].length];\n+        for (int i = 0; i < airdata[0].length; i++) {\n+            x[i] = new double[3];\n+            x[i][0] = Math.log(airdata[3][i]);\n+            x[i][1] = Math.log(airdata[4][i]);\n+            x[i][2] = airdata[5][i];\n+            y[i] = Math.log(airdata[2][i]);\n+        }\n+\n+        instance.addObservations(x, y);\n+        try {\n+            RegressionResults result = instance.regress();\n+            if (result == null) {\n+                fail(\"The test case is a prototype.\");\n+            }\n+            TestUtils.assertEquals(\n+                    new double[]{9.5169, 0.8827, 0.4540, -1.6275},\n+                    result.getParameterEstimates(), 1e-4);\n+\n+\n+            TestUtils.assertEquals(\n+                    new double[]{.2292445, .0132545, .0203042, .345302},\n+                    result.getStdErrorOfEstimates(), 1.0e-4);\n+\n+            TestUtils.assertEquals(0.9883, result.getRSquared(), 1.0e-4);\n+            TestUtils.assertEquals(0.01552839, result.getMeanSquareError(), 1.0e-8);\n+        } catch (Exception e) {\n+            fail(\"Should not throw exception but does\");\n+        }\n+    }\n+\n+    @Test\n+    public void testFilippelli() throws MathException {\n+        double[] data = new double[]{\n+            0.8116, -6.860120914,\n+            0.9072, -4.324130045,\n+            0.9052, -4.358625055,\n+            0.9039, -4.358426747,\n+            0.8053, -6.955852379,\n+            0.8377, -6.661145254,\n+            0.8667, -6.355462942,\n+            0.8809, -6.118102026,\n+            0.7975, -7.115148017,\n+            0.8162, -6.815308569,\n+            0.8515, -6.519993057,\n+            0.8766, -6.204119983,\n+            0.8885, -5.853871964,\n+            0.8859, -6.109523091,\n+            0.8959, -5.79832982,\n+            0.8913, -5.482672118,\n+            0.8959, -5.171791386,\n+            0.8971, -4.851705903,\n+            0.9021, -4.517126416,\n+            0.909, -4.143573228,\n+            0.9139, -3.709075441,\n+            0.9199, -3.499489089,\n+            0.8692, -6.300769497,\n+            0.8872, -5.953504836,\n+            0.89, -5.642065153,\n+            0.891, -5.031376979,\n+            0.8977, -4.680685696,\n+            0.9035, -4.329846955,\n+            0.9078, -3.928486195,\n+            0.7675, -8.56735134,\n+            0.7705, -8.363211311,\n+            0.7713, -8.107682739,\n+            0.7736, -7.823908741,\n+            0.7775, -7.522878745,\n+            0.7841, -7.218819279,\n+            0.7971, -6.920818754,\n+            0.8329, -6.628932138,\n+            0.8641, -6.323946875,\n+            0.8804, -5.991399828,\n+            0.7668, -8.781464495,\n+            0.7633, -8.663140179,\n+            0.7678, -8.473531488,\n+            0.7697, -8.247337057,\n+            0.77, -7.971428747,\n+            0.7749, -7.676129393,\n+            0.7796, -7.352812702,\n+            0.7897, -7.072065318,\n+            0.8131, -6.774174009,\n+            0.8498, -6.478861916,\n+            0.8741, -6.159517513,\n+            0.8061, -6.835647144,\n+            0.846, -6.53165267,\n+            0.8751, -6.224098421,\n+            0.8856, -5.910094889,\n+            0.8919, -5.598599459,\n+            0.8934, -5.290645224,\n+            0.894, -4.974284616,\n+            0.8957, -4.64454848,\n+            0.9047, -4.290560426,\n+            0.9129, -3.885055584,\n+            0.9209, -3.408378962,\n+            0.9219, -3.13200249,\n+            0.7739, -8.726767166,\n+            0.7681, -8.66695597,\n+            0.7665, -8.511026475,\n+            0.7703, -8.165388579,\n+            0.7702, -7.886056648,\n+            0.7761, -7.588043762,\n+            0.7809, -7.283412422,\n+            0.7961, -6.995678626,\n+            0.8253, -6.691862621,\n+            0.8602, -6.392544977,\n+            0.8809, -6.067374056,\n+            0.8301, -6.684029655,\n+            0.8664, -6.378719832,\n+            0.8834, -6.065855188,\n+            0.8898, -5.752272167,\n+            0.8964, -5.132414673,\n+            0.8963, -4.811352704,\n+            0.9074, -4.098269308,\n+            0.9119, -3.66174277,\n+            0.9228, -3.2644011\n+        };\n+        MillerUpdatingRegression model = new MillerUpdatingRegression(10, true);\n+        int off = 0;\n+        double[] tmp = new double[10];\n+        int nobs = 82;\n+        for (int i = 0; i < nobs; i++) {\n+            tmp[0] = data[off + 1];\n+//            tmp[1] = tmp[0] * tmp[0];\n+//            tmp[2] = tmp[0] * tmp[1]; //^3\n+//            tmp[3] = tmp[1] * tmp[1]; //^4\n+//            tmp[4] = tmp[2] * tmp[1]; //^5\n+//            tmp[5] = tmp[2] * tmp[2]; //^6\n+//            tmp[6] = tmp[2] * tmp[3]; //^7\n+//            tmp[7] = tmp[3] * tmp[3]; //^8\n+//            tmp[8] = tmp[4] * tmp[3]; //^9\n+//            tmp[9] = tmp[4] * tmp[4]; //^10           \n+            tmp[1] = tmp[0] * tmp[0];\n+            tmp[2] = tmp[0] * tmp[1];\n+            tmp[3] = tmp[0] * tmp[2];\n+            tmp[4] = tmp[0] * tmp[3];\n+            tmp[5] = tmp[0] * tmp[4];\n+            tmp[6] = tmp[0] * tmp[5];\n+            tmp[7] = tmp[0] * tmp[6];\n+            tmp[8] = tmp[0] * tmp[7];\n+            tmp[9] = tmp[0] * tmp[8];\n+            model.addObservation(tmp, data[off]);\n+            off += 2;\n+        }\n+        RegressionResults result = model.regress();\n+        double[] betaHat = result.getParameterEstimates();\n+        TestUtils.assertEquals(betaHat,\n+                new double[]{\n+                    -1467.48961422980,\n+                    -2772.17959193342,\n+                    -2316.37108160893,\n+                    -1127.97394098372,\n+                    -354.478233703349,\n+                    -75.1242017393757,\n+                    -10.8753180355343,\n+                    -1.06221498588947,\n+                    -0.670191154593408E-01,\n+                    -0.246781078275479E-02,\n+                    -0.402962525080404E-04\n+                }, 1E-5); //\n+//\n+        double[] se = result.getStdErrorOfEstimates();\n+        TestUtils.assertEquals(se,\n+                new double[]{\n+                    298.084530995537,\n+                    559.779865474950,\n+                    466.477572127796,\n+                    227.204274477751,\n+                    71.6478660875927,\n+                    15.2897178747400,\n+                    2.23691159816033,\n+                    0.221624321934227,\n+                    0.142363763154724E-01,\n+                    0.535617408889821E-03,\n+                    0.896632837373868E-05\n+                }, 1E-5); //\n+\n+        TestUtils.assertEquals(0.996727416185620, result.getRSquared(), 1.0e-8);\n+        TestUtils.assertEquals(0.112091743968020E-04, result.getMeanSquareError(), 1.0e-10);\n+        TestUtils.assertEquals(0.795851382172941E-03, result.getErrorSumSquares(), 1.0e-10);\n+\n+    }\n+\n+    @Test\n+    public void testWampler1() throws MathException {\n+        double[] data = new double[]{\n+            1, 0,\n+            6, 1,\n+            63, 2,\n+            364, 3,\n+            1365, 4,\n+            3906, 5,\n+            9331, 6,\n+            19608, 7,\n+            37449, 8,\n+            66430, 9,\n+            111111, 10,\n+            177156, 11,\n+            271453, 12,\n+            402234, 13,\n+            579195, 14,\n+            813616, 15,\n+            1118481, 16,\n+            1508598, 17,\n+            2000719, 18,\n+            2613660, 19,\n+            3368421, 20};\n+\n+        MillerUpdatingRegression model = new MillerUpdatingRegression(5, true);\n+        int off = 0;\n+        double[] tmp = new double[5];\n+        int nobs = 21;\n+        for (int i = 0; i < nobs; i++) {\n+            tmp[0] = data[off + 1];\n+            tmp[1] = tmp[0] * tmp[0];\n+            tmp[2] = tmp[0] * tmp[1];\n+            tmp[3] = tmp[0] * tmp[2];\n+            tmp[4] = tmp[0] * tmp[3];\n+            model.addObservation(tmp, data[off]);\n+            off += 2;\n+        }\n+        RegressionResults result = model.regress();\n+        double[] betaHat = result.getParameterEstimates();\n+        TestUtils.assertEquals(betaHat,\n+                new double[]{1.0,\n+                    1.0, 1.0,\n+                    1.0, 1.0,\n+                    1.0}, 1E-8); //\n+//\n+        double[] se = result.getStdErrorOfEstimates();\n+        TestUtils.assertEquals(se,\n+                new double[]{0.0,\n+                    0.0, 0.0,\n+                    0.0, 0.0,\n+                    0.0}, 1E-8); //\n+\n+        TestUtils.assertEquals(1.0, result.getRSquared(), 1.0e-10);\n+        TestUtils.assertEquals(0, result.getMeanSquareError(), 1.0e-7);\n+        TestUtils.assertEquals(0.00, result.getErrorSumSquares(), 1.0e-6);\n+\n+        return;\n+    }\n+\n+    @Test\n+    public void testWampler2() throws MathException {\n+        double[] data = new double[]{\n+            1.00000, 0,\n+            1.11111, 1,\n+            1.24992, 2,\n+            1.42753, 3,\n+            1.65984, 4,\n+            1.96875, 5,\n+            2.38336, 6,\n+            2.94117, 7,\n+            3.68928, 8,\n+            4.68559, 9,\n+            6.00000, 10,\n+            7.71561, 11,\n+            9.92992, 12,\n+            12.75603, 13,\n+            16.32384, 14,\n+            20.78125, 15,\n+            26.29536, 16,\n+            33.05367, 17,\n+            41.26528, 18,\n+            51.16209, 19,\n+            63.00000, 20};\n+\n+        MillerUpdatingRegression model = new MillerUpdatingRegression(5, true);\n+        int off = 0;\n+        double[] tmp = new double[5];\n+        int nobs = 21;\n+        for (int i = 0; i < nobs; i++) {\n+            tmp[0] = data[off + 1];\n+            tmp[1] = tmp[0] * tmp[0];\n+            tmp[2] = tmp[0] * tmp[1];\n+            tmp[3] = tmp[0] * tmp[2];\n+            tmp[4] = tmp[0] * tmp[3];\n+            model.addObservation(tmp, data[off]);\n+            off += 2;\n+        }\n+        RegressionResults result = model.regress();\n+        double[] betaHat = result.getParameterEstimates();\n+        TestUtils.assertEquals(betaHat,\n+                new double[]{1.0,\n+                    1.0e-1, 1.0e-2,\n+                    1.0e-3, 1.0e-4,\n+                    1.0e-5}, 1E-8); //\n+//\n+        double[] se = result.getStdErrorOfEstimates();\n+        TestUtils.assertEquals(se,\n+                new double[]{0.0,\n+                    0.0, 0.0,\n+                    0.0, 0.0,\n+                    0.0}, 1E-8); //\n+\n+        TestUtils.assertEquals(1.0, result.getRSquared(), 1.0e-10);\n+        TestUtils.assertEquals(0, result.getMeanSquareError(), 1.0e-7);\n+        TestUtils.assertEquals(0.00, result.getErrorSumSquares(), 1.0e-6);\n+        return;\n+    }\n+\n+    @Test\n+    public void testWampler3() throws MathException {\n+        double[] data = new double[]{\n+            760, 0,\n+            -2042, 1,\n+            2111, 2,\n+            -1684, 3,\n+            3888, 4,\n+            1858, 5,\n+            11379, 6,\n+            17560, 7,\n+            39287, 8,\n+            64382, 9,\n+            113159, 10,\n+            175108, 11,\n+            273291, 12,\n+            400186, 13,\n+            581243, 14,\n+            811568, 15,\n+            1121004, 16,\n+            1506550, 17,\n+            2002767, 18,\n+            2611612, 19,\n+            3369180, 20};\n+        MillerUpdatingRegression model = new MillerUpdatingRegression(5, true);\n+        int off = 0;\n+        double[] tmp = new double[5];\n+        int nobs = 21;\n+        for (int i = 0; i < nobs; i++) {\n+            tmp[0] = data[off + 1];\n+            tmp[1] = tmp[0] * tmp[0];\n+            tmp[2] = tmp[0] * tmp[1];\n+            tmp[3] = tmp[0] * tmp[2];\n+            tmp[4] = tmp[0] * tmp[3];\n+            model.addObservation(tmp, data[off]);\n+            off += 2;\n+        }\n+        RegressionResults result = model.regress();\n+        double[] betaHat = result.getParameterEstimates();\n+        TestUtils.assertEquals(betaHat,\n+                new double[]{1.0,\n+                    1.0, 1.0,\n+                    1.0, 1.0,\n+                    1.0}, 1E-8); //\n+        double[] se = result.getStdErrorOfEstimates();\n+        TestUtils.assertEquals(se,\n+                new double[]{2152.32624678170,\n+                    2363.55173469681, 779.343524331583,\n+                    101.475507550350, 5.64566512170752,\n+                    0.112324854679312}, 1E-8); //\n+\n+        TestUtils.assertEquals(.999995559025820, result.getRSquared(), 1.0e-10);\n+        TestUtils.assertEquals(5570284.53333333, result.getMeanSquareError(), 1.0e-7);\n+        TestUtils.assertEquals(83554268.0000000, result.getErrorSumSquares(), 1.0e-6);\n+        return;\n+    }\n+\n+    //@Test\n+    public void testWampler4() throws MathException {\n+        double[] data = new double[]{\n+            75901, 0,\n+            -204794, 1,\n+            204863, 2,\n+            -204436, 3,\n+            253665, 4,\n+            -200894, 5,\n+            214131, 6,\n+            -185192, 7,\n+            221249, 8,\n+            -138370, 9,\n+            315911, 10,\n+            -27644, 11,\n+            455253, 12,\n+            197434, 13,\n+            783995, 14,\n+            608816, 15,\n+            1370781, 16,\n+            1303798, 17,\n+            2205519, 18,\n+            2408860, 19,\n+            3444321, 20};\n+        MillerUpdatingRegression model = new MillerUpdatingRegression(5, true);\n+        int off = 0;\n+        double[] tmp = new double[5];\n+        int nobs = 21;\n+        for (int i = 0; i < nobs; i++) {\n+            tmp[0] = data[off + 1];\n+            tmp[1] = tmp[0] * tmp[0];\n+            tmp[2] = tmp[0] * tmp[1];\n+            tmp[3] = tmp[0] * tmp[2];\n+            tmp[4] = tmp[0] * tmp[3];\n+            model.addObservation(tmp, data[off]);\n+            off += 2;\n+        }\n+        RegressionResults result = model.regress();\n+        double[] betaHat = result.getParameterEstimates();\n+        TestUtils.assertEquals(betaHat,\n+                new double[]{1.0,\n+                    1.0, 1.0,\n+                    1.0, 1.0,\n+                    1.0}, 1E-8); //\n+//\n+        double[] se = result.getStdErrorOfEstimates();\n+        TestUtils.assertEquals(se,\n+                new double[]{215232.624678170,\n+                    236355.173469681, 77934.3524331583,\n+                    10147.5507550350, 564.566512170752,\n+                    11.2324854679312}, 1E-8); //\n+\n+        TestUtils.assertEquals(.957478440825662, result.getRSquared(), 1.0e-10);\n+        TestUtils.assertEquals(55702845333.3333, result.getMeanSquareError(), 1.0e-4);\n+        TestUtils.assertEquals(835542680000.000, result.getErrorSumSquares(), 1.0e-3);\n+\n+        return;\n+    }\n+\n+    /**\n+     * Test Longley dataset against certified values provided by NIST.\n+     * Data Source: J. Longley (1967) \"An Appraisal of Least Squares\n+     * Programs for the Electronic Computer from the Point of View of the User\"\n+     * Journal of the American Statistical Association, vol. 62. September,\n+     * pp. 819-841.\n+     *\n+     * Certified values (and data) are from NIST:\n+     * http://www.itl.nist.gov/div898/strd/lls/data/LINKS/DATA/Longley.dat\n+     */\n+    @Test\n+    public void testLongly() throws Exception {\n+        // Y values are first, then independent vars\n+        // Each row is one observation\n+        double[] design = new double[]{\n+            60323, 83.0, 234289, 2356, 1590, 107608, 1947,\n+            61122, 88.5, 259426, 2325, 1456, 108632, 1948,\n+            60171, 88.2, 258054, 3682, 1616, 109773, 1949,\n+            61187, 89.5, 284599, 3351, 1650, 110929, 1950,\n+            63221, 96.2, 328975, 2099, 3099, 112075, 1951,\n+            63639, 98.1, 346999, 1932, 3594, 113270, 1952,\n+            64989, 99.0, 365385, 1870, 3547, 115094, 1953,\n+            63761, 100.0, 363112, 3578, 3350, 116219, 1954,\n+            66019, 101.2, 397469, 2904, 3048, 117388, 1955,\n+            67857, 104.6, 419180, 2822, 2857, 118734, 1956,\n+            68169, 108.4, 442769, 2936, 2798, 120445, 1957,\n+            66513, 110.8, 444546, 4681, 2637, 121950, 1958,\n+            68655, 112.6, 482704, 3813, 2552, 123366, 1959,\n+            69564, 114.2, 502601, 3931, 2514, 125368, 1960,\n+            69331, 115.7, 518173, 4806, 2572, 127852, 1961,\n+            70551, 116.9, 554894, 4007, 2827, 130081, 1962\n+        };\n+\n+        final int nobs = 16;\n+        final int nvars = 6;\n+\n+        // Estimate the model\n+        MillerUpdatingRegression model = new MillerUpdatingRegression(6, true);\n+        int off = 0;\n+        double[] tmp = new double[6];\n+        for (int i = 0; i < nobs; i++) {\n+            System.arraycopy(design, off + 1, tmp, 0, nvars);\n+            model.addObservation(tmp, design[off]);\n+            off += nvars + 1;\n+        }\n+\n+        // Check expected beta values from NIST\n+        RegressionResults result = model.regress();\n+        double[] betaHat = result.getParameterEstimates();\n+        TestUtils.assertEquals(betaHat,\n+                new double[]{-3482258.63459582, 15.0618722713733,\n+                    -0.358191792925910E-01, -2.02022980381683,\n+                    -1.03322686717359, -0.511041056535807E-01,\n+                    1829.15146461355}, 1E-8); //\n+\n+        // Check standard errors from NIST\n+        double[] errors = result.getStdErrorOfEstimates();\n+        TestUtils.assertEquals(new double[]{890420.383607373,\n+                    84.9149257747669,\n+                    0.334910077722432E-01,\n+                    0.488399681651699,\n+                    0.214274163161675,\n+                    0.226073200069370,\n+                    455.478499142212}, errors, 1E-6);\n+//        \n+        // Check R-Square statistics against R\n+        TestUtils.assertEquals(0.995479004577296, result.getRSquared(), 1E-12);\n+        TestUtils.assertEquals(0.992465007628826, result.getAdjustedRSquared(), 1E-12);\n+//        \n+//        \n+//        // Estimate model without intercept\n+        model = new MillerUpdatingRegression(6, false);\n+        off = 0;\n+        for (int i = 0; i < nobs; i++) {\n+            System.arraycopy(design, off + 1, tmp, 0, nvars);\n+            model.addObservation(tmp, design[off]);\n+            off += nvars + 1;\n+        }\n+        // Check expected beta values from R\n+        result = model.regress();\n+        betaHat = result.getParameterEstimates();\n+        TestUtils.assertEquals(betaHat,\n+                new double[]{-52.99357013868291, 0.07107319907358,\n+                    -0.42346585566399, -0.57256866841929,\n+                    -0.41420358884978, 48.41786562001326}, 1E-11);\n+//        \n+        // Check standard errors from R\n+        errors = result.getStdErrorOfEstimates();\n+        TestUtils.assertEquals(new double[]{129.54486693117232, 0.03016640003786,\n+                    0.41773654056612, 0.27899087467676, 0.32128496193363,\n+                    17.68948737819961}, errors, 1E-11);\n+//        \n+\n+//        // Check R-Square statistics against R\n+        TestUtils.assertEquals(0.9999670130706, result.getRSquared(), 1E-12);\n+        TestUtils.assertEquals(0.999947220913, result.getAdjustedRSquared(), 1E-12);\n+\n+    }\n+\n+//    @Test\n+//    public void testRegressReorder() throws MathException {\n+//        // System.out.println(\"testRegressReorder\");\n+//        MillerUpdatingRegression instance = new MillerUpdatingRegression(4, false);\n+//        double[][] x = new double[airdata[0].length][];\n+//        double[] y = new double[airdata[0].length];\n+//        for (int i = 0; i < airdata[0].length; i++) {\n+//            x[i] = new double[4];\n+//            x[i][0] = 1.0;\n+//            x[i][1] = Math.log(airdata[3][i]);\n+//            x[i][2] = Math.log(airdata[4][i]);\n+//            x[i][3] = airdata[5][i];\n+//            y[i] = Math.log(airdata[2][i]);\n+//        }\n+//\n+//        instance.addObservations(x, y);\n+//        RegressionResults result = instance.regress();\n+//        if (result == null) {\n+//            fail(\"Null result....\");\n+//        }\n+//\n+//        instance.reorderRegressors(new int[]{3, 2}, 0);\n+//        RegressionResults resultInverse = instance.regress();\n+//\n+//        double[] beta = result.getParameterEstimates();\n+//        double[] betar = resultInverse.getParameterEstimates();\n+//        if (Math.abs(beta[0] - betar[0]) > 1.0e-14) {\n+//            fail(\"Parameters not correct after reorder (0,3)\");\n+//        }\n+//        if (Math.abs(beta[1] - betar[1]) > 1.0e-14) {\n+//            fail(\"Parameters not correct after reorder (1,2)\");\n+//        }\n+//        if (Math.abs(beta[2] - betar[2]) > 1.0e-14) {\n+//            fail(\"Parameters not correct after reorder (2,1)\");\n+//        }\n+//        if (Math.abs(beta[3] - betar[3]) > 1.0e-14) {\n+//            fail(\"Parameters not correct after reorder (3,0)\");\n+//        }\n+//    }\n+\n+    @Test\n+    public void testOneRedundantColumn() throws MathException {\n+        MillerUpdatingRegression instance = new MillerUpdatingRegression(4, false);\n+        MillerUpdatingRegression instance2 = new MillerUpdatingRegression(5, false);\n+        double[][] x = new double[airdata[0].length][];\n+        double[][] x2 = new double[airdata[0].length][];\n+        double[] y = new double[airdata[0].length];\n+        for (int i = 0; i < airdata[0].length; i++) {\n+            x[i] = new double[4];\n+            x2[i] = new double[5];\n+            x[i][0] = 1.0;\n+            x[i][1] = Math.log(airdata[3][i]);\n+            x[i][2] = Math.log(airdata[4][i]);\n+            x[i][3] = airdata[5][i];\n+\n+            x2[i][0] = x[i][0];\n+            x2[i][1] = x[i][1];\n+            x2[i][2] = x[i][2];\n+            x2[i][3] = x[i][3];\n+            x2[i][4] = x[i][3];\n+\n+            y[i] = Math.log(airdata[2][i]);\n+        }\n+\n+        instance.addObservations(x, y);\n+        RegressionResults result = instance.regress();\n+        if (result == null) {\n+            fail(\"Could not estimate initial regression\");\n+        }\n+\n+        instance2.addObservations(x2, y);\n+        RegressionResults resultRedundant = instance2.regress();\n+        if (resultRedundant == null) {\n+            fail(\"Could not estimate redundant regression\");\n+        }\n+        double[] beta = result.getParameterEstimates();\n+        double[] betar = resultRedundant.getParameterEstimates();\n+        double[] se = result.getStdErrorOfEstimates();\n+        double[] ser = resultRedundant.getStdErrorOfEstimates();\n+\n+        for (int i = 0; i < beta.length; i++) {\n+            if (Math.abs(beta[i] - betar[i]) > 1.0e-8) {\n+                fail(\"Parameters not correctly estimated\");\n+            }\n+            if (Math.abs(se[i] - ser[i]) > 1.0e-8) {\n+                fail(\"Standard errors not correctly estimated\");\n+            }\n+            for (int j = 0; j < i; j++) {\n+                if (Math.abs(result.getCovarianceOfParameters(i, j)\n+                        - resultRedundant.getCovarianceOfParameters(i, j)) > 1.0e-8) {\n+                    fail(\"Variance Covariance not correct\");\n+                }\n+            }\n+        }\n+\n+\n+        TestUtils.assertEquals(result.getAdjustedRSquared(), resultRedundant.getAdjustedRSquared(), 1.0e-8);\n+        TestUtils.assertEquals(result.getErrorSumSquares(), resultRedundant.getErrorSumSquares(), 1.0e-8);\n+        TestUtils.assertEquals(result.getMeanSquareError(), resultRedundant.getMeanSquareError(), 1.0e-8);\n+        TestUtils.assertEquals(result.getRSquared(), resultRedundant.getRSquared(), 1.0e-8);\n+        return;\n+    }\n+\n+    @Test\n+    public void testThreeRedundantColumn() throws MathException {\n+\n+        MillerUpdatingRegression instance = new MillerUpdatingRegression(4, false);\n+        MillerUpdatingRegression instance2 = new MillerUpdatingRegression(7, false);\n+        double[][] x = new double[airdata[0].length][];\n+        double[][] x2 = new double[airdata[0].length][];\n+        double[] y = new double[airdata[0].length];\n+        for (int i = 0; i < airdata[0].length; i++) {\n+            x[i] = new double[4];\n+            x2[i] = new double[7];\n+            x[i][0] = 1.0;\n+            x[i][1] = Math.log(airdata[3][i]);\n+            x[i][2] = Math.log(airdata[4][i]);\n+            x[i][3] = airdata[5][i];\n+\n+            x2[i][0] = x[i][0];\n+            x2[i][1] = x[i][0];\n+            x2[i][2] = x[i][1];\n+            x2[i][3] = x[i][2];\n+            x2[i][4] = x[i][1];\n+            x2[i][5] = x[i][3];\n+            x2[i][6] = x[i][2];\n+\n+            y[i] = Math.log(airdata[2][i]);\n+        }\n+\n+        instance.addObservations(x, y);\n+        RegressionResults result = instance.regress();\n+        if (result == null) {\n+            fail(\"Could not estimate initial regression\");\n+        }\n+\n+        instance2.addObservations(x2, y);\n+        RegressionResults resultRedundant = instance2.regress();\n+        if (resultRedundant == null) {\n+            fail(\"Could not estimate redundant regression\");\n+        }\n+        double[] beta = result.getParameterEstimates();\n+        double[] betar = resultRedundant.getParameterEstimates();\n+        double[] se = result.getStdErrorOfEstimates();\n+        double[] ser = resultRedundant.getStdErrorOfEstimates();\n+\n+        if (Math.abs(beta[0] - betar[0]) > 1.0e-8) {\n+            fail(\"Parameters not correct after reorder (0,3)\");\n+        }\n+        if (Math.abs(beta[1] - betar[2]) > 1.0e-8) {\n+            fail(\"Parameters not correct after reorder (1,2)\");\n+        }\n+        if (Math.abs(beta[2] - betar[3]) > 1.0e-8) {\n+            fail(\"Parameters not correct after reorder (2,1)\");\n+        }\n+        if (Math.abs(beta[3] - betar[5]) > 1.0e-8) {\n+            fail(\"Parameters not correct after reorder (3,0)\");\n+        }\n+\n+        if (Math.abs(se[0] - ser[0]) > 1.0e-8) {\n+            fail(\"Se not correct after reorder (0,3)\");\n+        }\n+        if (Math.abs(se[1] - ser[2]) > 1.0e-8) {\n+            fail(\"Se not correct after reorder (1,2)\");\n+        }\n+        if (Math.abs(se[2] - ser[3]) > 1.0e-8) {\n+            fail(\"Se not correct after reorder (2,1)\");\n+        }\n+        if (Math.abs(se[3] - ser[5]) > 1.0e-8) {\n+            fail(\"Se not correct after reorder (3,0)\");\n+        }\n+\n+        if (Math.abs(result.getCovarianceOfParameters(0, 0)\n+                - resultRedundant.getCovarianceOfParameters(0, 0)) > 1.0e-8) {\n+            fail(\"VCV not correct after reorder (0,0)\");\n+        }\n+        if (Math.abs(result.getCovarianceOfParameters(0, 1)\n+                - resultRedundant.getCovarianceOfParameters(0, 2)) > 1.0e-8) {\n+            fail(\"VCV not correct after reorder (0,1)<->(0,2)\");\n+        }\n+        if (Math.abs(result.getCovarianceOfParameters(0, 2)\n+                - resultRedundant.getCovarianceOfParameters(0, 3)) > 1.0e-8) {\n+            fail(\"VCV not correct after reorder (0,2)<->(0,1)\");\n+        }\n+        if (Math.abs(result.getCovarianceOfParameters(0, 3)\n+                - resultRedundant.getCovarianceOfParameters(0, 5)) > 1.0e-8) {\n+            fail(\"VCV not correct after reorder (0,3)<->(0,3)\");\n+        }\n+        if (Math.abs(result.getCovarianceOfParameters(1, 0)\n+                - resultRedundant.getCovarianceOfParameters(2, 0)) > 1.0e-8) {\n+            fail(\"VCV not correct after reorder (1,0)<->(2,0)\");\n+        }\n+        if (Math.abs(result.getCovarianceOfParameters(1, 1)\n+                - resultRedundant.getCovarianceOfParameters(2, 2)) > 1.0e-8) {\n+            fail(\"VCV not correct  (1,1)<->(2,1)\");\n+        }\n+        if (Math.abs(result.getCovarianceOfParameters(1, 2)\n+                - resultRedundant.getCovarianceOfParameters(2, 3)) > 1.0e-8) {\n+            fail(\"VCV not correct  (1,2)<->(2,2)\");\n+        }\n+\n+        if (Math.abs(result.getCovarianceOfParameters(2, 0)\n+                - resultRedundant.getCovarianceOfParameters(3, 0)) > 1.0e-8) {\n+            fail(\"VCV not correct  (2,0)<->(1,0)\");\n+        }\n+        if (Math.abs(result.getCovarianceOfParameters(2, 1)\n+                - resultRedundant.getCovarianceOfParameters(3, 2)) > 1.0e-8) {\n+            fail(\"VCV not correct  (2,1)<->(1,2)\");\n+        }\n+\n+        if (Math.abs(result.getCovarianceOfParameters(3, 3)\n+                - resultRedundant.getCovarianceOfParameters(5, 5)) > 1.0e-8) {\n+            fail(\"VCV not correct  (3,3)<->(3,2)\");\n+        }\n+\n+        TestUtils.assertEquals(result.getAdjustedRSquared(), resultRedundant.getAdjustedRSquared(), 1.0e-8);\n+        TestUtils.assertEquals(result.getErrorSumSquares(), resultRedundant.getErrorSumSquares(), 1.0e-8);\n+        TestUtils.assertEquals(result.getMeanSquareError(), resultRedundant.getMeanSquareError(), 1.0e-8);\n+        TestUtils.assertEquals(result.getRSquared(), resultRedundant.getRSquared(), 1.0e-8);\n+        return;\n+    }\n+\n+    @Test\n+    public void testPCorr() {\n+        MillerUpdatingRegression instance = new MillerUpdatingRegression(4, false);\n+        double[][] x = new double[airdata[0].length][];\n+        double[] y = new double[airdata[0].length];\n+        double[] cp = new double[10];\n+        double[] yxcorr = new double[4];\n+        double[] diag = new double[4];\n+        double sumysq = 0.0;\n+        int off = 0;\n+        for (int i = 0; i < airdata[0].length; i++) {\n+            x[i] = new double[4];\n+            x[i][0] = 1.0;\n+            x[i][1] = Math.log(airdata[3][i]);\n+            x[i][2] = Math.log(airdata[4][i]);\n+            x[i][3] = airdata[5][i];\n+            y[i] = Math.log(airdata[2][i]);\n+            off = 0;\n+            for (int j = 0; j < 4; j++) {\n+                double tmp = x[i][j];\n+                for (int k = 0; k <= j; k++, off++) {\n+                    cp[off] += tmp * x[i][k];\n+                }\n+                yxcorr[j] += tmp * y[i];\n+            }\n+            sumysq += y[i] * y[i];\n+        }\n+        PearsonsCorrelation pearson = new PearsonsCorrelation(x);\n+        RealMatrix corr = pearson.getCorrelationMatrix();\n+        off = 0;\n+        for (int i = 0; i < 4; i++, off += (i + 1)) {\n+            diag[i] = FastMath.sqrt(cp[off]);\n+        }\n+\n+        instance.addObservations(x, y);\n+        double[] pc = instance.getPartialCorrelations(0);\n+        int idx = 0;\n+        off = 0;\n+        int off2 = 6;\n+        for (int i = 0; i < 4; i++) {\n+            for (int j = 0; j < i; j++) {\n+                if (Math.abs(pc[idx] - cp[off] / (diag[i] * diag[j])) > 1.0e-8) {\n+                    fail(\"Failed cross products... i = \" + i + \" j = \" + j);\n+                }\n+                ++idx;\n+                ++off;\n+            }\n+            ++off;\n+            if (Math.abs(pc[i+off2] - yxcorr[ i] / (FastMath.sqrt(sumysq) * diag[i])) > 1.0e-8) {\n+                fail(\"failed cross product i = \" + i + \" y\");\n+            }\n+        }\n+        double[] pc2 = instance.getPartialCorrelations(1);\n+\n+        idx = 0;\n+\n+        for (int i = 1; i < 4; i++) {\n+            for (int j = 1; j < i; j++) {\n+                if (Math.abs(pc2[idx] - corr.getEntry(j, i)) > 1.0e-8) {\n+                    fail(\"Failed cross products... i = \" + i + \" j = \" + j);\n+                }\n+                ++idx;\n+            }\n+        }\n+        double[] pc3 = instance.getPartialCorrelations(2);\n+        if (pc3 == null) {\n+            fail(\"Should not be null\");\n+        }\n+        return;\n+    }\n+\n+    @Test\n+    public void testHdiag() {\n+        MillerUpdatingRegression instance = new MillerUpdatingRegression(4, false);\n+        double[][] x = new double[airdata[0].length][];\n+        double[] y = new double[airdata[0].length];\n+        for (int i = 0; i < airdata[0].length; i++) {\n+            x[i] = new double[4];\n+            x[i][0] = 1.0;\n+            x[i][1] = Math.log(airdata[3][i]);\n+            x[i][2] = Math.log(airdata[4][i]);\n+            x[i][3] = airdata[5][i];\n+            y[i] = Math.log(airdata[2][i]);\n+        }\n+        instance.addObservations(x, y);\n+        OLSMultipleLinearRegression ols = new OLSMultipleLinearRegression();\n+        ols.setNoIntercept(true);\n+        ols.newSampleData(y, x);\n+\n+        RealMatrix rm = ols.calculateHat();\n+        for (int i = 0; i < x.length; i++) {\n+            TestUtils.assertEquals(instance.getDiagonalOfHatMatrix(x[i]), rm.getEntry(i, i), 1.0e-8);\n+        }\n+        return;\n+    }\n+    @Test\n+    public void testHdiagConstant() {\n+        MillerUpdatingRegression instance = new MillerUpdatingRegression(3, true);\n+        double[][] x = new double[airdata[0].length][];\n+        double[] y = new double[airdata[0].length];\n+        for (int i = 0; i < airdata[0].length; i++) {\n+            x[i] = new double[3];\n+            x[i][0] = Math.log(airdata[3][i]);\n+            x[i][1] = Math.log(airdata[4][i]);\n+            x[i][2] = airdata[5][i];\n+            y[i] = Math.log(airdata[2][i]);\n+        }\n+        instance.addObservations(x, y);\n+        OLSMultipleLinearRegression ols = new OLSMultipleLinearRegression();\n+        ols.setNoIntercept(false);\n+        ols.newSampleData(y, x);\n+\n+        RealMatrix rm = ols.calculateHat();\n+        for (int i = 0; i < x.length; i++) {\n+            TestUtils.assertEquals(instance.getDiagonalOfHatMatrix(x[i]), rm.getEntry(i, i), 1.0e-8);\n+        }\n+        return;\n+    }\n+    \n+    \n+    @Test\n+    public void testSubsetRegression() throws MathException {\n+        \n+        MillerUpdatingRegression instance = new MillerUpdatingRegression(3, true);\n+        MillerUpdatingRegression redRegression = new MillerUpdatingRegression(2, true);\n+        double[][] x = new double[airdata[0].length][];\n+        double[][] xReduced = new double[airdata[0].length][];\n+        double[] y = new double[airdata[0].length];\n+        for (int i = 0; i < airdata[0].length; i++) {\n+            x[i] = new double[3];\n+            x[i][0] = Math.log(airdata[3][i]);\n+            x[i][1] = Math.log(airdata[4][i]);\n+            x[i][2] = airdata[5][i];\n+            \n+            xReduced[i] = new double[2];\n+            xReduced[i][0] = Math.log(airdata[3][i]);\n+            xReduced[i][1] = Math.log(airdata[4][i]);\n+            \n+            y[i] = Math.log(airdata[2][i]);\n+        }\n+\n+        instance.addObservations(x, y);\n+        redRegression.addObservations(xReduced, y);\n+        \n+        RegressionResults resultsInstance = instance.regress( new int[]{0,1,2} );\n+        RegressionResults resultsReduced = redRegression.regress();\n+        \n+        TestUtils.assertEquals(resultsInstance.getParameterEstimates(), resultsReduced.getParameterEstimates(), 1.0e-12);\n+        TestUtils.assertEquals(resultsInstance.getStdErrorOfEstimates(), resultsReduced.getStdErrorOfEstimates(), 1.0e-12);\n+    }\n+    \n+    \n+}", "timestamp": 1311111881, "metainfo": ""}