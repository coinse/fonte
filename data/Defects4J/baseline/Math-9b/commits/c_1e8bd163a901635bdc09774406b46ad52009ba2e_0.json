{"sha": "1e8bd163a901635bdc09774406b46ad52009ba2e", "log": "added an implementation of a non-linear conjugate gradient optimizer  ", "commit": "\n--- /dev/null\n+++ b/src/java/org/apache/commons/math/optimization/general/AbstractScalarDifferentiableOptimizer.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.commons.math.optimization.general;\n+\n+import org.apache.commons.math.FunctionEvaluationException;\n+import org.apache.commons.math.MaxIterationsExceededException;\n+import org.apache.commons.math.analysis.DifferentiableMultivariateRealFunction;\n+import org.apache.commons.math.analysis.MultivariateVectorialFunction;\n+import org.apache.commons.math.optimization.GoalType;\n+import org.apache.commons.math.optimization.OptimizationException;\n+import org.apache.commons.math.optimization.RealConvergenceChecker;\n+import org.apache.commons.math.optimization.DifferentiableMultivariateRealOptimizer;\n+import org.apache.commons.math.optimization.RealPointValuePair;\n+import org.apache.commons.math.optimization.SimpleScalarValueChecker;\n+\n+/**\n+ * Base class for implementing optimizers for multivariate scalar functions.\n+ * <p>This base class handles the boilerplate methods associated to thresholds\n+ * settings, iterations and evaluations counting.</p>\n+ * @version $Revision$ $Date$\n+ * @since 2.0\n+ */\n+public abstract class AbstractScalarDifferentiableOptimizer\n+    implements DifferentiableMultivariateRealOptimizer{\n+\n+    /** Default maximal number of iterations allowed. */\n+    public static final int DEFAULT_MAX_ITERATIONS = 100;\n+\n+    /** Serializable version identifier. */\n+    private static final long serialVersionUID = 1357126012308766636L;\n+\n+    /** Maximal number of iterations allowed. */\n+    private int maxIterations;\n+\n+    /** Number of iterations already performed. */\n+    private int iterations;\n+\n+    /** Number of evaluations already performed. */\n+    private int evaluations;\n+\n+    /** Number of gradient evaluations. */\n+    private int gradientEvaluations;\n+\n+    /** Convergence checker. */\n+    protected RealConvergenceChecker checker;\n+\n+    /** Objective function. */\n+    private DifferentiableMultivariateRealFunction f;\n+\n+    /** Objective function gradient. */\n+    private MultivariateVectorialFunction gradient;\n+\n+    /** Type of optimization. */\n+    protected GoalType goalType;\n+\n+    /** Current point set. */\n+    protected double[] point;\n+\n+    /** Simple constructor with default settings.\n+     * <p>The convergence check is set to a {@link SimpleScalarValueChecker}\n+     * and the maximal number of evaluation is set to its default value.</p>\n+     */\n+    protected AbstractScalarDifferentiableOptimizer() {\n+        setConvergenceChecker(new SimpleScalarValueChecker());\n+        setMaxIterations(DEFAULT_MAX_ITERATIONS);\n+    }\n+\n+    /** {@inheritDoc} */\n+    public void setMaxIterations(int maxIterations) {\n+        this.maxIterations = maxIterations;\n+    }\n+\n+    /** {@inheritDoc} */\n+    public int getMaxIterations() {\n+        return maxIterations;\n+    }\n+\n+    /** {@inheritDoc} */\n+    public int getIterations() {\n+        return iterations;\n+    }\n+\n+    /** {@inheritDoc} */\n+    public int getEvaluations() {\n+        return evaluations;\n+    }\n+\n+    /** {@inheritDoc} */\n+    public int getGradientEvaluations() {\n+        return gradientEvaluations;\n+    }\n+\n+    /** {@inheritDoc} */\n+    public void setConvergenceChecker(RealConvergenceChecker checker) {\n+        this.checker = checker;\n+    }\n+\n+    /** {@inheritDoc} */\n+    public RealConvergenceChecker getConvergenceChecker() {\n+        return checker;\n+    }\n+\n+    /** Increment the iterations counter by 1.\n+     * @exception OptimizationException if the maximal number\n+     * of iterations is exceeded\n+     */\n+    protected void incrementIterationsCounter()\n+        throws OptimizationException {\n+        if (++iterations > maxIterations) {\n+            if (++iterations > maxIterations) {\n+                throw new OptimizationException(new MaxIterationsExceededException(maxIterations));\n+            }\n+        }\n+    }\n+\n+    /** \n+     * Compute the gradient vector.\n+     * @param point point at which the gradient must be evaluated\n+     * @return gradient at the specified point\n+     * @exception FunctionEvaluationException if the function gradient\n+     */\n+    protected double[] computeObjectiveGradient(final double[] point)\n+        throws FunctionEvaluationException {\n+        ++gradientEvaluations;\n+        return gradient.value(point);\n+    }\n+\n+    /** \n+     * Compute the objective function value.\n+     * @param point point at which the objective function must be evaluated\n+     * @return objective function value at specified point\n+     * @exception FunctionEvaluationException if the function cannot be evaluated\n+     * or its dimension doesn't match problem dimension\n+     */\n+    protected double computeObjectiveValue(final double[] point)\n+        throws FunctionEvaluationException {\n+        ++evaluations;\n+        return f.value(point);\n+    }\n+\n+    /** {@inheritDoc} */\n+    public RealPointValuePair optimize(final DifferentiableMultivariateRealFunction f,\n+                                         final GoalType goalType,\n+                                         final double[] startPoint)\n+        throws FunctionEvaluationException, OptimizationException, IllegalArgumentException {\n+\n+        // reset counters\n+        iterations          = 0;\n+        evaluations         = 0;\n+        gradientEvaluations = 0;\n+\n+        // store optimization problem characteristics\n+        this.f        = f;\n+        gradient      = f.gradient();\n+        this.goalType = goalType;\n+        point         = startPoint.clone();\n+\n+        return doOptimize();\n+\n+    }\n+\n+    /** Perform the bulk of optimization algorithm.\n+     * @return the point/value pair giving the optimal value for objective function\n+     * @exception FunctionEvaluationException if the objective function throws one during\n+     * the search\n+     * @exception OptimizationException if the algorithm failed to converge\n+     * @exception IllegalArgumentException if the start point dimension is wrong\n+     */\n+    abstract protected RealPointValuePair doOptimize()\n+        throws FunctionEvaluationException, OptimizationException, IllegalArgumentException;\n+\n+}\n--- /dev/null\n+++ b/src/java/org/apache/commons/math/optimization/general/ConjugateGradientFormula.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.commons.math.optimization.general;\n+\n+/**\n+ * Available choices of update formulas for the &beta; parameter\n+ * in {@link NonLinearConjugateGradientOptimizer}.\n+ * <p>\n+ * The &beta; parameter is used to compute the successive conjugate\n+ * search directions. For non-linear conjugate gradients, there are\n+ * two formulas to compute &beta;:\n+ * <ul>\n+ *   <li>Fletcher-Reeves formula</li>\n+ *   <li>Polak-Ribi&egrave;re formula</li>\n+ * </ul>\n+ * On the one hand, the Fletcher-Reeves formula is guaranteed to converge\n+ * if the start point is close enough of the optimum whether the\n+ * Polak-Ribi&egrave;re formula may not converge in rare cases. On the\n+ * other hand, the Polak-Ribi&egrave;re formula is often faster when it\n+ * does converge. Polak-Ribi&egrave;re is often used.\n+ * <p>\n+ * @see NonLinearConjugateGradientOptimizer\n+ * @version $Revision$ $Date$\n+ * @since 2.0\n+ */\n+public enum ConjugateGradientFormula {\n+\n+    /** Fletcher-Reeves formula. */\n+    FLETCHER_REEVES,\n+\n+    /** Polak-Ribi&egrave;re formula. */\n+    POLAK_RIBIERE\n+\n+}\n--- /dev/null\n+++ b/src/java/org/apache/commons/math/optimization/general/NonLinearConjugateGradientOptimizer.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.commons.math.optimization.general;\n+\n+import org.apache.commons.math.ConvergenceException;\n+import org.apache.commons.math.FunctionEvaluationException;\n+import org.apache.commons.math.analysis.UnivariateRealFunction;\n+import org.apache.commons.math.analysis.solvers.BrentSolver;\n+import org.apache.commons.math.analysis.solvers.UnivariateRealSolver;\n+import org.apache.commons.math.optimization.GoalType;\n+import org.apache.commons.math.optimization.OptimizationException;\n+import org.apache.commons.math.optimization.DifferentiableMultivariateRealOptimizer;\n+import org.apache.commons.math.optimization.RealPointValuePair;\n+import org.apache.commons.math.optimization.SimpleVectorialValueChecker;\n+\n+/** \n+ * Non-linear conjugate gradient optimizer.\n+ * <p>\n+ * This class supports both the Fletcher-Reeves and the Polak-Ribi&egrave;re\n+ * update formulas for the conjugate search directions. It also supports\n+ * optional preconditioning.\n+ * </p>\n+ *\n+ * @version $Revision$ $Date$\n+ * @since 2.0\n+ *\n+ */\n+\n+public class NonLinearConjugateGradientOptimizer\n+    extends AbstractScalarDifferentiableOptimizer\n+    implements DifferentiableMultivariateRealOptimizer {\n+\n+    /** Serializable version identifier. */\n+    private static final long serialVersionUID = -6545223926568155458L;\n+\n+    /** Update formula for the beta parameter. */\n+    private final ConjugateGradientFormula updateFormula;\n+\n+    /** Preconditioner (may be null). */\n+    private Preconditioner preconditioner;\n+\n+    /** solver to use in the line search (may be null). */\n+    private UnivariateRealSolver solver;\n+\n+    /** Initial step used to bracket the optimum in line search. */\n+    private double initialStep;\n+\n+    /** Simple constructor with default settings.\n+     * <p>The convergence check is set to a {@link SimpleVectorialValueChecker}\n+     * and the maximal number of evaluation is set to\n+     * {@link AbstractLeastSquaresOptimizer#DEFAULT_MAX_EVALUATIONS}.\n+     * @param updateFormula formula to use for updating the &beta; parameter,\n+     * must be one of {@link UpdateFormula#FLETCHER_REEVES} or {@link\n+     * UpdateFormula#POLAK_RIBIERE}\n+     */\n+    public NonLinearConjugateGradientOptimizer(final ConjugateGradientFormula updateFormula) {\n+        this.updateFormula = updateFormula;\n+        preconditioner     = null;\n+        solver             = null;\n+        initialStep        = 1.0;\n+    }\n+\n+    /**\n+     * Set the preconditioner.\n+     * @param preconditioner preconditioner to use for next optimization,\n+     * may be null to remove an already registered preconditioner\n+     */\n+    public void setPreconditioner(final Preconditioner preconditioner) {\n+        this.preconditioner = preconditioner;\n+    }\n+\n+    /**\n+     * Set the solver to use during line search.\n+     * @param solver solver to use during line search, may be null\n+     * to remove an already registered solver and fall back to the\n+     * default {@link BrentSolver Brent solver}.\n+     */\n+    public void setLineSearchSolver(final UnivariateRealSolver solver) {\n+        this.solver = solver;\n+    }\n+\n+    /**\n+     * Set the initial step used to bracket the optimum in line search.\n+     * <p>\n+     * The initial step is a factor with respect to the search direction,\n+     * which itself is roughly related to the gradient of the function\n+     * </p>\n+     * @param initialStep initial step used to bracket the optimum in line search,\n+     * if a non-positive value is used, the initial step is reset to its\n+     * default value of 1.0\n+     */\n+    public void setInitialStep(final double initialStep) {\n+        if (initialStep <= 0) {\n+            this.initialStep = 1.0;\n+        } else {\n+            this.initialStep = initialStep;\n+        }\n+    }\n+\n+    /** {@inheritDoc} */\n+    protected RealPointValuePair doOptimize()\n+        throws FunctionEvaluationException, OptimizationException, IllegalArgumentException {\n+        try {\n+\n+            // initialization\n+            if (preconditioner == null) {\n+                preconditioner = new IdentityPreconditioner();\n+            }\n+            if (solver == null) {\n+                solver = new BrentSolver();\n+            }\n+            final int n = point.length;\n+            double[] r = computeObjectiveGradient(point);\n+            if (goalType == GoalType.MINIMIZE) {\n+                for (int i = 0; i < n; ++i) {\n+                    r[i] = -r[i];\n+                }\n+            }\n+\n+            // initial search direction\n+            double[] steepestDescent = preconditioner.precondition(point, r);\n+            double[] searchDirection = steepestDescent.clone();\n+\n+            double delta = 0;\n+            for (int i = 0; i < n; ++i) {\n+                delta += r[i] * searchDirection[i];\n+            }\n+\n+            RealPointValuePair current = null;\n+            while (true) {\n+\n+                final double objective = computeObjectiveValue(point);\n+                RealPointValuePair previous = current;\n+                current = new RealPointValuePair(point, objective);\n+                if (previous != null) {\n+                    if (checker.converged(getIterations(), previous, current)) {\n+                        // we have found an optimum\n+                        return current;\n+                    }\n+                }\n+\n+                incrementIterationsCounter();\n+\n+                double dTd = 0;\n+                for (final double di : searchDirection) {\n+                    dTd += di * di;\n+                }\n+\n+                // find the optimal step in the search direction\n+                final UnivariateRealFunction lsf = new LineSearchFunction(searchDirection);\n+                final double step = solver.solve(lsf, 0, findUpperBound(lsf, 0, initialStep));\n+\n+                // validate new point\n+                for (int i = 0; i < point.length; ++i) {\n+                    point[i] += step * searchDirection[i];\n+                }\n+                r = computeObjectiveGradient(point);\n+                if (goalType == GoalType.MINIMIZE) {\n+                    for (int i = 0; i < n; ++i) {\n+                        r[i] = -r[i];\n+                    }\n+                }\n+\n+                // compute beta\n+                final double deltaOld = delta;\n+                final double[] newSteepestDescent = preconditioner.precondition(point, r);\n+                delta = 0;\n+                for (int i = 0; i < n; ++i) {\n+                    delta += r[i] * newSteepestDescent[i];\n+                }\n+\n+                final double beta;\n+                if (updateFormula == ConjugateGradientFormula.FLETCHER_REEVES) {\n+                    beta = delta / deltaOld;\n+                } else {\n+                    double deltaMid = 0;\n+                    for (int i = 0; i < r.length; ++i) {\n+                        deltaMid += r[i] * steepestDescent[i];\n+                    }                    \n+                    beta = (delta - deltaMid) / deltaOld;\n+                }\n+                steepestDescent = newSteepestDescent;\n+\n+                // compute conjugate search direction\n+                if ((getIterations() % n == 0) || (beta < 0)) {\n+                    // break conjugation: reset search direction\n+                    searchDirection = steepestDescent.clone();\n+                } else {\n+                    // compute new conjugate search direction\n+                    for (int i = 0; i < n; ++i) {\n+                        searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n+                    }\n+                }\n+\n+            }\n+\n+        } catch (ConvergenceException ce) {\n+            throw new OptimizationException(ce);\n+        }\n+    }\n+\n+    /**\n+     * Find the upper bound b ensuring bracketing of a root between a and b\n+     * @param f function whose root must be bracketed\n+     * @param a lower bound of the interval\n+     * @param h initial step to try\n+     * @return b such that f(a) and f(b) have opposite signs\n+     * @exception FunctionEvaluationException if the function cannot be computed\n+     * @exception OptimizationException if no bracket can be found\n+     */\n+    private double findUpperBound(final UnivariateRealFunction f,\n+                                  final double a, final double h)\n+        throws FunctionEvaluationException, OptimizationException {\n+        final double yA = f.value(a);\n+        double yB = yA;\n+        for (double step = h; step < Double.MAX_VALUE; step *= Math.max(2, yA / yB)) {\n+            final double b = a + step;\n+            yB = f.value(b);\n+            if (yA * yB <= 0) {\n+                return b;\n+            }\n+        }\n+        throw new OptimizationException(\"unable to bracket optimum in line search\");\n+    }\n+\n+    /** Default identity preconditioner. */\n+    private static class IdentityPreconditioner implements Preconditioner {\n+\n+        /** Serializable version identifier. */\n+        private static final long serialVersionUID = 1868235977809734023L;\n+\n+        /** {@inheritDoc} */\n+        public double[] precondition(double[] variables, double[] r) {\n+            return r.clone();\n+        }\n+\n+    }\n+\n+    /** Internal class for line search.\n+     * <p>\n+     * The function represented by this class is the dot product of\n+     * the objective function gradient and the search direction. Its\n+     * value is zero when the gradient is orthogonal to the search\n+     * direction, i.e. when the objective function value is a local\n+     * extremum along the search direction.\n+     * </p>\n+     */\n+    private class LineSearchFunction implements UnivariateRealFunction {\n+\n+        /** Serializable version identifier. */\n+        private static final long serialVersionUID = 8184683950487801424L;\n+\n+        /** Search direction. */\n+        private final double[] searchDirection;\n+\n+        /** Simple constructor.\n+         * @param searchDirection search direction\n+         */\n+        public LineSearchFunction(final double[] searchDirection) {\n+            this.searchDirection = searchDirection;\n+        }\n+\n+        /** {@inheritDoc} */\n+        public double value(double x) throws FunctionEvaluationException {\n+\n+            // current point in the search direction\n+            final double[] shiftedPoint = point.clone();\n+            for (int i = 0; i < shiftedPoint.length; ++i) {\n+                shiftedPoint[i] += x * searchDirection[i];\n+            }\n+\n+            // gradient of the objective function\n+            final double[] gradient = computeObjectiveGradient(shiftedPoint);\n+\n+            // dot product with the search direction\n+            double dotProduct = 0;\n+            for (int i = 0; i < gradient.length; ++i) {\n+                dotProduct += gradient[i] * searchDirection[i];\n+            }\n+\n+            return dotProduct;\n+\n+        }\n+\n+    }\n+\n+}\n--- /dev/null\n+++ b/src/java/org/apache/commons/math/optimization/general/Preconditioner.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.commons.math.optimization.general;\n+\n+import java.io.Serializable;\n+\n+import org.apache.commons.math.FunctionEvaluationException;\n+\n+/** \n+ * This interface represents a preconditioner for differentiable scalar\n+ * objective function optimizers.\n+ * @version $Revision$ $Date$\n+ * @since 2.0\n+ */\n+public interface Preconditioner extends Serializable {\n+\n+    /** \n+     * Precondition a search direction.\n+     * <p>\n+     * The returned preconditioned search direction must be computed fast or\n+     * the algorithm performances will drop drastically. A classical approach\n+     * is to compute only the diagonal elements of the hessian and to divide\n+     * the raw search direction by these elements if they are all positive.\n+     * If at least one of them is negative, it is safer to return a clone of\n+     * the raw search direction as if the hessian was the identity matrix. The\n+     * rationale for this simplified choice is that a negative diagonal element\n+     * means the current point is far from the optimum and preconditioning will\n+     * not be efficient anyway in this case.\n+     * </p>\n+     * @param point current point at which the search direction was computed\n+     * @param r raw search direction (i.e. opposite of the gradient)\n+     * @return approximation of H<sup>-1</sup>r where H is the objective function hessian\n+     * @exception FunctionEvaluationException if no cost can be computed for the parameters\n+     * @exception IllegalArgumentException if point dimension is wrong\n+     */\n+    double[] precondition(double[] point, double[] r)\n+        throws FunctionEvaluationException, IllegalArgumentException;\n+\n+}\n--- /dev/null\n+++ b/src/test/org/apache/commons/math/optimization/general/NonLinearConjugateGradientOptimizerTest.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.commons.math.optimization.general;\n+\n+import java.awt.geom.Point2D;\n+import java.util.ArrayList;\n+\n+import junit.framework.Test;\n+import junit.framework.TestCase;\n+import junit.framework.TestSuite;\n+\n+import org.apache.commons.math.FunctionEvaluationException;\n+import org.apache.commons.math.analysis.DifferentiableMultivariateRealFunction;\n+import org.apache.commons.math.analysis.MultivariateRealFunction;\n+import org.apache.commons.math.analysis.MultivariateVectorialFunction;\n+import org.apache.commons.math.analysis.solvers.BrentSolver;\n+import org.apache.commons.math.linear.DenseRealMatrix;\n+import org.apache.commons.math.linear.RealMatrix;\n+import org.apache.commons.math.optimization.GoalType;\n+import org.apache.commons.math.optimization.OptimizationException;\n+import org.apache.commons.math.optimization.RealPointValuePair;\n+import org.apache.commons.math.optimization.SimpleScalarValueChecker;\n+\n+/**\n+ * <p>Some of the unit tests are re-implementations of the MINPACK <a\n+ * href=\"http://www.netlib.org/minpack/ex/file17\">file17</a> and <a\n+ * href=\"http://www.netlib.org/minpack/ex/file22\">file22</a> test files. \n+ * The redistribution policy for MINPACK is available <a\n+ * href=\"http://www.netlib.org/minpack/disclaimer\">here</a>, for\n+ * convenience, it is reproduced below.</p>\n+\n+ * <table border=\"0\" width=\"80%\" cellpadding=\"10\" align=\"center\" bgcolor=\"#E0E0E0\">\n+ * <tr><td>\n+ *    Minpack Copyright Notice (1999) University of Chicago.\n+ *    All rights reserved\n+ * </td></tr>\n+ * <tr><td>\n+ * Redistribution and use in source and binary forms, with or without\n+ * modification, are permitted provided that the following conditions\n+ * are met:\n+ * <ol>\n+ *  <li>Redistributions of source code must retain the above copyright\n+ *      notice, this list of conditions and the following disclaimer.</li>\n+ * <li>Redistributions in binary form must reproduce the above\n+ *     copyright notice, this list of conditions and the following\n+ *     disclaimer in the documentation and/or other materials provided\n+ *     with the distribution.</li>\n+ * <li>The end-user documentation included with the redistribution, if any,\n+ *     must include the following acknowledgment:\n+ *     <code>This product includes software developed by the University of\n+ *           Chicago, as Operator of Argonne National Laboratory.</code>\n+ *     Alternately, this acknowledgment may appear in the software itself,\n+ *     if and wherever such third-party acknowledgments normally appear.</li>\n+ * <li><strong>WARRANTY DISCLAIMER. THE SOFTWARE IS SUPPLIED \"AS IS\"\n+ *     WITHOUT WARRANTY OF ANY KIND. THE COPYRIGHT HOLDER, THE\n+ *     UNITED STATES, THE UNITED STATES DEPARTMENT OF ENERGY, AND\n+ *     THEIR EMPLOYEES: (1) DISCLAIM ANY WARRANTIES, EXPRESS OR\n+ *     IMPLIED, INCLUDING BUT NOT LIMITED TO ANY IMPLIED WARRANTIES\n+ *     OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE\n+ *     OR NON-INFRINGEMENT, (2) DO NOT ASSUME ANY LEGAL LIABILITY\n+ *     OR RESPONSIBILITY FOR THE ACCURACY, COMPLETENESS, OR\n+ *     USEFULNESS OF THE SOFTWARE, (3) DO NOT REPRESENT THAT USE OF\n+ *     THE SOFTWARE WOULD NOT INFRINGE PRIVATELY OWNED RIGHTS, (4)\n+ *     DO NOT WARRANT THAT THE SOFTWARE WILL FUNCTION\n+ *     UNINTERRUPTED, THAT IT IS ERROR-FREE OR THAT ANY ERRORS WILL\n+ *     BE CORRECTED.</strong></li>\n+ * <li><strong>LIMITATION OF LIABILITY. IN NO EVENT WILL THE COPYRIGHT\n+ *     HOLDER, THE UNITED STATES, THE UNITED STATES DEPARTMENT OF\n+ *     ENERGY, OR THEIR EMPLOYEES: BE LIABLE FOR ANY INDIRECT,\n+ *     INCIDENTAL, CONSEQUENTIAL, SPECIAL OR PUNITIVE DAMAGES OF\n+ *     ANY KIND OR NATURE, INCLUDING BUT NOT LIMITED TO LOSS OF\n+ *     PROFITS OR LOSS OF DATA, FOR ANY REASON WHATSOEVER, WHETHER\n+ *     SUCH LIABILITY IS ASSERTED ON THE BASIS OF CONTRACT, TORT\n+ *     (INCLUDING NEGLIGENCE OR STRICT LIABILITY), OR OTHERWISE,\n+ *     EVEN IF ANY OF SAID PARTIES HAS BEEN WARNED OF THE\n+ *     POSSIBILITY OF SUCH LOSS OR DAMAGES.</strong></li>\n+ * <ol></td></tr>\n+ * </table>\n+\n+ * @author Argonne National Laboratory. MINPACK project. March 1980 (original fortran minpack tests)\n+ * @author Burton S. Garbow (original fortran minpack tests)\n+ * @author Kenneth E. Hillstrom (original fortran minpack tests)\n+ * @author Jorge J. More (original fortran minpack tests)\n+ * @author Luc Maisonobe (non-minpack tests and minpack tests Java translation)\n+ */\n+public class NonLinearConjugateGradientOptimizerTest\n+extends TestCase {\n+\n+    public NonLinearConjugateGradientOptimizerTest(String name) {\n+        super(name);\n+    }\n+\n+    public void testTrivial() throws FunctionEvaluationException, OptimizationException {\n+        LinearProblem problem =\n+            new LinearProblem(new double[][] { { 2 } }, new double[] { 3 });\n+        NonLinearConjugateGradientOptimizer optimizer =\n+            new NonLinearConjugateGradientOptimizer(ConjugateGradientFormula.POLAK_RIBIERE);\n+        optimizer.setMaxIterations(100);\n+        optimizer.setConvergenceChecker(new SimpleScalarValueChecker(1.0e-6, 1.0e-6));\n+        RealPointValuePair optimum =\n+            optimizer.optimize(problem, GoalType.MINIMIZE, new double[] { 0 });\n+        assertEquals(1.5, optimum.getPoint()[0], 1.0e-10);\n+        assertEquals(0.0, optimum.getValue(), 1.0e-10);\n+    }\n+\n+    public void testColumnsPermutation() throws FunctionEvaluationException, OptimizationException {\n+\n+        LinearProblem problem =\n+            new LinearProblem(new double[][] { { 1.0, -1.0 }, { 0.0, 2.0 }, { 1.0, -2.0 } },\n+                              new double[] { 4.0, 6.0, 1.0 });\n+\n+        NonLinearConjugateGradientOptimizer optimizer =\n+            new NonLinearConjugateGradientOptimizer(ConjugateGradientFormula.POLAK_RIBIERE);\n+        optimizer.setMaxIterations(100);\n+        optimizer.setConvergenceChecker(new SimpleScalarValueChecker(1.0e-6, 1.0e-6));\n+        RealPointValuePair optimum =\n+            optimizer.optimize(problem, GoalType.MINIMIZE, new double[] { 0, 0 });\n+        assertEquals(7.0, optimum.getPoint()[0], 1.0e-10);\n+        assertEquals(3.0, optimum.getPoint()[1], 1.0e-10);\n+        assertEquals(0.0, optimum.getValue(), 1.0e-10);\n+\n+    }\n+\n+    public void testNoDependency() throws FunctionEvaluationException, OptimizationException {\n+        LinearProblem problem = new LinearProblem(new double[][] {\n+                { 2, 0, 0, 0, 0, 0 },\n+                { 0, 2, 0, 0, 0, 0 },\n+                { 0, 0, 2, 0, 0, 0 },\n+                { 0, 0, 0, 2, 0, 0 },\n+                { 0, 0, 0, 0, 2, 0 },\n+                { 0, 0, 0, 0, 0, 2 }\n+        }, new double[] { 0.0, 1.1, 2.2, 3.3, 4.4, 5.5 });\n+        NonLinearConjugateGradientOptimizer optimizer =\n+            new NonLinearConjugateGradientOptimizer(ConjugateGradientFormula.POLAK_RIBIERE);\n+        optimizer.setMaxIterations(100);\n+        optimizer.setConvergenceChecker(new SimpleScalarValueChecker(1.0e-6, 1.0e-6));\n+        RealPointValuePair optimum =\n+            optimizer.optimize(problem, GoalType.MINIMIZE, new double[] { 0, 0, 0, 0, 0, 0 });\n+        for (int i = 0; i < problem.target.length; ++i) {\n+            assertEquals(0.55 * i, optimum.getPoint()[i], 1.0e-10);\n+        }\n+    }\n+\n+    public void testOneSet() throws FunctionEvaluationException, OptimizationException {\n+\n+        LinearProblem problem = new LinearProblem(new double[][] {\n+                {  1,  0, 0 },\n+                { -1,  1, 0 },\n+                {  0, -1, 1 }\n+        }, new double[] { 1, 1, 1});\n+        NonLinearConjugateGradientOptimizer optimizer =\n+            new NonLinearConjugateGradientOptimizer(ConjugateGradientFormula.POLAK_RIBIERE);\n+        optimizer.setMaxIterations(100);\n+        optimizer.setConvergenceChecker(new SimpleScalarValueChecker(1.0e-6, 1.0e-6));\n+        RealPointValuePair optimum =\n+            optimizer.optimize(problem, GoalType.MINIMIZE, new double[] { 0, 0, 0 });\n+        assertEquals(1.0, optimum.getPoint()[0], 1.0e-10);\n+        assertEquals(2.0, optimum.getPoint()[1], 1.0e-10);\n+        assertEquals(3.0, optimum.getPoint()[2], 1.0e-10);\n+\n+    }\n+\n+    public void testTwoSets() throws FunctionEvaluationException, OptimizationException {\n+        final double epsilon = 1.0e-7;\n+        LinearProblem problem = new LinearProblem(new double[][] {\n+                {  2,  1,   0,  4,       0, 0 },\n+                { -4, -2,   3, -7,       0, 0 },\n+                {  4,  1,  -2,  8,       0, 0 },\n+                {  0, -3, -12, -1,       0, 0 },\n+                {  0,  0,   0,  0, epsilon, 1 },\n+                {  0,  0,   0,  0,       1, 1 }\n+        }, new double[] { 2, -9, 2, 2, 1 + epsilon * epsilon, 2});\n+\n+        NonLinearConjugateGradientOptimizer optimizer =\n+            new NonLinearConjugateGradientOptimizer(ConjugateGradientFormula.POLAK_RIBIERE);\n+        optimizer.setMaxIterations(100);\n+        optimizer.setPreconditioner(new Preconditioner() {\n+            private static final long serialVersionUID = -2935127802358453014L;\n+            public double[] precondition(double[] point, double[] r) {\n+                double[] d = r.clone();\n+                d[0] /=  72.0;\n+                d[1] /=  30.0;\n+                d[2] /= 314.0;\n+                d[3] /= 260.0;\n+                d[4] /= 2 * (1 + epsilon * epsilon);\n+                d[5] /= 4.0;\n+                return d;\n+            }\n+        });\n+        optimizer.setConvergenceChecker(new SimpleScalarValueChecker(1.0e-13, 1.0e-13));\n+\n+        RealPointValuePair optimum =\n+            optimizer.optimize(problem, GoalType.MINIMIZE, new double[] { 0, 0, 0, 0, 0, 0 });\n+        assertEquals( 3.0, optimum.getPoint()[0], 1.0e-10);\n+        assertEquals( 4.0, optimum.getPoint()[1], 1.0e-10);\n+        assertEquals(-1.0, optimum.getPoint()[2], 1.0e-10);\n+        assertEquals(-2.0, optimum.getPoint()[3], 1.0e-10);\n+        assertEquals( 1.0 + epsilon, optimum.getPoint()[4], 1.0e-10);\n+        assertEquals( 1.0 - epsilon, optimum.getPoint()[5], 1.0e-10);\n+\n+    }\n+\n+    public void testNonInversible() throws FunctionEvaluationException, OptimizationException {\n+\n+        LinearProblem problem = new LinearProblem(new double[][] {\n+                {  1, 2, -3 },\n+                {  2, 1,  3 },\n+                { -3, 0, -9 }\n+        }, new double[] { 1, 1, 1 });\n+        NonLinearConjugateGradientOptimizer optimizer =\n+            new NonLinearConjugateGradientOptimizer(ConjugateGradientFormula.POLAK_RIBIERE);\n+        optimizer.setMaxIterations(100);\n+        optimizer.setConvergenceChecker(new SimpleScalarValueChecker(1.0e-6, 1.0e-6));\n+        RealPointValuePair optimum =\n+                optimizer.optimize(problem, GoalType.MINIMIZE, new double[] { 0, 0, 0 });\n+        assertTrue(optimum.getValue() > 0.5);\n+    }\n+\n+    public void testIllConditioned() throws FunctionEvaluationException, OptimizationException {\n+        LinearProblem problem1 = new LinearProblem(new double[][] {\n+                { 10.0, 7.0,  8.0,  7.0 },\n+                {  7.0, 5.0,  6.0,  5.0 },\n+                {  8.0, 6.0, 10.0,  9.0 },\n+                {  7.0, 5.0,  9.0, 10.0 }\n+        }, new double[] { 32, 23, 33, 31 });\n+        NonLinearConjugateGradientOptimizer optimizer =\n+            new NonLinearConjugateGradientOptimizer(ConjugateGradientFormula.POLAK_RIBIERE);\n+        optimizer.setMaxIterations(100);\n+        optimizer.setConvergenceChecker(new SimpleScalarValueChecker(1.0e-13, 1.0e-13));\n+        BrentSolver solver = new BrentSolver();\n+        solver.setAbsoluteAccuracy(1.0e-15);\n+        solver.setRelativeAccuracy(1.0e-15);\n+        optimizer.setLineSearchSolver(solver);\n+        RealPointValuePair optimum1 =\n+            optimizer.optimize(problem1, GoalType.MINIMIZE, new double[] { 0, 1, 2, 3 });\n+        assertEquals(1.0, optimum1.getPoint()[0], 1.0e-5);\n+        assertEquals(1.0, optimum1.getPoint()[1], 1.0e-5);\n+        assertEquals(1.0, optimum1.getPoint()[2], 1.0e-5);\n+        assertEquals(1.0, optimum1.getPoint()[3], 1.0e-5);\n+\n+        LinearProblem problem2 = new LinearProblem(new double[][] {\n+                { 10.00, 7.00, 8.10, 7.20 },\n+                {  7.08, 5.04, 6.00, 5.00 },\n+                {  8.00, 5.98, 9.89, 9.00 },\n+                {  6.99, 4.99, 9.00, 9.98 }\n+        }, new double[] { 32, 23, 33, 31 });\n+        RealPointValuePair optimum2 =\n+            optimizer.optimize(problem2, GoalType.MINIMIZE, new double[] { 0, 1, 2, 3 });\n+        assertEquals(-81.0, optimum2.getPoint()[0], 1.0e-1);\n+        assertEquals(137.0, optimum2.getPoint()[1], 1.0e-1);\n+        assertEquals(-34.0, optimum2.getPoint()[2], 1.0e-1);\n+        assertEquals( 22.0, optimum2.getPoint()[3], 1.0e-1);\n+\n+    }\n+\n+    public void testMoreEstimatedParametersSimple()\n+        throws FunctionEvaluationException, OptimizationException {\n+\n+        LinearProblem problem = new LinearProblem(new double[][] {\n+                { 3.0, 2.0,  0.0, 0.0 },\n+                { 0.0, 1.0, -1.0, 1.0 },\n+                { 2.0, 0.0,  1.0, 0.0 }\n+        }, new double[] { 7.0, 3.0, 5.0 });\n+\n+        NonLinearConjugateGradientOptimizer optimizer =\n+            new NonLinearConjugateGradientOptimizer(ConjugateGradientFormula.POLAK_RIBIERE);\n+        optimizer.setMaxIterations(100);\n+        optimizer.setConvergenceChecker(new SimpleScalarValueChecker(1.0e-6, 1.0e-6));\n+        RealPointValuePair optimum =\n+            optimizer.optimize(problem, GoalType.MINIMIZE, new double[] { 7, 6, 5, 4 });\n+        assertEquals(0, optimum.getValue(), 1.0e-10);\n+\n+    }\n+\n+    public void testMoreEstimatedParametersUnsorted()\n+        throws FunctionEvaluationException, OptimizationException {\n+        LinearProblem problem = new LinearProblem(new double[][] {\n+                 { 1.0, 1.0,  0.0,  0.0, 0.0,  0.0 },\n+                 { 0.0, 0.0,  1.0,  1.0, 1.0,  0.0 },\n+                 { 0.0, 0.0,  0.0,  0.0, 1.0, -1.0 },\n+                 { 0.0, 0.0, -1.0,  1.0, 0.0,  1.0 },\n+                 { 0.0, 0.0,  0.0, -1.0, 1.0,  0.0 }\n+        }, new double[] { 3.0, 12.0, -1.0, 7.0, 1.0 });\n+        NonLinearConjugateGradientOptimizer optimizer =\n+            new NonLinearConjugateGradientOptimizer(ConjugateGradientFormula.POLAK_RIBIERE);\n+        optimizer.setMaxIterations(100);\n+        optimizer.setConvergenceChecker(new SimpleScalarValueChecker(1.0e-6, 1.0e-6));\n+        RealPointValuePair optimum =\n+            optimizer.optimize(problem, GoalType.MINIMIZE, new double[] { 2, 2, 2, 2, 2, 2 });\n+        assertEquals(0, optimum.getValue(), 1.0e-10);\n+    }\n+\n+    public void testRedundantEquations() throws FunctionEvaluationException, OptimizationException {\n+        LinearProblem problem = new LinearProblem(new double[][] {\n+                { 1.0,  1.0 },\n+                { 1.0, -1.0 },\n+                { 1.0,  3.0 }\n+        }, new double[] { 3.0, 1.0, 5.0 });\n+\n+        NonLinearConjugateGradientOptimizer optimizer =\n+            new NonLinearConjugateGradientOptimizer(ConjugateGradientFormula.POLAK_RIBIERE);\n+        optimizer.setMaxIterations(100);\n+        optimizer.setConvergenceChecker(new SimpleScalarValueChecker(1.0e-6, 1.0e-6));\n+        RealPointValuePair optimum =\n+            optimizer.optimize(problem, GoalType.MINIMIZE, new double[] { 1, 1 });\n+        assertEquals(2.0, optimum.getPoint()[0], 1.0e-8);\n+        assertEquals(1.0, optimum.getPoint()[1], 1.0e-8);\n+\n+    }\n+\n+    public void testInconsistentEquations() throws FunctionEvaluationException, OptimizationException {\n+        LinearProblem problem = new LinearProblem(new double[][] {\n+                { 1.0,  1.0 },\n+                { 1.0, -1.0 },\n+                { 1.0,  3.0 }\n+        }, new double[] { 3.0, 1.0, 4.0 });\n+\n+        NonLinearConjugateGradientOptimizer optimizer =\n+            new NonLinearConjugateGradientOptimizer(ConjugateGradientFormula.POLAK_RIBIERE);\n+        optimizer.setMaxIterations(100);\n+        optimizer.setConvergenceChecker(new SimpleScalarValueChecker(1.0e-6, 1.0e-6));\n+        RealPointValuePair optimum =\n+            optimizer.optimize(problem, GoalType.MINIMIZE, new double[] { 1, 1 });\n+        assertTrue(optimum.getValue() > 0.1);\n+\n+    }\n+\n+    public void testCircleFitting() throws FunctionEvaluationException, OptimizationException {\n+        Circle circle = new Circle();\n+        circle.addPoint( 30.0,  68.0);\n+        circle.addPoint( 50.0,  -6.0);\n+        circle.addPoint(110.0, -20.0);\n+        circle.addPoint( 35.0,  15.0);\n+        circle.addPoint( 45.0,  97.0);\n+        NonLinearConjugateGradientOptimizer optimizer =\n+            new NonLinearConjugateGradientOptimizer(ConjugateGradientFormula.POLAK_RIBIERE);\n+        optimizer.setMaxIterations(100);\n+        optimizer.setConvergenceChecker(new SimpleScalarValueChecker(1.0e-30, 1.0e-30));\n+        BrentSolver solver = new BrentSolver();\n+        solver.setAbsoluteAccuracy(1.0e-13);\n+        solver.setRelativeAccuracy(1.0e-15);\n+        optimizer.setLineSearchSolver(solver);\n+        RealPointValuePair optimum =\n+            optimizer.optimize(circle, GoalType.MINIMIZE, new double[] { 98.680, 47.345 });\n+        Point2D.Double center = new Point2D.Double(optimum.getPointRef()[0], optimum.getPointRef()[1]);\n+        assertEquals(69.960161753, circle.getRadius(center), 1.0e-8);\n+        assertEquals(96.075902096, center.x, 1.0e-8);\n+        assertEquals(48.135167894, center.y, 1.0e-8);\n+    }\n+\n+    private static class LinearProblem implements DifferentiableMultivariateRealFunction {\n+\n+        private static final long serialVersionUID = 703247177355019415L;\n+        final RealMatrix factors;\n+        final double[] target;\n+        public LinearProblem(double[][] factors, double[] target) {\n+            this.factors = new DenseRealMatrix(factors);\n+            this.target  = target;\n+        }\n+\n+        private double[] gradient(double[] point) {\n+            double[] r = factors.operate(point);\n+            for (int i = 0; i < r.length; ++i) {\n+                r[i] -= target[i];\n+            }\n+            double[] p = factors.transpose().operate(r);\n+            for (int i = 0; i < p.length; ++i) {\n+                p[i] *= 2;\n+            }\n+            return p;\n+        }\n+\n+        public double value(double[] variables) throws FunctionEvaluationException {\n+            double[] y = factors.operate(variables);\n+            double sum = 0;\n+            for (int i = 0; i < y.length; ++i) {\n+                double ri = y[i] - target[i];\n+                sum += ri * ri;\n+            }\n+            return sum;\n+        }\n+\n+        public MultivariateVectorialFunction gradient() {\n+            return new MultivariateVectorialFunction() {\n+                private static final long serialVersionUID = 2621997811350805819L;\n+                public double[] value(double[] point) {\n+                    return gradient(point);\n+                }\n+            };\n+        }\n+\n+        public MultivariateRealFunction partialDerivative(final int k) {\n+            return new MultivariateRealFunction() {\n+                private static final long serialVersionUID = -6186178619133562011L;\n+                public double value(double[] point) {\n+                    return gradient(point)[k];\n+                }\n+            };\n+        }\n+\n+    }\n+\n+    private static class Circle implements DifferentiableMultivariateRealFunction {\n+\n+        private static final long serialVersionUID = -4711170319243817874L;\n+\n+        private ArrayList<Point2D.Double> points;\n+\n+        public Circle() {\n+            points  = new ArrayList<Point2D.Double>();\n+        }\n+\n+        public void addPoint(double px, double py) {\n+            points.add(new Point2D.Double(px, py));\n+        }\n+\n+        public int getN() {\n+            return points.size();\n+        }\n+\n+        public double getRadius(Point2D.Double center) {\n+            double r = 0;\n+            for (Point2D.Double point : points) {\n+                r += point.distance(center);\n+            }\n+            return r / points.size();\n+        }\n+\n+        private double[] gradient(double[] point) {\n+\n+            // optimal radius\n+            Point2D.Double center = new Point2D.Double(point[0], point[1]);\n+            double radius = getRadius(center);\n+\n+            // gradient of the sum of squared residuals\n+            double dJdX = 0;\n+            double dJdY = 0;\n+            for (Point2D.Double pk : points) {\n+                double dk = pk.distance(center);\n+                dJdX += (center.x - pk.x) * (dk - radius) / dk;\n+                dJdY += (center.y - pk.y) * (dk - radius) / dk;\n+            }\n+            dJdX *= 2;\n+            dJdY *= 2;\n+\n+            return new double[] { dJdX, dJdY };\n+\n+        }\n+\n+        public double value(double[] variables)\n+                throws IllegalArgumentException, FunctionEvaluationException {\n+\n+            Point2D.Double center = new Point2D.Double(variables[0], variables[1]);\n+            double radius = getRadius(center);\n+\n+            double sum = 0;\n+            for (Point2D.Double point : points) {\n+                double di = point.distance(center) - radius;\n+                sum += di * di;\n+            }\n+\n+            return sum;\n+\n+        }\n+\n+        public MultivariateVectorialFunction gradient() {\n+            return new MultivariateVectorialFunction() {\n+                private static final long serialVersionUID = 3174909643301201710L;\n+                public double[] value(double[] point) {\n+                    return gradient(point);\n+                }\n+            };\n+        }\n+\n+        public MultivariateRealFunction partialDerivative(final int k) {\n+            return new MultivariateRealFunction() {\n+                private static final long serialVersionUID = 3073956364104833888L;\n+                public double value(double[] point) {\n+                    return gradient(point)[k];\n+                }\n+            };\n+        }\n+\n+    }\n+\n+    public static Test suite() {\n+        return new TestSuite(NonLinearConjugateGradientOptimizerTest.class);\n+    }\n+\n+}", "timestamp": 1237932981, "metainfo": ""}