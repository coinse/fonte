{"sha": "4b7a3596be872ea15ee1cece9526a3f7cd4dc968", "log": "Javadoc only.  Cleanup formatting.  ", "commit": "\n--- a/src/java/org/apache/commons/math/stat/regression/OLSMultipleLinearRegression.java\n+++ b/src/java/org/apache/commons/math/stat/regression/OLSMultipleLinearRegression.java\n  * multiple linear regression model.</p>\n  * \n  * <p>OLS assumes the covariance matrix of the error to be diagonal and with\n- * equal variance.\n- * <pre>\n- * u ~ N(0, sigma^2*I)\n- * </pre></p>\n+ * equal variance.</p>\n+ * <p>\n+ * u ~ N(0, &sigma;<sup>2</sup>I)\n+ * </p>\n  * \n  * <p>The regression coefficients, b, satisfy the normal equations:\n- * <pre>\n- * X^T X b = X^T y\n- * </pre></p>\n+ * <p>\n+ * X<sup>T</sup> X b = X<sup>T</sup> y\n+ * </p>\n  * \n  * <p>To solve the normal equations, this implementation uses QR decomposition\n  * of the X matrix. (See {@link QRDecompositionImpl} for details on the\n  * decomposition algorithm.)\n- * <pre>\n- * X^T X b = X^T y\n- * (QR)^T (QR) b = (QR)^T y\n- * R^T (Q^T Q) R b = R^T Q^T y\n- * R^T R b = R^T Q^T y\n- * (R^T)^{-1} R^T R b = (R^T)^{-1} R^T Q^T y\n- * R b = Q^T y\n- * </pre>\n+ * </p>\n+ * <p>X<sup>T</sup>X b = X<sup>T</sup> y <br/>\n+ * (QR)<sup>T</sup> (QR) b = (QR)<sup>T</sup>y <br/>\n+ * R<sup>T</sup> (Q<sup>T</sup>Q) R b = R<sup>T</sup> Q<sup>T</sup> y <br/>\n+ * R<sup>T</sup> R b = R<sup>T</sup> Q<sup>T</sup> y <br/>\n+ * (R<sup>T</sup>)<sup>-1</sup> R<sup>T</sup> R b = (R<sup>T</sup>)<sup>-1</sup> R<sup>T</sup> Q<sup>T</sup> y <br/>\n+ * R b = Q<sup>T</sup> y\n+ * </p>\n  * Given Q and R, the last equation is solved by back-subsitution.</p>\n  * \n  * @version $Revision$ $Date$\n      * <p>Compute the \"hat\" matrix.\n      * </p>\n      * <p>The hat matrix is defined in terms of the design matrix X\n-     *  by X(X^TX)^-1X^T\n-     * <p>\n+     *  by X(X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>\n+     * </p>\n      * <p>The implementation here uses the QR decomposition to compute the\n-     * hat matrix as QIpQ^T where Ip is the p-dimensional identity matrix\n-     * augmented by 0's.  This computational formula is from \"The Hat Matrix\n-     * in Regression and ANOVA\", David C. Hoaglin and Roy E. Welsch, \n-     * The American Statistician, Vol. 32, No. 1 (Feb., 1978), pp. 17-22.\n+     * hat matrix as Q I<sub>p</sub>Q<sup>T</sup> where I<sub>p</sub> is the\n+     * p-dimensional identity matrix augmented by 0's.  This computational\n+     * formula is from \"The Hat Matrix in Regression and ANOVA\",\n+     * David C. Hoaglin and Roy E. Welsch, \n+     * <i>The American Statistician</i>, Vol. 32, No. 1 (Feb., 1978), pp. 17-22.\n      * \n      * @return the hat matrix\n      */\n     }\n \n     /**\n-     * Calculates the variance on the beta by OLS.\n-     * <pre>\n-     *  Var(b)=(X'X)^-1\n-     * </pre>\n+     * <p>Calculates the variance on the beta by OLS.\n+     * </p>\n+     * <p>Var(b) = (X<sup>T</sup>X)<sup>-1</sup>\n+     * </p>\n+     * \n      * @return The beta variance\n      */\n     protected RealMatrix calculateBetaVariance() {\n     \n \n     /**\n-     * Calculates the variance on the Y by OLS.\n-     * <pre>\n-     *  Var(y)=Tr(u'u)/(n-k)\n-     * </pre>\n+     * <p>Calculates the variance on the Y by OLS.\n+     * </p>\n+     * <p> Var(y) = Tr(u<sup>T</sup>u)/(n - k)\n+     * </p>\n      * @return The Y variance\n      */\n     protected double calculateYVariance() {", "timestamp": 1231094309, "metainfo": ""}