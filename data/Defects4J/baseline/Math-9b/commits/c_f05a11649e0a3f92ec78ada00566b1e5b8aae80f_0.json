{"sha": "f05a11649e0a3f92ec78ada00566b1e5b8aae80f", "log": "Javadoc only.  ", "commit": "\n--- a/src/main/java/org/apache/commons/math/optimization/direct/CMAESOptimizer.java\n+++ b/src/main/java/org/apache/commons/math/optimization/direct/CMAESOptimizer.java\n import org.apache.commons.math.util.MathUtils;\n \n /**\n- * CMA-ES algorithm. This code is translated and adapted from the Matlab version\n- * of this algorithm as implemented in module {@code cmaes.m} version 3.51.\n- *\n- * Implements the active Covariance Matrix Adaptation Evolution Strategy (CMA-ES)\n+ * <p>An implementation of the active Covariance Matrix Adaptation Evolution Strategy (CMA-ES)\n  * for non-linear, non-convex, non-smooth, global function minimization.\n  * The CMA-Evolution Strategy (CMA-ES) is a reliable stochastic optimization method\n- * which should be applied, if derivative based methods, e.g. quasi-Newton BFGS or\n+ * which should be applied if derivative-based methods, e.g. quasi-Newton BFGS or\n  * conjugate gradient, fail due to a rugged search landscape (e.g. noise, local\n- * optima, outlier, etc.)  of the objective function. Like a\n- * quasi-Newton method the CMA-ES learns and applies a variable metric\n- * of the underlying search space. Unlike a quasi-Newton method the\n- * CMA-ES does neither estimate nor use gradients, making it considerably more\n- * reliable in terms of finding a good, or even close to optimal, solution, finally.\n+ * optima, outlier, etc.) of the objective function. Like a\n+ * quasi-Newton method, the CMA-ES learns and applies a variable metric\n+ * on the underlying search space. Unlike a quasi-Newton method, the\n+ * CMA-ES neither estimates nor uses gradients, making it considerably more\n+ * reliable in terms of finding a good, or even close to optimal, solution.</p>\n  *\n  * <p>In general, on smooth objective functions the CMA-ES is roughly ten times\n  * slower than BFGS (counting objective function evaluations, no gradients provided).\n  * far less reliable than CMA-ES.</p>\n  *\n  * <p>The CMA-ES is particularly well suited for non-separable\n- * and/or badly conditioned problems.\n- * To observe the advantage of CMA compared to a conventional\n- * evolution strategy, it will usually take about <math>30 N</math> function\n- * evaluations. On difficult problems the complete\n+ * and/or badly conditioned problems. To observe the advantage of CMA compared\n+ * to a conventional evolution strategy, it will usually take about\n+ * <math>30 N</math> function evaluations. On difficult problems the complete\n  * optimization (a single run) is expected to take <em>roughly</em> between\n  * <math>30 N</math> and <math>300 N<sup>2</sup></math>\n  * function evaluations.</p>\n+ * \n+ * <p>This implementation is translated and adapted from the Matlab version\n+ * of the CMA-ES algorithm as implemented in module {@code cmaes.m} version 3.51.</p>\n  *\n  * For more information, please refer to the following links:\n  * <ul>\n      * Population size, offspring number. The primary strategy parameter to play\n      * with, which can be increased from its default value. Increasing the\n      * population size improves global search properties in exchange to speed.\n-     * Speed decreases, as a rule, at most linearely with increasing population\n+     * Speed decreases, as a rule, at most linearly with increasing population\n      * size. It is advisable to begin with the default small population size.\n      */\n     private int lambda; // population size\n     /**\n      * @param m\n      *            Input matrix.\n-     * @return Diagonal n X n matrix if m is a column matrix, Rolumn matrix\n+     * @return Diagonal n X n matrix if m is a column matrix, Column matrix\n      *         representing the diagonal if m is a nXn matrix.\n      */\n     private static RealMatrix diag(final RealMatrix m) {", "timestamp": 1301956831, "metainfo": ""}