{"sha": "0320f5e668e2784bd6f6bce077bfff9bc9683370", "log": "Formatting. \"final\" keyword.   ", "commit": "\n--- a/src/main/java/org/apache/commons/math3/optimization/direct/CMAESOptimizer.java\n+++ b/src/main/java/org/apache/commons/math3/optimization/direct/CMAESOptimizer.java\n         // -------------------- Generation Loop --------------------------------\n \n         generationLoop:\n-            for (iterations = 1; iterations <= maxIterations; iterations++) {\n-                // Generate and evaluate lambda offspring\n-                RealMatrix arz = randn1(dimension, lambda);\n-                RealMatrix arx = zeros(dimension, lambda);\n-                double[] fitness = new double[lambda];\n-                // generate random offspring\n-                for (int k = 0; k < lambda; k++) {\n-                    RealMatrix arxk = null;\n-                    for (int i = 0; i < checkFeasableCount+1; i++) {\n-                        if (diagonalOnly <= 0) {\n-                            arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n-                                    .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n-                        } else {\n-                            arxk = xmean.add(times(diagD,arz.getColumnMatrix(k))\n-                                    .scalarMultiply(sigma));\n-                        }\n-                        if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n-                            break;\n-                        }\n-                        // regenerate random arguments for row\n-                        arz.setColumn(k, randn(dimension));\n+        for (iterations = 1; iterations <= maxIterations; iterations++) {\n+            // Generate and evaluate lambda offspring\n+            final RealMatrix arz = randn1(dimension, lambda);\n+            final RealMatrix arx = zeros(dimension, lambda);\n+            final double[] fitness = new double[lambda];\n+            // generate random offspring\n+            for (int k = 0; k < lambda; k++) {\n+                RealMatrix arxk = null;\n+                for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                    if (diagonalOnly <= 0) {\n+                        arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n+                                         .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n+                    } else {\n+                        arxk = xmean.add(times(diagD,arz.getColumnMatrix(k))\n+                                         .scalarMultiply(sigma));\n                     }\n-                    copyColumn(arxk, 0, arx, k);\n-                    try {\n-                        fitness[k] = fitfun.value(arx.getColumn(k)); // compute fitness\n-                    } catch (TooManyEvaluationsException e) {\n+                    if (i >= checkFeasableCount ||\n+                        fitfun.isFeasible(arxk.getColumn(0))) {\n+                        break;\n+                    }\n+                    // regenerate random arguments for row\n+                    arz.setColumn(k, randn(dimension));\n+                }\n+                copyColumn(arxk, 0, arx, k);\n+                try {\n+                    fitness[k] = fitfun.value(arx.getColumn(k)); // compute fitness\n+                } catch (TooManyEvaluationsException e) {\n+                    break generationLoop;\n+                }\n+            }\n+            // Sort by fitness and compute weighted mean into xmean\n+            final int[] arindex = sortedIndices(fitness);\n+            // Calculate new xmean, this is selection and recombination\n+            final RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n+            final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n+            xmean = bestArx.multiply(weights);\n+            final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n+            final RealMatrix zmean = bestArz.multiply(weights);\n+            final boolean hsig = updateEvolutionPaths(zmean, xold);\n+            if (diagonalOnly <= 0) {\n+                updateCovariance(hsig, bestArx, arz, arindex, xold);\n+            } else {\n+                updateCovarianceDiagonalOnly(hsig, bestArz, xold);\n+            }\n+            // Adapt step size sigma - Eq. (5)\n+            sigma *= Math.exp(Math.min(1, (normps/chiN - 1) * cs / damps));\n+            final double bestFitness = fitness[arindex[0]];\n+            final double worstFitness = fitness[arindex[arindex.length - 1]];\n+            if (bestValue > bestFitness) {\n+                bestValue = bestFitness;\n+                lastResult = optimum;\n+                optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)),\n+                                             isMinimize ? bestFitness : -bestFitness);\n+                if (getConvergenceChecker() != null &&\n+                    lastResult != null) {\n+                    if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                         break generationLoop;\n                     }\n                 }\n-                // Sort by fitness and compute weighted mean into xmean\n-                int[] arindex = sortedIndices(fitness);\n-                // Calculate new xmean, this is selection and recombination\n-                RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n-                RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n-                xmean = bestArx.multiply(weights);\n-                RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n-                RealMatrix zmean = bestArz.multiply(weights);\n-                boolean hsig = updateEvolutionPaths(zmean, xold);\n-                if (diagonalOnly <= 0) {\n-                    updateCovariance(hsig, bestArx, arz, arindex, xold);\n-                } else {\n-                    updateCovarianceDiagonalOnly(hsig, bestArz, xold);\n-                }\n-                // Adapt step size sigma - Eq. (5)\n-                sigma *= Math.exp(Math.min(1, (normps/chiN - 1) * cs / damps));\n-                double bestFitness = fitness[arindex[0]];\n-                double worstFitness = fitness[arindex[arindex.length - 1]];\n-                if (bestValue > bestFitness) {\n-                    bestValue = bestFitness;\n-                    lastResult = optimum;\n-                    optimum = new PointValuePair(\n-                            fitfun.repair(bestArx.getColumn(0)),\n-                            isMinimize ? bestFitness : -bestFitness);\n-                    if (getConvergenceChecker() != null && lastResult != null) {\n-                        if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n-                            break generationLoop;\n-                        }\n+            }\n+            // handle termination criteria\n+            // Break, if fitness is good enough\n+            if (stopFitness != 0) { // only if stopFitness is defined\n+                if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                    break generationLoop;\n+                }\n+            }\n+            final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+            final double[] pcCol = pc.getColumn(0);\n+            for (int i = 0; i < dimension; i++) {\n+                if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                    break;\n+                }\n+                if (i >= dimension - 1) {\n+                    break generationLoop;\n+                }\n+            }\n+            for (int i = 0; i < dimension; i++) {\n+                if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                    break generationLoop;\n+                }\n+            }\n+            final double historyBest = min(fitnessHistory);\n+            final double historyWorst = max(fitnessHistory);\n+            if (iterations > 2 &&\n+                Math.max(historyWorst, worstFitness) -\n+                Math.min(historyBest, bestFitness) < stopTolFun) {\n+                break generationLoop;\n+            }\n+            if (iterations > fitnessHistory.length &&\n+                historyWorst-historyBest < stopTolHistFun) {\n+                break generationLoop;\n+            }\n+            // condition number of the covariance matrix exceeds 1e14\n+            if (max(diagD)/min(diagD) > 1e7) {\n+                break generationLoop;\n+            }\n+            // user defined termination\n+            if (getConvergenceChecker() != null) {\n+                final PointValuePair current\n+                    = new PointValuePair(bestArx.getColumn(0),\n+                                         isMinimize ? bestFitness : -bestFitness);\n+                if (lastResult != null &&\n+                    getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                    break generationLoop;\n                     }\n-                }\n-                // handle termination criteria\n-                // Break, if fitness is good enough\n-                if (stopFitness != 0) { // only if stopFitness is defined\n-                    if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n-                        break generationLoop;\n-                    }\n-                }\n-                double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n-                double[] pcCol = pc.getColumn(0);\n-                for (int i = 0; i < dimension; i++) {\n-                    if (sigma*(Math.max(Math.abs(pcCol[i]), sqrtDiagC[i])) > stopTolX) {\n-                        break;\n-                    }\n-                    if (i >= dimension-1) {\n-                        break generationLoop;\n-                    }\n-                }\n-                for (int i = 0; i < dimension; i++) {\n-                    if (sigma*sqrtDiagC[i] > stopTolUpX) {\n-                        break generationLoop;\n-                    }\n-                }\n-                double historyBest = min(fitnessHistory);\n-                double historyWorst = max(fitnessHistory);\n-                if (iterations > 2 && Math.max(historyWorst, worstFitness) -\n-                        Math.min(historyBest, bestFitness) < stopTolFun) {\n-                    break generationLoop;\n-                }\n-                if (iterations > fitnessHistory.length &&\n-                        historyWorst-historyBest < stopTolHistFun) {\n-                    break generationLoop;\n-                }\n-                // condition number of the covariance matrix exceeds 1e14\n-                if (max(diagD)/min(diagD) > 1e7) {\n-                    break generationLoop;\n-                }\n-                // user defined termination\n-                if (getConvergenceChecker() != null) {\n-                    PointValuePair current =\n-                        new PointValuePair(bestArx.getColumn(0),\n-                                isMinimize ? bestFitness : -bestFitness);\n-                    if (lastResult != null &&\n-                        getConvergenceChecker().converged(iterations, current, lastResult)) {\n-                        break generationLoop;\n-                    }\n-                    lastResult = current;\n-                }\n-                // Adjust step size in case of equal function values (flat fitness)\n-                if (bestValue == fitness[arindex[(int)(0.1+lambda/4.)]]) {\n-                    sigma = sigma * Math.exp(0.2 + cs / damps);\n-                }\n-                if (iterations > 2 && Math.max(historyWorst, bestFitness) -\n-                        Math.min(historyBest, bestFitness) == 0) {\n-                    sigma = sigma * Math.exp(0.2 + cs / damps);\n-                }\n-                // store best in history\n-                push(fitnessHistory,bestFitness);\n-                fitfun.setValueRange(worstFitness-bestFitness);\n-                if (generateStatistics) {\n-                    statisticsSigmaHistory.add(sigma);\n-                    statisticsFitnessHistory.add(bestFitness);\n-                    statisticsMeanHistory.add(xmean.transpose());\n-                    statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n-                }\n-            }\n+                lastResult = current;\n+            }\n+            // Adjust step size in case of equal function values (flat fitness)\n+            if (bestValue == fitness[arindex[(int)(0.1+lambda/4.)]]) {\n+                sigma = sigma * Math.exp(0.2 + cs / damps);\n+            }\n+            if (iterations > 2 && Math.max(historyWorst, bestFitness) -\n+                Math.min(historyBest, bestFitness) == 0) {\n+                sigma = sigma * Math.exp(0.2 + cs / damps);\n+            }\n+            // store best in history\n+            push(fitnessHistory,bestFitness);\n+            fitfun.setValueRange(worstFitness-bestFitness);\n+            if (generateStatistics) {\n+                statisticsSigmaHistory.add(sigma);\n+                statisticsFitnessHistory.add(bestFitness);\n+                statisticsMeanHistory.add(xmean.transpose());\n+                statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+            }\n+        }\n         return optimum;\n     }\n \n             lambda = 4 + (int) (3 * Math.log(dimension));\n         }\n         // initialize sigma\n-        double[][] sigmaArray = new double[guess.length][1];\n+        final double[][] sigmaArray = new double[guess.length][1];\n         for (int i = 0; i < guess.length; i++) {\n             sigmaArray[i][0] = inputSigma == null ? 0.3 : inputSigma[i];\n         }\n-        RealMatrix insigma = new Array2DRowRealMatrix(sigmaArray, false);\n+        final RealMatrix insigma = new Array2DRowRealMatrix(sigmaArray, false);\n         sigma = max(insigma); // overall standard deviation\n \n         // initialize termination criteria\n                 B.multiply(zmean).scalarMultiply(\n                         Math.sqrt(cs * (2 - cs) * mueff)));\n         normps = ps.getFrobeniusNorm();\n-        boolean hsig = normps /\n+        final boolean hsig = normps /\n             Math.sqrt(1 - Math.pow(1 - cs, 2 * iterations)) /\n             chiN < 1.4 + 2 / ((double) dimension + 1);\n         pc = pc.scalarMultiply(1 - cc);\n                                   final RealMatrix xold) {\n         double negccov = 0;\n         if (ccov1 + ccovmu > 0) {\n-            RealMatrix arpos = bestArx.subtract(repmat(xold, 1, mu))\n-                    .scalarMultiply(1 / sigma); // mu difference vectors\n-            RealMatrix roneu = pc.multiply(pc.transpose())\n-                    .scalarMultiply(ccov1); // rank one update\n+            final RealMatrix arpos = bestArx.subtract(repmat(xold, 1, mu))\n+                .scalarMultiply(1 / sigma); // mu difference vectors\n+            final RealMatrix roneu = pc.multiply(pc.transpose())\n+                .scalarMultiply(ccov1); // rank one update\n             // minor correction if hsig==false\n             double oldFac = hsig ? 0 : ccov1 * cc * (2 - cc);\n             oldFac += 1 - ccov1 - ccovmu;\n                 // Adapt covariance matrix C active CMA\n                 negccov = (1 - ccovmu) * 0.25 * mueff /\n                     (Math.pow(dimension + 2, 1.5) + 2 * mueff);\n-                double negminresidualvariance = 0.66;\n                 // keep at least 0.66 in all directions, small popsize are most\n                 // critical\n-                double negalphaold = 0.5; // where to make up for the variance\n-                                          // loss,\n+                final double negminresidualvariance = 0.66;\n+                // where to make up for the variance loss\n+                final double negalphaold = 0.5;\n                 // prepare vectors, compute negative updating matrix Cneg\n-                int[] arReverseIndex = reverse(arindex);\n+                final int[] arReverseIndex = reverse(arindex);\n                 RealMatrix arzneg = selectColumns(arz, MathArrays.copyOf(arReverseIndex, mu));\n                 RealMatrix arnorms = sqrt(sumRows(square(arzneg)));\n-                int[] idxnorms = sortedIndices(arnorms.getRow(0));\n-                RealMatrix arnormsSorted = selectColumns(arnorms, idxnorms);\n-                int[] idxReverse = reverse(idxnorms);\n-                RealMatrix arnormsReverse = selectColumns(arnorms, idxReverse);\n+                final int[] idxnorms = sortedIndices(arnorms.getRow(0));\n+                final RealMatrix arnormsSorted = selectColumns(arnorms, idxnorms);\n+                final int[] idxReverse = reverse(idxnorms);\n+                final RealMatrix arnormsReverse = selectColumns(arnorms, idxReverse);\n                 arnorms = divide(arnormsReverse, arnormsSorted);\n-                int[] idxInv = inverse(idxnorms);\n-                RealMatrix arnormsInv = selectColumns(arnorms, idxInv);\n+                final int[] idxInv = inverse(idxnorms);\n+                final RealMatrix arnormsInv = selectColumns(arnorms, idxInv);\n                 // check and set learning rate negccov\n-                double negcovMax = (1 - negminresidualvariance) /\n+                final double negcovMax = (1 - negminresidualvariance) /\n                     square(arnormsInv).multiply(weights).getEntry(0, 0);\n                 if (negccov > negcovMax) {\n                     negccov = negcovMax;\n                 }\n                 arzneg = times(arzneg, repmat(arnormsInv, dimension, 1));\n-                RealMatrix artmp = BD.multiply(arzneg);\n-                RealMatrix Cneg = artmp.multiply(diag(weights)).multiply(artmp.transpose());\n+                final RealMatrix artmp = BD.multiply(arzneg);\n+                final RealMatrix Cneg = artmp.multiply(diag(weights)).multiply(artmp.transpose());\n                 oldFac += negalphaold * negccov;\n                 C = C.scalarMultiply(oldFac)\n                     .add(roneu) // regard old matrix\n             // to achieve O(N^2)\n             C = triu(C, 0).add(triu(C, 1).transpose());\n             // enforce symmetry to prevent complex numbers\n-            EigenDecomposition eig = new EigenDecomposition(C);\n+            final EigenDecomposition eig = new EigenDecomposition(C);\n             B = eig.getV(); // eigen decomposition, B==normalized eigenvectors\n             D = eig.getD();\n             diagD = diag(D);\n                         diagD.setEntry(i, 0, 0);\n                     }\n                 }\n-                double tfac = max(diagD) / 1e14;\n+                final double tfac = max(diagD) / 1e14;\n                 C = C.add(eye(dimension, dimension).scalarMultiply(tfac));\n                 diagD = diagD.add(ones(dimension, 1).scalarMultiply(tfac));\n             }\n             if (max(diagD) > 1e14 * min(diagD)) {\n-                double tfac = max(diagD) / 1e14 - min(diagD);\n+                final double tfac = max(diagD) / 1e14 - min(diagD);\n                 C = C.add(eye(dimension, dimension).scalarMultiply(tfac));\n                 diagD = diagD.add(ones(dimension, 1).scalarMultiply(tfac));\n             }\n      * @return a sorted array of indices pointing into doubles.\n      */\n     private int[] sortedIndices(final double[] doubles) {\n-        DoubleIndex[] dis = new DoubleIndex[doubles.length];\n+        final DoubleIndex[] dis = new DoubleIndex[doubles.length];\n         for (int i = 0; i < doubles.length; i++) {\n             dis[i] = new DoubleIndex(doubles[i], i);\n         }\n         Arrays.sort(dis);\n-        int[] indices = new int[doubles.length];\n+        final int[] indices = new int[doubles.length];\n         for (int i = 0; i < doubles.length; i++) {\n             indices[i] = dis[i].index;\n         }\n      */\n     private static class DoubleIndex implements Comparable<DoubleIndex> {\n         /** Value to compare. */\n-        private double value;\n+        private final double value;\n         /** Index into sorted array. */\n-        private int index;\n+        private final int index;\n \n         /**\n          * @param value Value to compare.\n          * Flag indicating whether the objective variables are forced into their\n          * bounds if defined\n          */\n-        private boolean isRepairMode;\n+        private final boolean isRepairMode;\n \n         /** Simple constructor.\n          */\n             double value;\n             if (isRepairMode) {\n                 double[] repaired = repair(point);\n-                value = CMAESOptimizer.this\n-                        .computeObjectiveValue(repaired) +\n-                        penalty(point, repaired);\n+                value = CMAESOptimizer.this.computeObjectiveValue(repaired) +\n+                    penalty(point, repaired);\n             } else {\n-                value = CMAESOptimizer.this\n-                        .computeObjectiveValue(point);\n+                value = CMAESOptimizer.this.computeObjectiveValue(point);\n             }\n             return isMinimize ? value : -value;\n         }\n      * @return Matrix representing the element-wise logarithm of m.\n      */\n     private static RealMatrix log(final RealMatrix m) {\n-        double[][] d = new double[m.getRowDimension()][m.getColumnDimension()];\n+        final double[][] d = new double[m.getRowDimension()][m.getColumnDimension()];\n         for (int r = 0; r < m.getRowDimension(); r++) {\n             for (int c = 0; c < m.getColumnDimension(); c++) {\n                 d[r][c] = Math.log(m.getEntry(r, c));\n      * @return Matrix representing the element-wise square root of m.\n      */\n     private static RealMatrix sqrt(final RealMatrix m) {\n-        double[][] d = new double[m.getRowDimension()][m.getColumnDimension()];\n+        final double[][] d = new double[m.getRowDimension()][m.getColumnDimension()];\n         for (int r = 0; r < m.getRowDimension(); r++) {\n             for (int c = 0; c < m.getColumnDimension(); c++) {\n                 d[r][c] = Math.sqrt(m.getEntry(r, c));\n      * @return Matrix representing the element-wise square of m.\n      */\n     private static RealMatrix square(final RealMatrix m) {\n-        double[][] d = new double[m.getRowDimension()][m.getColumnDimension()];\n+        final double[][] d = new double[m.getRowDimension()][m.getColumnDimension()];\n         for (int r = 0; r < m.getRowDimension(); r++) {\n             for (int c = 0; c < m.getColumnDimension(); c++) {\n                 double e = m.getEntry(r, c);\n      * @return the matrix where the elements of m and n are element-wise multiplied.\n      */\n     private static RealMatrix times(final RealMatrix m, final RealMatrix n) {\n-        double[][] d = new double[m.getRowDimension()][m.getColumnDimension()];\n+        final double[][] d = new double[m.getRowDimension()][m.getColumnDimension()];\n         for (int r = 0; r < m.getRowDimension(); r++) {\n             for (int c = 0; c < m.getColumnDimension(); c++) {\n                 d[r][c] = m.getEntry(r, c) * n.getEntry(r, c);\n      * @return Matrix where the elements of m and n are element-wise divided.\n      */\n     private static RealMatrix divide(final RealMatrix m, final RealMatrix n) {\n-        double[][] d = new double[m.getRowDimension()][m.getColumnDimension()];\n+        final double[][] d = new double[m.getRowDimension()][m.getColumnDimension()];\n         for (int r = 0; r < m.getRowDimension(); r++) {\n             for (int c = 0; c < m.getColumnDimension(); c++) {\n                 d[r][c] = m.getEntry(r, c) / n.getEntry(r, c);\n      * @return Matrix representing the selected columns.\n      */\n     private static RealMatrix selectColumns(final RealMatrix m, final int[] cols) {\n-        double[][] d = new double[m.getRowDimension()][cols.length];\n+        final double[][] d = new double[m.getRowDimension()][cols.length];\n         for (int r = 0; r < m.getRowDimension(); r++) {\n             for (int c = 0; c < cols.length; c++) {\n                 d[r][c] = m.getEntry(r, cols[c]);\n      * @return Upper triangular part of matrix.\n      */\n     private static RealMatrix triu(final RealMatrix m, int k) {\n-        double[][] d = new double[m.getRowDimension()][m.getColumnDimension()];\n+        final double[][] d = new double[m.getRowDimension()][m.getColumnDimension()];\n         for (int r = 0; r < m.getRowDimension(); r++) {\n             for (int c = 0; c < m.getColumnDimension(); c++) {\n                 d[r][c] = r <= c - k ? m.getEntry(r, c) : 0;\n      * @return Row matrix representing the sums of the rows.\n      */\n     private static RealMatrix sumRows(final RealMatrix m) {\n-        double[][] d = new double[1][m.getColumnDimension()];\n+        final double[][] d = new double[1][m.getColumnDimension()];\n         for (int c = 0; c < m.getColumnDimension(); c++) {\n             double sum = 0;\n             for (int r = 0; r < m.getRowDimension(); r++) {\n      */\n     private static RealMatrix diag(final RealMatrix m) {\n         if (m.getColumnDimension() == 1) {\n-            double[][] d = new double[m.getRowDimension()][m.getRowDimension()];\n+            final double[][] d = new double[m.getRowDimension()][m.getRowDimension()];\n             for (int i = 0; i < m.getRowDimension(); i++) {\n                 d[i][i] = m.getEntry(i, 0);\n             }\n             return new Array2DRowRealMatrix(d, false);\n         } else {\n-            double[][] d = new double[m.getRowDimension()][1];\n+            final double[][] d = new double[m.getRowDimension()][1];\n             for (int i = 0; i < m.getColumnDimension(); i++) {\n                 d[i][0] = m.getEntry(i, i);\n             }\n      * @param m2 Target matrix.\n      * @param col2 Target column.\n      */\n-    private static void copyColumn(final RealMatrix m1, int col1, RealMatrix m2, int col2) {\n+    private static void copyColumn(final RealMatrix m1, int col1,\n+                                   RealMatrix m2, int col2) {\n         for (int i = 0; i < m1.getRowDimension(); i++) {\n             m2.setEntry(i, col2, m1.getEntry(i, col1));\n         }\n      * @return n-by-m matrix filled with 1.\n      */\n     private static RealMatrix ones(int n, int m) {\n-        double[][] d = new double[n][m];\n+        final double[][] d = new double[n][m];\n         for (int r = 0; r < n; r++) {\n             Arrays.fill(d[r], 1);\n         }\n      * the diagonal.\n      */\n     private static RealMatrix eye(int n, int m) {\n-        double[][] d = new double[n][m];\n+        final double[][] d = new double[n][m];\n         for (int r = 0; r < n; r++) {\n             if (r < m) {\n                 d[r][r] = 1;\n      * @return a matrix which replicates the input matrix in both directions.\n      */\n     private static RealMatrix repmat(final RealMatrix mat, int n, int m) {\n-        int rd = mat.getRowDimension();\n-        int cd = mat.getColumnDimension();\n-        double[][] d = new double[n * rd][m * cd];\n+        final int rd = mat.getRowDimension();\n+        final int cd = mat.getColumnDimension();\n+        final double[][] d = new double[n * rd][m * cd];\n         for (int r = 0; r < n * rd; r++) {\n             for (int c = 0; c < m * cd; c++) {\n                 d[r][c] = mat.getEntry(r % rd, c % cd);\n      * @return a sequence as column matrix.\n      */\n     private static RealMatrix sequence(double start, double end, double step) {\n-        int size = (int) ((end - start) / step + 1);\n-        double[][] d = new double[size][1];\n+        final int size = (int) ((end - start) / step + 1);\n+        final double[][] d = new double[size][1];\n         double value = start;\n         for (int r = 0; r < size; r++) {\n             d[r][0] = value;\n      * @return the inverse of the mapping defined by indices.\n      */\n     private static int[] inverse(final int[] indices) {\n-        int[] inverse = new int[indices.length];\n+        final int[] inverse = new int[indices.length];\n         for (int i = 0; i < indices.length; i++) {\n             inverse[indices[i]] = i;\n         }\n      * @return the indices in inverse order (last is first).\n      */\n     private static int[] reverse(final int[] indices) {\n-        int[] reverse = new int[indices.length];\n+        final int[] reverse = new int[indices.length];\n         for (int i = 0; i < indices.length; i++) {\n             reverse[i] = indices[indices.length - i - 1];\n         }\n      * @return an array of Gaussian random numbers.\n      */\n     private double[] randn(int size) {\n-        double[] randn = new double[size];\n+        final double[] randn = new double[size];\n         for (int i = 0; i < size; i++) {\n             randn[i] = random.nextGaussian();\n         }\n      * @return a 2-dimensional matrix of Gaussian random numbers.\n      */\n     private RealMatrix randn1(int size, int popSize) {\n-        double[][] d = new double[size][popSize];\n+        final double[][] d = new double[size][popSize];\n         for (int r = 0; r < size; r++) {\n             for (int c = 0; c < popSize; c++) {\n                 d[r][c] = random.nextGaussian();", "timestamp": 1350340230, "metainfo": ""}