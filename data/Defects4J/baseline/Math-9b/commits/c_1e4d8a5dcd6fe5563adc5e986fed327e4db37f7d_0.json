{"sha": "1e4d8a5dcd6fe5563adc5e986fed327e4db37f7d", "log": "Formatting. Removed unnecessary decimal point on constants, and added explicit cast where necessary.   ", "commit": "\n--- a/src/main/java/org/apache/commons/math3/optimization/direct/CMAESOptimizer.java\n+++ b/src/main/java/org/apache/commons/math3/optimization/direct/CMAESOptimizer.java\n                     updateCovarianceDiagonalOnly(hsig, bestArz, xold);\n                 }\n                 // Adapt step size sigma - Eq. (5)\n-                sigma *= Math.exp(Math.min(1.0,(normps/chiN - 1.)*cs/damps));\n+                sigma *= Math.exp(Math.min(1, (normps/chiN - 1) * cs / damps));\n                 double bestFitness = fitness[arindex[0]];\n-                double worstFitness = fitness[arindex[arindex.length-1]];\n+                double worstFitness = fitness[arindex[arindex.length - 1]];\n                 if (bestValue > bestFitness) {\n                     bestValue = bestFitness;\n                     lastResult = optimum;\n                 }\n                 // Adjust step size in case of equal function values (flat fitness)\n                 if (bestValue == fitness[arindex[(int)(0.1+lambda/4.)]]) {\n-                    sigma = sigma * Math.exp(0.2+cs/damps);\n+                    sigma = sigma * Math.exp(0.2 + cs / damps);\n                 }\n                 if (iterations > 2 && Math.max(historyWorst, bestFitness) -\n                         Math.min(historyBest, bestFitness) == 0) {\n-                    sigma = sigma * Math.exp(0.2+cs/damps);\n+                    sigma = sigma * Math.exp(0.2 + cs / damps);\n                 }\n                 // store best in history\n                 push(fitnessHistory,bestFitness);\n      */\n     private void initializeCMA(double[] guess) {\n         if (lambda <= 0) {\n-            lambda = 4 + (int) (3. * Math.log(dimension));\n+            lambda = 4 + (int) (3 * Math.log(dimension));\n         }\n         // initialize sigma\n         double[][] sigmaArray = new double[guess.length][1];\n         // initialize selection strategy parameters\n         mu = lambda / 2; // number of parents/points for recombination\n         logMu2 = Math.log(mu + 0.5);\n-        weights = log(sequence(1, mu, 1)).scalarMultiply(-1.).scalarAdd(logMu2);\n+        weights = log(sequence(1, mu, 1)).scalarMultiply(-1).scalarAdd(logMu2);\n         double sumw = 0;\n         double sumwq = 0;\n         for (int i = 0; i < mu; i++) {\n             sumw += w;\n             sumwq += w * w;\n         }\n-        weights = weights.scalarMultiply(1. / sumw);\n+        weights = weights.scalarMultiply(1 / sumw);\n         mueff = sumw * sumw / sumwq; // variance-effectiveness of sum w_i x_i\n \n         // initialize dynamic strategy parameters and constants\n-        cc = (4. + mueff / dimension) /\n-                (dimension + 4. + 2. * mueff / dimension);\n-        cs = (mueff + 2.) / (dimension + mueff + 3.);\n-        damps = (1. + 2. * Math.max(0, Math.sqrt((mueff - 1.) /\n-                (dimension + 1.)) - 1.)) *\n-                Math.max(0.3,\n-                         1. - dimension / (1e-6 + maxIterations)) + cs; // minor increment\n-        ccov1 = 2. / ((dimension + 1.3) * (dimension + 1.3) + mueff);\n-        ccovmu = Math.min(1 - ccov1, 2. * (mueff - 2. + 1. / mueff) /\n-                ((dimension + 2.) * (dimension + 2.) + mueff));\n-        ccov1Sep = Math.min(1, ccov1 * (dimension + 1.5) / 3.);\n-        ccovmuSep = Math.min(1 - ccov1, ccovmu * (dimension + 1.5) / 3.);\n+        cc = (4 + mueff / dimension) /\n+                (dimension + 4 + 2 * mueff / dimension);\n+        cs = (mueff + 2) / (dimension + mueff + 3.);\n+        damps = (1 + 2 * Math.max(0, Math.sqrt((mueff - 1) /\n+                                               (dimension + 1)) - 1)) *\n+            Math.max(0.3,\n+                     1 - dimension / (1e-6 + maxIterations)) + cs; // minor increment\n+        ccov1 = 2 / ((dimension + 1.3) * (dimension + 1.3) + mueff);\n+        ccovmu = Math.min(1 - ccov1, 2 * (mueff - 2 + 1 / mueff) /\n+                          ((dimension + 2) * (dimension + 2) + mueff));\n+        ccov1Sep = Math.min(1, ccov1 * (dimension + 1.5) / 3);\n+        ccovmuSep = Math.min(1 - ccov1, ccovmu * (dimension + 1.5) / 3);\n         chiN = Math.sqrt(dimension) *\n-                (1. - 1. / (4. * dimension) + 1 / (21. * dimension * dimension));\n+            (1 - 1 / ((double) 4 * dimension) + 1 / ((double) 21 * dimension * dimension));\n         // intialize CMA internal values - updated each generation\n-        xmean = MatrixUtils.createColumnRealMatrix(guess); // objective\n-                                                           // variables\n-        diagD = insigma.scalarMultiply(1. / sigma);\n+        xmean = MatrixUtils.createColumnRealMatrix(guess); // objective variables\n+        diagD = insigma.scalarMultiply(1 / sigma);\n         diagC = square(diagD);\n         pc = zeros(dimension, 1); // evolution paths for C and sigma\n         ps = zeros(dimension, 1); // B defines the coordinate system\n         D = ones(dimension, 1); // diagonal D defines the scaling\n         BD = times(B, repmat(diagD.transpose(), dimension, 1));\n         C = B.multiply(diag(square(D)).multiply(B.transpose())); // covariance\n-        historySize = 10 + (int) (3. * 10. * dimension / lambda);\n+        historySize = 10 + (int) (3 * 10 * dimension / (double) lambda);\n         fitnessHistory = new double[historySize]; // history of fitness values\n         for (int i = 0; i < historySize; i++) {\n             fitnessHistory[i] = Double.MAX_VALUE;\n      * @return hsig flag indicating a small correction.\n      */\n     private boolean updateEvolutionPaths(RealMatrix zmean, RealMatrix xold) {\n-        ps = ps.scalarMultiply(1. - cs).add(\n+        ps = ps.scalarMultiply(1 - cs).add(\n                 B.multiply(zmean).scalarMultiply(\n-                        Math.sqrt(cs * (2. - cs) * mueff)));\n+                        Math.sqrt(cs * (2 - cs) * mueff)));\n         normps = ps.getFrobeniusNorm();\n         boolean hsig = normps /\n-            Math.sqrt(1. - Math.pow(1. - cs, 2. * iterations)) /\n-                chiN < 1.4 + 2. / (dimension + 1.);\n-        pc = pc.scalarMultiply(1. - cc);\n+            Math.sqrt(1 - Math.pow(1 - cs, 2 * iterations)) /\n+            chiN < 1.4 + 2 / ((double) dimension + 1);\n+        pc = pc.scalarMultiply(1 - cc);\n         if (hsig) {\n-            pc = pc.add(xmean.subtract(xold).scalarMultiply(\n-                    Math.sqrt(cc * (2. - cc) * mueff) / sigma));\n+            pc = pc.add(xmean.subtract(xold).scalarMultiply(Math.sqrt(cc * (2 - cc) * mueff) / sigma));\n         }\n         return hsig;\n     }\n                                               final RealMatrix bestArz,\n                                               final RealMatrix xold) {\n         // minor correction if hsig==false\n-        double oldFac = hsig ? 0 : ccov1Sep * cc * (2. - cc);\n-        oldFac += 1. - ccov1Sep - ccovmuSep;\n+        double oldFac = hsig ? 0 : ccov1Sep * cc * (2 - cc);\n+        oldFac += 1 - ccov1Sep - ccovmuSep;\n         diagC = diagC.scalarMultiply(oldFac) // regard old matrix\n-                // plus rank one update\n-                .add(square(pc).scalarMultiply(ccov1Sep))\n-                // plus rank mu update\n-                .add((times(diagC, square(bestArz).multiply(weights)))\n-                        .scalarMultiply(ccovmuSep));\n+            .add(square(pc).scalarMultiply(ccov1Sep)) // plus rank one update\n+            .add((times(diagC, square(bestArz).multiply(weights))) // plus rank mu update\n+                 .scalarMultiply(ccovmuSep));\n         diagD = sqrt(diagC); // replaces eig(C)\n-        if (diagonalOnly > 1 && iterations > diagonalOnly) {\n+        if (diagonalOnly > 1 &&\n+            iterations > diagonalOnly) {\n             // full covariance matrix from now on\n             diagonalOnly = 0;\n             B = eye(dimension, dimension);\n      * @param xold xmean matrix of the previous generation.\n      */\n     private void updateCovariance(boolean hsig, final RealMatrix bestArx,\n-            final RealMatrix arz, final int[] arindex, final RealMatrix xold) {\n+                                  final RealMatrix arz, final int[] arindex,\n+                                  final RealMatrix xold) {\n         double negccov = 0;\n         if (ccov1 + ccovmu > 0) {\n             RealMatrix arpos = bestArx.subtract(repmat(xold, 1, mu))\n-                    .scalarMultiply(1. / sigma); // mu difference vectors\n+                    .scalarMultiply(1 / sigma); // mu difference vectors\n             RealMatrix roneu = pc.multiply(pc.transpose())\n                     .scalarMultiply(ccov1); // rank one update\n             // minor correction if hsig==false\n-            double oldFac = hsig ? 0 : ccov1 * cc * (2. - cc);\n-            oldFac += 1. - ccov1 - ccovmu;\n+            double oldFac = hsig ? 0 : ccov1 * cc * (2 - cc);\n+            oldFac += 1 - ccov1 - ccovmu;\n             if (isActiveCMA) {\n                 // Adapt covariance matrix C active CMA\n-                negccov = (1. - ccovmu) * 0.25 * mueff /\n-                (Math.pow(dimension + 2., 1.5) + 2. * mueff);\n+                negccov = (1 - ccovmu) * 0.25 * mueff /\n+                    (Math.pow(dimension + 2, 1.5) + 2 * mueff);\n                 double negminresidualvariance = 0.66;\n                 // keep at least 0.66 in all directions, small popsize are most\n                 // critical\n                                           // loss,\n                 // prepare vectors, compute negative updating matrix Cneg\n                 int[] arReverseIndex = reverse(arindex);\n-                RealMatrix arzneg\n-                    = selectColumns(arz, MathArrays.copyOf(arReverseIndex, mu));\n+                RealMatrix arzneg = selectColumns(arz, MathArrays.copyOf(arReverseIndex, mu));\n                 RealMatrix arnorms = sqrt(sumRows(square(arzneg)));\n                 int[] idxnorms = sortedIndices(arnorms.getRow(0));\n                 RealMatrix arnormsSorted = selectColumns(arnorms, idxnorms);\n                 int[] idxInv = inverse(idxnorms);\n                 RealMatrix arnormsInv = selectColumns(arnorms, idxInv);\n                 // check and set learning rate negccov\n-                double negcovMax = (1. - negminresidualvariance) /\n-                        square(arnormsInv).multiply(weights).getEntry(0, 0);\n+                double negcovMax = (1 - negminresidualvariance) /\n+                    square(arnormsInv).multiply(weights).getEntry(0, 0);\n                 if (negccov > negcovMax) {\n                     negccov = negcovMax;\n                 }\n                 arzneg = times(arzneg, repmat(arnormsInv, dimension, 1));\n                 RealMatrix artmp = BD.multiply(arzneg);\n-                RealMatrix Cneg = artmp.multiply(diag(weights)).multiply(\n-                        artmp.transpose());\n+                RealMatrix Cneg = artmp.multiply(diag(weights)).multiply(artmp.transpose());\n                 oldFac += negalphaold * negccov;\n                 C = C.scalarMultiply(oldFac)\n-                        // regard old matrix\n-                        .add(roneu)\n-                        // plus rank one update\n-                        .add(arpos.scalarMultiply(\n-                                // plus rank mu update\n-                                ccovmu + (1. - negalphaold) * negccov)\n-                                .multiply(\n-                                        times(repmat(weights, 1, dimension),\n-                                                arpos.transpose())))\n-                        .subtract(Cneg.scalarMultiply(negccov));\n+                    .add(roneu) // regard old matrix\n+                    .add(arpos.scalarMultiply( // plus rank one update\n+                                              ccovmu + (1 - negalphaold) * negccov) // plus rank mu update\n+                         .multiply(times(repmat(weights, 1, dimension),\n+                                         arpos.transpose())))\n+                    .subtract(Cneg.scalarMultiply(negccov));\n             } else {\n                 // Adapt covariance matrix C - nonactive\n                 C = C.scalarMultiply(oldFac) // regard old matrix\n-                        .add(roneu)\n-                        // plus rank one update\n-                        .add(arpos.scalarMultiply(ccovmu) // plus rank mu update\n-                                .multiply(\n-                                        times(repmat(weights, 1, dimension),\n-                                                arpos.transpose())));\n+                    .add(roneu) // plus rank one update\n+                    .add(arpos.scalarMultiply(ccovmu) // plus rank mu update\n+                         .multiply(times(repmat(weights, 1, dimension),\n+                                         arpos.transpose())));\n             }\n         }\n         updateBD(negccov);\n      */\n     private void updateBD(double negccov) {\n         if (ccov1 + ccovmu + negccov > 0 &&\n-                (iterations % 1. / (ccov1 + ccovmu + negccov) / dimension / 10.) < 1.) {\n+            (iterations % 1. / (ccov1 + ccovmu + negccov) / dimension / 10.) < 1) {\n             // to achieve O(N^2)\n             C = triu(C, 0).add(triu(C, 1).transpose());\n             // enforce symmetry to prevent complex numbers\n             if (min(diagD) <= 0) {\n                 for (int i = 0; i < dimension; i++) {\n                     if (diagD.getEntry(i, 0) < 0) {\n-                        diagD.setEntry(i, 0, 0.);\n+                        diagD.setEntry(i, 0, 0);\n                     }\n                 }\n                 double tfac = max(diagD) / 1e14;\n             }\n \n             return false;\n-\n         }\n \n         /** {@inheritDoc} */\n             long bits = Double.doubleToLongBits(value);\n             return (int) ((1438542 ^ (bits >>> 32) ^ bits) & 0xffffffff);\n         }\n-\n     }\n \n     /**\n         /** Simple constructor.\n          */\n         public FitnessFunction() {\n-            valueRange = 1.0;\n+            valueRange = 1;\n             isRepairMode = true;\n         }\n \n \n         /**\n          * @param x Normalized objective variables.\n-         * @return the repaired objective variables - all in bounds.\n+         * @return the repaired (i.e. all in bounds) objective variables.\n          */\n         private double[] repair(final double[] x) {\n             final double[] lB = CMAESOptimizer.this.getLowerBound();\n     }\n \n     /**\n-     * @param m\n-     *            Input matrix\n+     * @param m Input matrix.\n      * @return Matrix representing the element-wise square root of m.\n      */\n     private static RealMatrix sqrt(final RealMatrix m) {\n     }\n \n     /**\n-     * @param m Input matrix\n-     * @return Matrix representing the element-wise square (^2) of m.\n+     * @param m Input matrix.\n+     * @return Matrix representing the element-wise square of m.\n      */\n     private static RealMatrix square(final RealMatrix m) {\n         double[][] d = new double[m.getRowDimension()][m.getColumnDimension()];\n     /**\n      * Copies a column from m1 to m2.\n      *\n-     * @param m1 Source matrix 1.\n+     * @param m1 Source matrix.\n      * @param col1 Source column.\n      * @param m2 Target matrix.\n      * @param col2 Target column.\n     private static RealMatrix ones(int n, int m) {\n         double[][] d = new double[n][m];\n         for (int r = 0; r < n; r++) {\n-            Arrays.fill(d[r], 1.0);\n+            Arrays.fill(d[r], 1);\n         }\n         return new Array2DRowRealMatrix(d, false);\n     }\n     /**\n      * @param n Number of rows.\n      * @param m Number of columns.\n-     * @return n-by-m matrix of 0.0-values, diagonal has values 1.0.\n+     * @return n-by-m matrix of 0 values out of diagonal, and 1 values on\n+     * the diagonal.\n      */\n     private static RealMatrix eye(int n, int m) {\n         double[][] d = new double[n][m];\n     /**\n      * @param n Number of rows.\n      * @param m Number of columns.\n-     * @return n-by-m matrix of 0.0-values.\n+     * @return n-by-m matrix of zero values.\n      */\n     private static RealMatrix zeros(int n, int m) {\n         return new Array2DRowRealMatrix(n, m);", "timestamp": 1350310172, "metainfo": ""}