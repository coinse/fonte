{"sha": "8608520aa13bec03cdcb8d4273116e81004321ba", "log": "Rename pname from 'tkn' to 'token'.  ", "commit": "\n--- a/src/main/java/org/apache/commons/csv/CSVLexer.java\n+++ b/src/main/java/org/apache/commons/csv/CSVLexer.java\n      * <p/>\n      * A token corresponds to a term, a record change or an end-of-file indicator.\n      * \n-     * @param tkn\n+     * @param token\n      *            an existing Token object to reuse. The caller is responsible to initialize the Token.\n      * @return the next token found\n      * @throws java.io.IOException\n      *             on stream access error\n      */\n     @Override\n-    Token nextToken(Token tkn) throws IOException {\n+    Token nextToken(Token token) throws IOException {\n \n         // get the last read char (required for empty line detection)\n         int lastChar = in.readAgain();\n                 eol = isEndOfLine(c);\n                 // reached end of file without any content (empty line at the end)\n                 if (isEndOfFile(c)) {\n-                    tkn.type = EOF;\n+                    token.type = EOF;\n                     // don't set tkn.isReady here because no content\n-                    return tkn;\n+                    return token;\n                 }\n             }\n         }\n \n         // did we reach eof during the last iteration already ? EOF\n         if (isEndOfFile(lastChar) || (!isDelimiter(lastChar) && isEndOfFile(c))) {\n-            tkn.type = EOF;\n+            token.type = EOF;\n             // don't set tkn.isReady here because no content\n-            return tkn;\n+            return token;\n         }\n \n         if (isStartOfLine(lastChar) && isCommentStart(c)) {\n             String comment = in.readLine().trim();\n-            tkn.content.append(comment);\n-            tkn.type = COMMENT;\n-            return tkn;\n+            token.content.append(comment);\n+            token.type = COMMENT;\n+            return token;\n         }\n \n         // important: make sure a new char gets consumed in each iteration\n-        while (tkn.type == INVALID) {\n+        while (token.type == INVALID) {\n             // ignore whitespaces at beginning of a token\n             if (surroundingSpacesIgnored) {\n                 while (isWhitespace(c) && !eol) {\n             // ok, start of token reached: encapsulated, or token\n             if (isDelimiter(c)) {\n                 // empty token return TOKEN(\"\")\n-                tkn.type = TOKEN;\n+                token.type = TOKEN;\n             } else if (eol) {\n                 // empty token return EORECORD(\"\")\n                 // noop: tkn.content.append(\"\");\n-                tkn.type = EORECORD;\n+                token.type = EORECORD;\n             } else if (isEncapsulator(c)) {\n                 // consume encapsulated token\n-                encapsulatedTokenLexer(tkn);\n+                encapsulatedTokenLexer(token);\n             } else if (isEndOfFile(c)) {\n                 // end of file return EOF()\n                 // noop: tkn.content.append(\"\");\n-                tkn.type = EOF;\n-                tkn.isReady = true; // there is data at EOF\n+                token.type = EOF;\n+                token.isReady = true; // there is data at EOF\n             } else {\n                 // next token must be a simple token\n                 // add removed blanks when not ignoring whitespace chars...\n-                simpleTokenLexer(tkn, c);\n-            }\n-        }\n-        return tkn;\n+                simpleTokenLexer(token, c);\n+            }\n+        }\n+        return token;\n     }\n \n     /**", "timestamp": 1347392871, "metainfo": ""}